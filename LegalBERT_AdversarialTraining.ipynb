{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"LegalBERT_AdversarialTraining.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm","authorship_tag":"ABX9TyN1xzDjFwhtHAxdGO/Nii9y"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"markdown","source":["# ***Adversarial Training Legal BERT***\n","\n","This notebook combines the LegalBert model classifier as well as the trigger generation. The implementation is structured as follows:\n","* Read files\n","* Generate tokens (no natural). The previous model is taken into consideration to know what words make the model to perform the worst. N (100) candidates are taken per token (i.e. if it's desired to generate 3 tokens that will be added (concatenated) at the beginning, end or randomly. Then, 100 candidates are taken per each of those 3 tokens). These values are stored in and array, s.t. at the end, we can create new sentences using them.\n","* Data Augmentation. Generated tokens are used to generate new sentences.\n","* Adversarial training. A new model is created using the original dataset and the augmented samples.\n","\n","\n","Technnological details:\n","* Model name: LegalBert (nlpaueb/legal-bert-small-uncased)\n","* Class model: BertForSequenceClassification\n","* Batch size: 16|32\n","* Embedding size: 512 (default)\n","* Dataset: Claudette (Terms of Service)\n","* Num classes: 2 (just considered as a binary classification as {Fair|Unfair} sentences)\n","* Vocabulary size: 30,522 (default)\n","* Num tokens: 3|4|6"],"metadata":{"id":"7pdKG_0kdKNY"}},{"cell_type":"markdown","source":["### Global variables"],"metadata":{"id":"ph2Sd0V0f8zA"}},{"cell_type":"code","execution_count":61,"metadata":{"id":"OtMDzI63cxXF","executionInfo":{"status":"ok","timestamp":1659631660975,"user_tz":-120,"elapsed":5,"user":{"displayName":"Isaac Olguín","userId":"09697151780329788960"}}},"outputs":[],"source":["# Global variables\n","\n","BATCH_SIZE = 32\n","MODEL_NAME = 'nlpaueb/legal-bert-small-uncased'#'bert-base-uncased'\n","EPOCHS = 3\n","EMBEDDING_SIZE = 512\n","NUM_CLASSES = 2\n","VOCABULARY_SIZE = 30522\n","NUM_TOKENS = 6\n","DEFAULT_TOKEN = 207 # What in BERT represents the word \"the\"\n","LIST_ID_SPECIAL_TOKENS = [0, 101, 102, 103]\n","LIST_SPECIAL_TOKENS = ['[PAD]', '[CLS]', '[SEP]', '[MASK]']"]},{"cell_type":"markdown","source":["### Installation of packages"],"metadata":{"id":"GkoBzkPfgGbV"}},{"cell_type":"code","source":["!pip install transformers\n","!pip install torch-lr-finder"],"metadata":{"id":"yvLTD3JcgJjj","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1659626877751,"user_tz":-120,"elapsed":32876,"user":{"displayName":"Isaac Olguín","userId":"09697151780329788960"}},"outputId":"97223c09-f444-4c85-a8ab-ac90a78b51c4"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting transformers\n","  Downloading transformers-4.21.1-py3-none-any.whl (4.7 MB)\n","\u001b[K     |████████████████████████████████| 4.7 MB 7.2 MB/s \n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n","Collecting huggingface-hub<1.0,>=0.1.0\n","  Downloading huggingface_hub-0.8.1-py3-none-any.whl (101 kB)\n","\u001b[K     |████████████████████████████████| 101 kB 11.0 MB/s \n","\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n","Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n","  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n","\u001b[K     |████████████████████████████████| 6.6 MB 79.6 MB/s \n","\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.7.1)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.12.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Collecting pyyaml>=5.1\n","  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n","\u001b[K     |████████████████████████████████| 596 kB 76.1 MB/s \n","\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.1.1)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.1)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.6.15)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Installing collected packages: pyyaml, tokenizers, huggingface-hub, transformers\n","  Attempting uninstall: pyyaml\n","    Found existing installation: PyYAML 3.13\n","    Uninstalling PyYAML-3.13:\n","      Successfully uninstalled PyYAML-3.13\n","Successfully installed huggingface-hub-0.8.1 pyyaml-6.0 tokenizers-0.12.1 transformers-4.21.1\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting torch-lr-finder\n","  Downloading torch_lr_finder-0.2.1-py3-none-any.whl (11 kB)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torch-lr-finder) (4.64.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch-lr-finder) (1.21.6)\n","Requirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from torch-lr-finder) (1.12.0+cu113)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from torch-lr-finder) (3.2.2)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from torch-lr-finder) (21.3)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=0.4.1->torch-lr-finder) (4.1.1)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->torch-lr-finder) (3.0.9)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->torch-lr-finder) (1.4.4)\n","Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->torch-lr-finder) (2.8.2)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->torch-lr-finder) (0.11.0)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib->torch-lr-finder) (1.15.0)\n","Installing collected packages: torch-lr-finder\n","Successfully installed torch-lr-finder-0.2.1\n"]}]},{"cell_type":"markdown","source":["### Imports"],"metadata":{"id":"qTV9i90-gMrK"}},{"cell_type":"code","source":["import torch\n","import os\n","from transformers import BertTokenizer\n","from google.colab import drive\n","from torch.utils.data import TensorDataset, random_split\n","from transformers import BertForSequenceClassification, AdamW, BertConfig\n","from transformers import get_linear_schedule_with_warmup\n","import numpy as np\n","import time\n","import datetime\n","import random\n","import gc\n","from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n","from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score\n","from sklearn.model_selection import train_test_split\n","from copy import deepcopy\n","from datetime import datetime\n","import datetime"],"metadata":{"id":"sr1sOPFfgOC1","executionInfo":{"status":"ok","timestamp":1659628171075,"user_tz":-120,"elapsed":243,"user":{"displayName":"Isaac Olguín","userId":"09697151780329788960"}}},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":["### Mount on GoogleDrive"],"metadata":{"id":"jH9Kb1j6gski"}},{"cell_type":"code","source":["# Mount drive to have access to your files\n","drive.mount('/content/drive')\n","%cd /content/drive/MyDrive/\"Colab Notebooks\"/DefenseAdvAttacks"],"metadata":{"id":"ieb8-zc_gvw9","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1659627001726,"user_tz":-120,"elapsed":111398,"user":{"displayName":"Isaac Olguín","userId":"09697151780329788960"}},"outputId":"ab89cb8f-d785-4866-cac6-8cf8b3d80674"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","/content/drive/MyDrive/Colab Notebooks/DefenseAdvAttacks\n"]}]},{"cell_type":"markdown","source":["### Device"],"metadata":{"id":"_vK3qX7DgQWi"}},{"cell_type":"code","source":["# If there's a GPU available...\n","if torch.cuda.is_available():    \n","\n","    # Tell PyTorch to use the GPU.    \n","    device = torch.device(\"cuda\")\n","\n","    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n","\n","    print('We will use the GPU:', torch.cuda.get_device_name(0))\n","\n","# If not...\n","else:\n","    print('No GPU available, using the CPU instead.')\n","    device = torch.device(\"cpu\")"],"metadata":{"id":"5Bdcy5UAgRVf","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1659627001728,"user_tz":-120,"elapsed":33,"user":{"displayName":"Isaac Olguín","userId":"09697151780329788960"}},"outputId":"c297b32a-9dcc-4d37-c32b-bcae3c695a70"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["There are 1 GPU(s) available.\n","We will use the GPU: Tesla P100-PCIE-16GB\n"]}]},{"cell_type":"markdown","source":["### Reading original dataset"],"metadata":{"id":"osD8Dk0mgjmd"}},{"cell_type":"code","source":["# Funtion to read all sentences\n","def get_sentences(path):\n","    sentences= []\n","    for filename in sorted(os.listdir(path)):\n","        with open(path+filename, 'r') as f:\n","            for sentence in f :\n","                if sentence.strip() != '':\n","                    sentences.append(sentence)\n","    return sentences\n","\n","# Function to read get all labels\n","def get_labels(path):\n","    all_labels = []\n","    for filename in sorted(os.listdir(path)):\n","        file_labels = []\n","        with open(path+filename, 'r') as f:\n","            for label in f :\n","                if label.strip() != '':\n","                    all_labels.append(int(label))\n","    return all_labels"],"metadata":{"id":"DudLv10rgX8t","executionInfo":{"status":"ok","timestamp":1659627001730,"user_tz":-120,"elapsed":22,"user":{"displayName":"Isaac Olguín","userId":"09697151780329788960"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["# Reading sentences and labels of the Claudette Dataset\n","all_sentences = get_sentences(\"ToS/TrainValSet/Sentences/\")\n","all_labels = get_labels(\"ToS/TrainValSet/Labels/\")\n","\n","# Conversion of unfair sentence's flag|tag. Since unfair sentences are marked as \"-1\", we change them to \"0\" for simplicity. \n","#   Zero means fair, \n","#   One means unfair\n","all_labels =  [0 if label ==-1 else label for label in all_labels]"],"metadata":{"id":"-Mm3CtQmhKZw","executionInfo":{"status":"ok","timestamp":1659631586421,"user_tz":-120,"elapsed":1973,"user":{"displayName":"Isaac Olguín","userId":"09697151780329788960"}}},"execution_count":57,"outputs":[]},{"cell_type":"markdown","source":["## **Trigger Generation**"],"metadata":{"id":"GmMyL6l6hgfi"}},{"cell_type":"markdown","source":["### Bert (Model, Tokenizer and Load)"],"metadata":{"id":"VojPVT9why_j"}},{"cell_type":"code","source":["# Load the BERT tokenizer.\n","print('Loading BERT tokenizer...')\n","tokenizer = BertTokenizer.from_pretrained(MODEL_NAME, do_lower_case=True) # the model 'bert-base-uncased' only contains lower case sentences"],"metadata":{"id":"K7BsbevuhvaT","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1659631608579,"user_tz":-120,"elapsed":572,"user":{"displayName":"Isaac Olguín","userId":"09697151780329788960"}},"outputId":"77563ae2-36fa-4f14-93f9-1cbd46d5576f"},"execution_count":58,"outputs":[{"output_type":"stream","name":"stdout","text":["Loading BERT tokenizer...\n"]}]},{"cell_type":"markdown","source":["In case that it's desired to know basic functions how to convert sentences to tokens or the other way around.\n","\n","```\n","# ==> Example of first sentence\n","\n","# Print the original sentence.\n","print(' Original: ', all_sentences[0])\n","\n","# Print the sentence split into tokens.\n","print('Tokenized: ', tokenizer.tokenize(all_sentences[0]))\n","\n","# Print the sentence mapped to token ids.\n","print('Token IDs: ', tokenizer.convert_tokens_to_ids(tokenizer.tokenize(all_sentences[0])))\n","```\n","\n"],"metadata":{"id":"kotGUF2liGj_"}},{"cell_type":"markdown","source":["To get the max length of a sentence within the dataset (sentences that have been already read)\n","```\n","# ==> Get the max length of a sentence\n","\n","max_len = 0\n","\n","# For every sentence...\n","for sent in all_sentences:\n","\n","    # Tokenize the text and add `[CLS]` and `[SEP]` tokens.\n","    input_ids = tokenizer.encode(sent, add_special_tokens=True)\n","\n","    # Update the maximum sentence length.\n","    max_len = max(max_len, len(input_ids))\n","\n","print('Max sentence length: ', max_len)\n","```\n","\n","Output:\n","```\n","Token indices sequence length is longer than the specified maximum sequence length for this model (598 > 512). Running this sequence through the model will result in indexing errors\n","\n","Max sentence length:  598\n","```\n","\n"],"metadata":{"id":"OPrdoqOLibPR"}},{"cell_type":"markdown","source":["### Model BertForSequenceClassification (Load model)"],"metadata":{"id":"M7vhHZYAi7sm"}},{"cell_type":"code","source":["# Load BertForSequenceClassification, the pretrained BERT model with a single \n","# linear classification layer on top. \n","model = BertForSequenceClassification.from_pretrained(\n","    MODEL_NAME, # Use the 12-layer BERT model, with an uncased vocab.\n","    num_labels = NUM_CLASSES, # The number of output labels--2 for binary classification.\n","                    # You can increase this for multi-class tasks.   \n","    output_attentions = False, # Whether the model returns attentions weights.\n","    output_hidden_states = False, # Whether the model returns all hidden-states.\n",")\n","\n","# Tell pytorch to run this model on the GPU.\n","model.cuda()"],"metadata":{"id":"-2fcQVw-i1eb","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1659631624771,"user_tz":-120,"elapsed":7274,"user":{"displayName":"Isaac Olguín","userId":"09697151780329788960"}},"outputId":"a1a4e042-28f3-49a0-9281-cfdd1d315233"},"execution_count":59,"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at nlpaueb/legal-bert-small-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias']\n","- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at nlpaueb/legal-bert-small-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"execute_result","data":{"text/plain":["BertForSequenceClassification(\n","  (bert): BertModel(\n","    (embeddings): BertEmbeddings(\n","      (word_embeddings): Embedding(30522, 512, padding_idx=0)\n","      (position_embeddings): Embedding(512, 512)\n","      (token_type_embeddings): Embedding(2, 512)\n","      (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (encoder): BertEncoder(\n","      (layer): ModuleList(\n","        (0): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=512, out_features=512, bias=True)\n","              (key): Linear(in_features=512, out_features=512, bias=True)\n","              (value): Linear(in_features=512, out_features=512, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=512, out_features=512, bias=True)\n","              (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=512, out_features=2048, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=2048, out_features=512, bias=True)\n","            (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (1): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=512, out_features=512, bias=True)\n","              (key): Linear(in_features=512, out_features=512, bias=True)\n","              (value): Linear(in_features=512, out_features=512, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=512, out_features=512, bias=True)\n","              (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=512, out_features=2048, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=2048, out_features=512, bias=True)\n","            (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (2): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=512, out_features=512, bias=True)\n","              (key): Linear(in_features=512, out_features=512, bias=True)\n","              (value): Linear(in_features=512, out_features=512, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=512, out_features=512, bias=True)\n","              (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=512, out_features=2048, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=2048, out_features=512, bias=True)\n","            (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (3): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=512, out_features=512, bias=True)\n","              (key): Linear(in_features=512, out_features=512, bias=True)\n","              (value): Linear(in_features=512, out_features=512, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=512, out_features=512, bias=True)\n","              (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=512, out_features=2048, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=2048, out_features=512, bias=True)\n","            (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (4): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=512, out_features=512, bias=True)\n","              (key): Linear(in_features=512, out_features=512, bias=True)\n","              (value): Linear(in_features=512, out_features=512, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=512, out_features=512, bias=True)\n","              (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=512, out_features=2048, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=2048, out_features=512, bias=True)\n","            (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (5): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=512, out_features=512, bias=True)\n","              (key): Linear(in_features=512, out_features=512, bias=True)\n","              (value): Linear(in_features=512, out_features=512, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=512, out_features=512, bias=True)\n","              (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=512, out_features=2048, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=2048, out_features=512, bias=True)\n","            (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","    (pooler): BertPooler(\n","      (dense): Linear(in_features=512, out_features=512, bias=True)\n","      (activation): Tanh()\n","    )\n","  )\n","  (dropout): Dropout(p=0.1, inplace=False)\n","  (classifier): Linear(in_features=512, out_features=2, bias=True)\n",")"]},"metadata":{},"execution_count":59}]},{"cell_type":"code","source":["# Load the model and dictionary\n","model.load_state_dict(torch.load('Bert4SeqClassif_20220804_1134_wPersistency.pt'))#, map_location=torch.device('cpu') or cuda. Both work"],"metadata":{"id":"DLgYcXdvjH37","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1659631642861,"user_tz":-120,"elapsed":258,"user":{"displayName":"Isaac Olguín","userId":"09697151780329788960"}},"outputId":"565f2c88-4524-4f49-ba8e-a940a409dd92"},"execution_count":60,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<All keys matched successfully>"]},"metadata":{},"execution_count":60}]},{"cell_type":"markdown","source":["### Trigger generation"],"metadata":{"id":"6ov6kgpbjZF1"}},{"cell_type":"markdown","source":["##### General functions"],"metadata":{"id":"fHepLK4AkHD_"}},{"cell_type":"code","source":["# hook used in add_hooks()\n","extracted_grads = []\n","def extract_grad_hook(module, grad_in, grad_out):\n","    extracted_grads.append(grad_out[0])\n","\n","# returns the wordpiece embedding weight matrix\n","def get_embedding_weight(language_model):\n","    for module in language_model.modules():\n","        if isinstance(module, torch.nn.Embedding):\n","            if module.weight.shape[0] == 30522: # only add a hook to wordpiece embeddings, not position embeddings \n","                ##50257 is the size of the vocabulary of GPT\n","                return module.weight.detach()\n","\n","# add hooks for embeddings\n","def add_hooks(language_model):\n","    for module in language_model.modules():\n","        if isinstance(module, torch.nn.Embedding):\n","            if module.weight.shape[0] == 30522: # only add a hook to wordpiece embeddings, not position\n","                ##50257 is the size of the vocabulary of GPT\n","                module.weight.requires_grad = True\n","                #module.register_backward_hook(extract_grad_hook)\n","                module.register_full_backward_hook(extract_grad_hook)\n","\n","\n","# creates the batch of target texts with -1 placed at the end of the sequences for padding (for masking out the loss).\n","def make_target_batch(tokenizer, device, target_texts):\n","    # encode items and get the max length\n","    encoded_texts = []\n","    max_len = 0\n","    for target_text in target_texts:\n","        encoded_target_text = tokenizer.encode_plus(\n","            target_text,\n","            add_special_tokens = True,\n","            max_length = EMBEDDING_SIZE - NUM_TOKENS,\n","            pad_to_max_length = True,\n","            return_attention_mask = True\n","        )\n","        encoded_texts.append(encoded_target_text.input_ids)\n","        if len(encoded_target_text.input_ids) > max_len:\n","            max_len = len(encoded_target_text)\n","\n","    # pad tokens, i.e., append -1 to the end of the non-longest ones\n","    for indx, encoded_text in enumerate(encoded_texts):\n","        if len(encoded_text) < max_len:\n","            encoded_texts[indx].extend([-1] * (max_len - len(encoded_text)))\n","\n","    # convert to tensors and batch them up\n","    target_tokens_batch = None\n","    for encoded_text in encoded_texts:\n","        target_tokens = torch.tensor(encoded_text, device=device, dtype=torch.long).unsqueeze(0)\n","        if target_tokens_batch is None:\n","            target_tokens_batch = target_tokens\n","        else:\n","            target_tokens_batch = torch.cat((target_tokens, target_tokens_batch), dim=0)\n","    return target_tokens_batch\n","\n","# Got from https://github.com/Eric-Wallace/universal-triggers/blob/master/attacks.py\n","def hotflip_attack(averaged_grad, embedding_matrix, trigger_token_ids,\n","                   increase_loss=False, num_candidates=1):\n","    \"\"\"\n","    The \"Hotflip\" attack described in Equation (2) of the paper. This code is heavily inspired by\n","    the nice code of Paul Michel here https://github.com/pmichel31415/translate/blob/paul/\n","    pytorch_translate/research/adversarial/adversaries/brute_force_adversary.py\n","    This function takes in the model's average_grad over a batch of examples, the model's\n","    token embedding matrix, and the current trigger token IDs. It returns the top token\n","    candidates for each position.\n","    If increase_loss=True, then the attack reverses the sign of the gradient and tries to increase\n","    the loss (decrease the model's probability of the true class). For targeted attacks, you want\n","    to decrease the loss of the target class (increase_loss=False).\n","    \"\"\"\n","    averaged_grad = averaged_grad.cpu()\n","    embedding_matrix = embedding_matrix.cpu()\n","    trigger_token_embeds = torch.nn.functional.embedding(torch.LongTensor(trigger_token_ids),\n","                                                         embedding_matrix).detach().unsqueeze(0)\n","    averaged_grad = averaged_grad.unsqueeze(0)\n","    gradient_dot_embedding_matrix = torch.einsum(\"bij,kj->bik\",\n","                                                 (averaged_grad, embedding_matrix))        \n","    if not increase_loss:\n","        gradient_dot_embedding_matrix *= -1    # lower versus increase the class probability.\n","    if num_candidates > 1: # get top k options\n","        _, best_k_ids = torch.topk(gradient_dot_embedding_matrix, num_candidates, dim=2)\n","        return best_k_ids.detach().cpu().numpy()[0]\n","    _, best_at_each_step = gradient_dot_embedding_matrix.max(2)\n","    return best_at_each_step[0].detach().cpu().numpy()\n","\n","def flat_accuracy(preds, labels):\n","    pred_flat = np.argmax(preds, axis=1).flatten()\n","    labels_flat = labels.flatten()\n","    return np.sum(pred_flat == labels_flat) / len(labels_flat)"],"metadata":{"id":"xTpEhRQmjNEQ","executionInfo":{"status":"ok","timestamp":1659631679645,"user_tz":-120,"elapsed":4,"user":{"displayName":"Isaac Olguín","userId":"09697151780329788960"}}},"execution_count":62,"outputs":[]},{"cell_type":"code","source":["def get_input_masks_and_labels_with_tokens(sentences, labels, tokens, position='B'):\n","    input_ids = []\n","    attention_masks = []\n","    number_of_tokens = []\n","\n","    for sent in sentences:\n","\n","        if position == 'B':\n","            sent_with_tokens = tokens + \" \" + sent\n","        elif position == 'E':\n","            sent_with_tokens = sent + \" \" + tokens\n","        else:\n","            print('Wrong position command, please enter \"E\" or \"B\"')\n","            return\n","\n","        encoded_dict = tokenizer.encode_plus(\n","                        sent_with_tokens,\n","                        add_special_tokens = True,\n","                        max_length = 512,\n","                        pad_to_max_length = True,\n","                        return_attention_mask = True,\n","                        return_tensors = 'pt',\n","                   )\n","\n","\n","        input_ids.append(encoded_dict['input_ids'])\n","\n","        attention_masks.append(encoded_dict['attention_mask'])\n","\n","    input_ids = torch.cat(input_ids, dim=0)\n","    attention_masks = torch.cat(attention_masks, dim=0)\n","    labels = torch.tensor(labels)\n","\n","    # count number of tokens of each sentence\n","    for idx in range(len(input_ids)):\n","      sent_ids = input_ids[idx, :]\n","\n","      cnt = 0\n","      for id in sent_ids:\n","          if id != 0:\n","              cnt += 1\n","\n","      number_of_tokens.append(cnt)  \n","\n","    return input_ids, attention_masks, labels, number_of_tokens"],"metadata":{"id":"eiuMnLcZkR2j","executionInfo":{"status":"ok","timestamp":1659631681424,"user_tz":-120,"elapsed":3,"user":{"displayName":"Isaac Olguín","userId":"09697151780329788960"}}},"execution_count":63,"outputs":[]},{"cell_type":"code","source":["def get_loss_and_metrics(model, dataloader, device):\n","    # get initial loss for the trigger\n","    model.zero_grad()\n","\n","    test_preds = []\n","    test_targets = []\n","\n","    # Tracking variables \n","    total_test_accuracy = 0\n","    total_test_loss = 0\n","    io_total_test_acc = 0\n","    io_total_test_prec = 0\n","    io_total_test_recall = 0\n","    io_total_test_f1 = 0\n","\n","    for batch in dataloader:\n","        b_input_ids = batch[0].to(device)\n","        b_input_mask = batch[1].to(device)\n","        b_labels = batch[2].to(device)\n","\n","        model.zero_grad()\n","\n","        result = model(b_input_ids, \n","                    token_type_ids=None, \n","                    attention_mask=b_input_mask, \n","                    labels=b_labels,\n","                    return_dict=True)\n","\n","        loss = result.loss\n","        logits = result.logits\n","\n","        test_preds.extend(logits.argmax(dim=1).cpu().numpy())\n","        test_targets.extend(batch[2].numpy())\n","\n","        # Accumulate the validation loss.\n","        total_test_loss += loss.item()\n","\n","        test_preds.extend(logits.argmax(dim=1).cpu().numpy())\n","        test_targets.extend(batch[2].numpy())\n","\n","        # Move logits and labels to CPU\n","        logits = logits.detach().cpu().numpy()\n","        label_ids = b_labels.to('cpu').numpy()\n","\n","        loss.backward()        \n","\n","        # Calculate the accuracy for this batch of test sentences, and\n","        # accumulate it over all batches.        \n","        test_acc = accuracy_score(test_targets, test_preds)\n","        test_precision = precision_score(test_targets, test_preds)\n","        test_recall = recall_score(test_targets, test_preds)\n","        test_f1 = f1_score(test_targets, test_preds)\n","\n","        io_total_test_acc += test_acc\n","        io_total_test_prec += test_precision\n","        io_total_test_recall += test_recall\n","        io_total_test_f1 += test_f1\n","\n","    io_avg_test_loss = total_test_loss/len(dataloader)\n","    io_avg_test_acc = io_total_test_acc / len(dataloader)\n","    io_avg_test_prec = io_total_test_prec / len(dataloader)\n","    io_avg_test_recall = io_total_test_recall / len(dataloader)\n","    io_avg_test_f1 = io_total_test_f1 / len(dataloader)\n","    \"\"\"\n","    print(\n","            f'Loss {io_avg_test_loss} : \\t\\\n","            Valid_acc : {io_avg_test_acc}\\t\\\n","            Valid_F1 : {io_avg_test_f1}\\t\\\n","            Valid_precision : {io_avg_test_prec}\\t\\\n","            Valid_recall : {io_avg_test_recall}'\n","          )\n","    \"\"\"\n","\n","    return io_avg_test_loss, io_avg_test_acc, io_avg_test_prec, io_avg_test_recall, io_avg_test_f1"],"metadata":{"id":"cVi1JG1ekMkL","executionInfo":{"status":"ok","timestamp":1659631684675,"user_tz":-120,"elapsed":4,"user":{"displayName":"Isaac Olguín","userId":"09697151780329788960"}}},"execution_count":64,"outputs":[]},{"cell_type":"code","source":["def change_input_ids_with_candidate_token(input_ids, position, candidate, number_of_tokens, trigger_position='B'):\n","    if trigger_position == 'B':\n","        input_ids[:, position] = candidate\n","    elif trigger_position == 'E':\n","        for idx in range(len(input_ids)):\n","            if number_of_tokens[idx] > EMBEDDING_SIZE:\n","                input_ids[idx, EMBEDDING_SIZE-NUM_TOKENS-2+position] = candidate\n","            else:\n","                input_ids[idx, number_of_tokens[idx]-NUM_TOKENS-2+position] = candidate\n","    else:\n","        print('Wrong position command, please enter \"E\" or \"B\"')\n","        return\n","    return input_ids"],"metadata":{"id":"yoPr3yJe1xrG","executionInfo":{"status":"ok","timestamp":1659631686009,"user_tz":-120,"elapsed":4,"user":{"displayName":"Isaac Olguín","userId":"09697151780329788960"}}},"execution_count":65,"outputs":[]},{"cell_type":"markdown","source":["##### Execution of the trigger generation"],"metadata":{"id":"XJsvcJua2FVP"}},{"cell_type":"markdown","source":["###### Get unfair sentences"],"metadata":{"id":"G4fBirI72W6s"}},{"cell_type":"code","source":["positions_unfair = np.where(np.array(all_labels) == 1)[0]\n","print(f'First 32 positions: {positions_unfair[0:32]} with total of unfair sentences {len(positions_unfair)}')\n","\n","target_unfair_sentences = []\n","labels_unfair_sentences = []\n","for index in range(len(positions_unfair)):\n","    target_unfair_sentences.append(all_sentences[positions_unfair[index]])\n","    labels_unfair_sentences.append(all_labels[positions_unfair[index]])\n","\n","# Initialization of tokens with the particle \"the\" whose id number in BERT model is 207\n","trigger_tokens = np.array([DEFAULT_TOKEN]*NUM_TOKENS)\n","print(tokenizer.decode(trigger_tokens))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QTKzAMth1-64","executionInfo":{"status":"ok","timestamp":1659631699838,"user_tz":-120,"elapsed":6,"user":{"displayName":"Isaac Olguín","userId":"09697151780329788960"}},"outputId":"5bce102b-8395-492f-c2be-8a0f51f0f2e6"},"execution_count":66,"outputs":[{"output_type":"stream","name":"stdout","text":["First 32 positions: [  7  12  15  24  26  48  54  57  61  62  77  93 102 104 143 151 161 163\n"," 183 194 200 205 219 226 237 241 244 265 275 278 282 294] with total of unfair sentences 991\n","the the the the the the\n"]}]},{"cell_type":"markdown","source":["###### Addition of hooks and get word embeddings"],"metadata":{"id":"7G6pl7nF2vCB"}},{"cell_type":"code","source":["model.eval()\n","model.to(device)\n","\n","add_hooks(model) # add gradient hooks to embeddings\n","embedding_weight = get_embedding_weight(model) # save the word embedding matrix"],"metadata":{"id":"hUPKtMir24Ww","executionInfo":{"status":"ok","timestamp":1659631705020,"user_tz":-120,"elapsed":5,"user":{"displayName":"Isaac Olguín","userId":"09697151780329788960"}}},"execution_count":67,"outputs":[]},{"cell_type":"markdown","source":["###### Get tensors and creation of dataset and dataloader"],"metadata":{"id":"IBgwSKMk275K"}},{"cell_type":"code","source":["## Define at which position we want to have the triggers\n","position = 'B' # Possible values {B|E} for beginning and end, respectively\n","\n","input_ids, attention_masks, labels, number_of_tokens = get_input_masks_and_labels_with_tokens(target_unfair_sentences, labels_unfair_sentences, tokenizer.decode(trigger_tokens), position=position)\n","\n","dataset = TensorDataset(input_ids, attention_masks, labels)\n","\n","dataloader = torch.utils.data.DataLoader(dataset, batch_size=BATCH_SIZE)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yv4m7W-w3LQb","executionInfo":{"status":"ok","timestamp":1659631726443,"user_tz":-120,"elapsed":7721,"user":{"displayName":"Isaac Olguín","userId":"09697151780329788960"}},"outputId":"172ae650-95ff-4042-93cc-bc7353a7fc13"},"execution_count":68,"outputs":[{"output_type":"stream","name":"stderr","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2329: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"]}]},{"cell_type":"markdown","source":["###### Execution of trigger generation"],"metadata":{"id":"qH9EPKm73or2"}},{"cell_type":"code","source":["timestamp = datetime.datetime.now().strftime(\"%y%m%d_%H_%M_%S_%p\")\n","f = open(f\"Execution_Pos{position}_NumTokens_{NUM_TOKENS}_{timestamp}.txt\", \"w\")"],"metadata":{"id":"qJc-5gdiRmpu","executionInfo":{"status":"ok","timestamp":1659631726834,"user_tz":-120,"elapsed":6,"user":{"displayName":"Isaac Olguín","userId":"09697151780329788960"}}},"execution_count":69,"outputs":[]},{"cell_type":"code","source":["candidates_dict = {}\n","candidates_dict[\"combinations_ids\"] = []\n","candidates_dict[\"combinations_labels\"] = []\n","foundX = -1\n","foundY = -1\n","candidates_combination = [DEFAULT_TOKEN]*NUM_TOKENS\n","goalTag = 'Loss'\n","\n","extracted_grads = []\n","\n","loss_obtained, acc_obtained, prec_obtained, recall_obtained, f1_obtained = get_loss_and_metrics(model, dataloader, device)\n","#print(f'loss_obtained {loss_obtained}')\n","\n","candidates_selected = [DEFAULT_TOKEN]*NUM_TOKENS\n","# try all the candidates and pick the best\n","curr_best_loss = loss_obtained\n","curr_best_trigger_tokens = None\n","\n","print(f\"{position}[{foundX:3},{foundY:3}] TokenID[{candidates_combination}] TokensDesc[{tokenizer.decode(candidates_combination)}] Loss[{loss_obtained:3.5}] Acc[{acc_obtained:3.5}] Prec[{prec_obtained:3.5}] Recall[{recall_obtained:3.5}] F1[{f1_obtained:3.5}] => Worst<<{goalTag}>>[{curr_best_loss:3.5}] Found at [{foundX:3},{foundY:3}]\")\n","f.write(f'{position}[{foundX:3},{foundY:3}] TokenID[{candidates_combination}] TokensDesc[{tokenizer.decode(candidates_combination)}] Loss[{loss_obtained:3.5}] Acc[{acc_obtained:3.5}] Prec[{prec_obtained:3.5}] Recall[{recall_obtained:3.5}] F1[{f1_obtained:3.5}] => Worst<<{goalTag}>>[{curr_best_loss:3.5}] Found at [{foundX:3},{foundY:3}]')\n","\n","for id_token_to_flip in range(0, NUM_TOKENS):\n","\n","    averaged_grad = torch.sum(extracted_grads[0], dim=0)\n","    averaged_grad = averaged_grad[id_token_to_flip].unsqueeze(0)\n","\n","    # Use hotflip (linear approximation) attack to get the top num_candidates\n","    candidates = hotflip_attack(averaged_grad, embedding_weight,\n","                                        [trigger_tokens[id_token_to_flip]], \n","                                        increase_loss=False, num_candidates=100)[0]\n","    print(f'candidates {candidates}')\n","    f.write(f'{position} candidates {candidates}')\n","    candidates_dict[id_token_to_flip] = candidates\n","    \n","    for index, cand in enumerate(candidates):\n","        extracted_grads = []\n","\n","        if cand in LIST_ID_SPECIAL_TOKENS:\n","          continue\n","\n","        input_ids_with_candidate_trigger = change_input_ids_with_candidate_token(deepcopy(input_ids), id_token_to_flip+1, cand, number_of_tokens, trigger_position=position)\n","        dataset_with_candidate_trigger = TensorDataset(input_ids_with_candidate_trigger, attention_masks, labels)\n","        dataloader_with_candidate_trigger = torch.utils.data.DataLoader(dataset_with_candidate_trigger, batch_size=BATCH_SIZE)\n","\n","        current_loss, current_acc, current_prec, current_recall, current_f1 = get_loss_and_metrics(model, dataloader_with_candidate_trigger, device)\n","\n","        if curr_best_loss < current_loss:\n","            curr_best_loss = current_loss\n","            candidates_selected[id_token_to_flip] = cand\n","\n","            foundX = id_token_to_flip\n","            foundY = index\n","        candidates_combination[id_token_to_flip] = cand\n","        candidates_dict[\"combinations_ids\"].append(candidates_combination)\n","        candidates_dict[\"combinations_labels\"].append(tokenizer.decode(candidates_combination))\n","        print(f'[{id_token_to_flip:3},{index:3}] TokenID[{candidates_combination}] TokensDesc[{tokenizer.decode(candidates_combination)}] Loss[{current_loss:3.5}] Acc[{current_acc:3.5}] Prec[{current_prec:3.5}] Recall[{current_recall:3.5}] F1[{current_f1:3.5}] => Worst<<{goalTag}>>[{curr_best_loss:3.5}] Found at [{foundX:3},{foundY:3}]')\n","        f.write(f'{position}[{id_token_to_flip:3},{index:3}] TokenID[{candidates_combination}] TokensDesc[{tokenizer.decode(candidates_combination)}] Loss[{current_loss:3.5}] Acc[{current_acc:3.5}] Prec[{current_prec:3.5}] Recall[{current_recall:3.5}] F1[{current_f1:3.5}] => Worst<<{goalTag}>>[{curr_best_loss:3.5}] Found at [{foundX:3},{foundY:3}]\\n')\n","        \n","        del input_ids_with_candidate_trigger\n","        del dataset_with_candidate_trigger\n","        del dataloader_with_candidate_trigger\n","\n","        gc.collect()\n","        torch.cuda.empty_cache()\n","\n","        #print(f'Candidates selected {candidates_selected} VS candidates_combination {candidates_combination}')\n","        #print(f'[{id_token_to_flip}][{index}] loss[{index}] {current_loss} ({curr_best_loss})')\n","\n","    candidates_combination = deepcopy(candidates_selected)\n","    #extracted_grads = []\n","    input_ids = change_input_ids_with_candidate_token(deepcopy(input_ids), id_token_to_flip+1, candidates_selected[id_token_to_flip], number_of_tokens, trigger_position=position)\n","    print(f'Worst loss {curr_best_loss} with candidates {candidates_selected} in the {id_token_to_flip}-iteration with tokens [{tokenizer.decode(candidates_selected)}]')\n","    f.write(f'{position}Worst loss {curr_best_loss} with candidates {candidates_selected}\\n')\n","f.close()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sG9jGWwL3t7d","executionInfo":{"status":"ok","timestamp":1659639316827,"user_tz":-120,"elapsed":7589998,"user":{"displayName":"Isaac Olguín","userId":"09697151780329788960"}},"outputId":"25bd65a3-2c70-40a9-a8ef-739cb6bdf230"},"execution_count":70,"outputs":[{"output_type":"stream","name":"stdout","text":["B[ -1, -1] TokenID[[207, 207, 207, 207, 207, 207]] TokensDesc[the the the the the the] Loss[0.28326] Acc[0.9351] Prec[1.0] Recall[0.9351] F1[0.96638] => Worst<<Loss>>[0.28326] Found at [ -1, -1]\n","candidates [  908  4303   906   608  1744  1923   232  1297   681  1824  2436  5557\n","  4599 18883  2143  1534  1664   460  7716  2399 14391  3566  3641   266\n","  1216  6665  4084  3705 11858   753  2921  8397  1151 15577  1412   371\n","   699  4065  7396  1865  2012  5818  1435  1434   377  6054   579 11734\n","   436  3552  2679  1791 14479  2527  2823  3053 19090 18658  9225  1059\n","  9978 25625  3770 15476  2957 30517 17000  2698  1086  3643  4270 18191\n","  9828   986  8436  8873   378 20750  1263  1314  6410 18451  9148 13048\n","   454   474  3750  2340  3437  3575  2624  6966 21649  6247 10879 21358\n","  5617  1072  1753  1364]\n","[  0,  0] TokenID[[908, 207, 207, 207, 207, 207]] TokensDesc[award the the the the the] Loss[0.36778] Acc[0.9095] Prec[1.0] Recall[0.9095] F1[0.95243] => Worst<<Loss>>[0.36778] Found at [  0,  0]\n","[  0,  1] TokenID[[4303, 207, 207, 207, 207, 207]] TokensDesc[suffering the the the the the] Loss[0.27607] Acc[0.93498] Prec[1.0] Recall[0.93498] F1[0.96632] => Worst<<Loss>>[0.36778] Found at [  0,  0]\n","[  0,  2] TokenID[[906, 207, 207, 207, 207, 207]] TokensDesc[settlement the the the the the] Loss[0.3415] Acc[0.91759] Prec[1.0] Recall[0.91759] F1[0.95689] => Worst<<Loss>>[0.36778] Found at [  0,  0]\n","[  0,  3] TokenID[[608, 207, 207, 207, 207, 207]] TokensDesc[agreements the the the the the] Loss[0.29206] Acc[0.93591] Prec[1.0] Recall[0.93591] F1[0.96681] => Worst<<Loss>>[0.36778] Found at [  0,  0]\n","[  0,  4] TokenID[[1744, 207, 207, 207, 207, 207]] TokensDesc[oral the the the the the] Loss[0.30877] Acc[0.92813] Prec[1.0] Recall[0.92813] F1[0.96263] => Worst<<Loss>>[0.36778] Found at [  0,  0]\n","[  0,  5] TokenID[[1923, 207, 207, 207, 207, 207]] TokensDesc[plea the the the the the] Loss[0.35161] Acc[0.90807] Prec[1.0] Recall[0.90807] F1[0.95164] => Worst<<Loss>>[0.36778] Found at [  0,  0]\n","[  0,  6] TokenID[[232, 207, 207, 207, 207, 207]] TokensDesc[agreement the the the the the] Loss[0.28038] Acc[0.93843] Prec[1.0] Recall[0.93843] F1[0.96816] => Worst<<Loss>>[0.36778] Found at [  0,  0]\n","[  0,  7] TokenID[[1297, 207, 207, 207, 207, 207]] TokensDesc[signature the the the the the] Loss[0.38413] Acc[0.90957] Prec[1.0] Recall[0.90957] F1[0.95251] => Worst<<Loss>>[0.38413] Found at [  0,  7]\n","[  0,  8] TokenID[[681, 207, 207, 207, 207, 207]] TokensDesc[breach the the the the the] Loss[0.22612] Acc[0.94957] Prec[1.0] Recall[0.94957] F1[0.97408] => Worst<<Loss>>[0.38413] Found at [  0,  7]\n","[  0,  9] TokenID[[1824, 207, 207, 207, 207, 207]] TokensDesc[am the the the the the] Loss[0.32942] Acc[0.92315] Prec[1.0] Recall[0.92315] F1[0.95991] => Worst<<Loss>>[0.38413] Found at [  0,  7]\n","[  0, 10] TokenID[[2436, 207, 207, 207, 207, 207]] TokensDesc[awards the the the the the] Loss[0.33756] Acc[0.91829] Prec[1.0] Recall[0.91829] F1[0.95727] => Worst<<Loss>>[0.38413] Found at [  0,  7]\n","[  0, 11] TokenID[[5557, 207, 207, 207, 207, 207]] TokensDesc[actively the the the the the] Loss[0.29802] Acc[0.93046] Prec[1.0] Recall[0.93046] F1[0.9639] => Worst<<Loss>>[0.38413] Found at [  0,  7]\n","[  0, 12] TokenID[[4599, 207, 207, 207, 207, 207]] TokensDesc[negotiate the the the the the] Loss[0.3064] Acc[0.93067] Prec[1.0] Recall[0.93067] F1[0.964] => Worst<<Loss>>[0.38413] Found at [  0,  7]\n","[  0, 13] TokenID[[18883, 207, 207, 207, 207, 207]] TokensDesc[minds the the the the the] Loss[0.28902] Acc[0.93267] Prec[1.0] Recall[0.93267] F1[0.96507] => Worst<<Loss>>[0.38413] Found at [  0,  7]\n","[  0, 14] TokenID[[2143, 207, 207, 207, 207, 207]] TokensDesc[contractual the the the the the] Loss[0.27665] Acc[0.93923] Prec[1.0] Recall[0.93923] F1[0.96859] => Worst<<Loss>>[0.38413] Found at [  0,  7]\n","[  0, 15] TokenID[[1534, 207, 207, 207, 207, 207]] TokensDesc[events the the the the the] Loss[0.27557] Acc[0.93837] Prec[1.0] Recall[0.93837] F1[0.96813] => Worst<<Loss>>[0.38413] Found at [  0,  7]\n","[  0, 16] TokenID[[1664, 207, 207, 207, 207, 207]] TokensDesc[decree the the the the the] Loss[0.3836] Acc[0.89842] Prec[1.0] Recall[0.89842] F1[0.94639] => Worst<<Loss>>[0.38413] Found at [  0,  7]\n","[  0, 17] TokenID[[460, 207, 207, 207, 207, 207]] TokensDesc[evidence the the the the the] Loss[0.38074] Acc[0.90027] Prec[1.0] Recall[0.90027] F1[0.94743] => Worst<<Loss>>[0.38413] Found at [  0,  7]\n","[  0, 18] TokenID[[7716, 207, 207, 207, 207, 207]] TokensDesc[inaction the the the the the] Loss[0.20667] Acc[0.95274] Prec[1.0] Recall[0.95274] F1[0.97577] => Worst<<Loss>>[0.38413] Found at [  0,  7]\n","[  0, 19] TokenID[[2399, 207, 207, 207, 207, 207]] TokensDesc[negotiations the the the the the] Loss[0.31762] Acc[0.92567] Prec[1.0] Recall[0.92567] F1[0.96129] => Worst<<Loss>>[0.38413] Found at [  0,  7]\n","[  0, 20] TokenID[[14391, 207, 207, 207, 207, 207]] TokensDesc[myself the the the the the] Loss[0.28926] Acc[0.93216] Prec[1.0] Recall[0.93216] F1[0.96481] => Worst<<Loss>>[0.38413] Found at [  0,  7]\n","[  0, 21] TokenID[[3566, 207, 207, 207, 207, 207]] TokensDesc[email the the the the the] Loss[0.40847] Acc[0.89937] Prec[1.0] Recall[0.89937] F1[0.94693] => Worst<<Loss>>[0.40847] Found at [  0, 21]\n","[  0, 22] TokenID[[3641, 207, 207, 207, 207, 207]] TokensDesc[encourage the the the the the] Loss[0.36101] Acc[0.90188] Prec[1.0] Recall[0.90188] F1[0.94836] => Worst<<Loss>>[0.40847] Found at [  0, 21]\n","[  0, 23] TokenID[[266, 207, 207, 207, 207, 207]] TokensDesc[law the the the the the] Loss[0.24327] Acc[0.9419] Prec[1.0] Recall[0.9419] F1[0.97003] => Worst<<Loss>>[0.40847] Found at [  0, 21]\n","[  0, 24] TokenID[[1216, 207, 207, 207, 207, 207]] TokensDesc[your the the the the the] Loss[0.31292] Acc[0.92168] Prec[1.0] Recall[0.92168] F1[0.95915] => Worst<<Loss>>[0.40847] Found at [  0, 21]\n","[  0, 25] TokenID[[6665, 207, 207, 207, 207, 207]] TokensDesc[excuse the the the the the] Loss[0.37955] Acc[0.89562] Prec[1.0] Recall[0.89562] F1[0.94482] => Worst<<Loss>>[0.40847] Found at [  0, 21]\n","[  0, 26] TokenID[[4084, 207, 207, 207, 207, 207]] TokensDesc[negotiation the the the the the] Loss[0.32436] Acc[0.92282] Prec[1.0] Recall[0.92282] F1[0.95975] => Worst<<Loss>>[0.40847] Found at [  0, 21]\n","[  0, 27] TokenID[[3705, 207, 207, 207, 207, 207]] TokensDesc[settle the the the the the] Loss[0.33875] Acc[0.91635] Prec[1.0] Recall[0.91635] F1[0.95627] => Worst<<Loss>>[0.40847] Found at [  0, 21]\n","[  0, 28] TokenID[[11858, 207, 207, 207, 207, 207]] TokensDesc[telegram the the the the the] Loss[0.5282] Acc[0.83549] Prec[1.0] Recall[0.83549] F1[0.91033] => Worst<<Loss>>[0.5282] Found at [  0, 28]\n","[  0, 29] TokenID[[753, 207, 207, 207, 207, 207]] TokensDesc[agree the the the the the] Loss[0.19644] Acc[0.95557] Prec[1.0] Recall[0.95557] F1[0.97724] => Worst<<Loss>>[0.5282] Found at [  0, 28]\n","[  0, 30] TokenID[[2921, 207, 207, 207, 207, 207]] TokensDesc[mind the the the the the] Loss[0.34255] Acc[0.91738] Prec[1.0] Recall[0.91738] F1[0.95679] => Worst<<Loss>>[0.5282] Found at [  0, 28]\n","[  0, 31] TokenID[[8397, 207, 207, 207, 207, 207]] TokensDesc[honour the the the the the] Loss[0.32486] Acc[0.92522] Prec[1.0] Recall[0.92522] F1[0.96105] => Worst<<Loss>>[0.5282] Found at [  0, 28]\n","[  0, 32] TokenID[[1151, 207, 207, 207, 207, 207]] TokensDesc[argument the the the the the] Loss[0.36224] Acc[0.91088] Prec[1.0] Recall[0.91088] F1[0.95318] => Worst<<Loss>>[0.5282] Found at [  0, 28]\n","[  0, 33] TokenID[[15577, 207, 207, 207, 207, 207]] TokensDesc[affirmation the the the the the] Loss[0.37895] Acc[0.90749] Prec[1.0] Recall[0.90749] F1[0.95139] => Worst<<Loss>>[0.5282] Found at [  0, 28]\n","[  0, 34] TokenID[[1412, 207, 207, 207, 207, 207]] TokensDesc[language the the the the the] Loss[0.35513] Acc[0.91587] Prec[1.0] Recall[0.91587] F1[0.95597] => Worst<<Loss>>[0.5282] Found at [  0, 28]\n","[  0, 35] TokenID[[371, 207, 207, 207, 207, 207]] TokensDesc[judgment the the the the the] Loss[0.2847] Acc[0.93172] Prec[1.0] Recall[0.93172] F1[0.96456] => Worst<<Loss>>[0.5282] Found at [  0, 28]\n","[  0, 36] TokenID[[699, 207, 207, 207, 207, 207]] TokensDesc[writing the the the the the] Loss[0.36157] Acc[0.90897] Prec[1.0] Recall[0.90897] F1[0.95218] => Worst<<Loss>>[0.5282] Found at [  0, 28]\n","[  0, 37] TokenID[[4065, 207, 207, 207, 207, 207]] TokensDesc[signatures the the the the the] Loss[0.32292] Acc[0.92607] Prec[1.0] Recall[0.92607] F1[0.96154] => Worst<<Loss>>[0.5282] Found at [  0, 28]\n","[  0, 38] TokenID[[7396, 207, 207, 207, 207, 207]] TokensDesc[bench the the the the the] Loss[0.32327] Acc[0.92464] Prec[1.0] Recall[0.92464] F1[0.96072] => Worst<<Loss>>[0.5282] Found at [  0, 28]\n","[  0, 39] TokenID[[1865, 207, 207, 207, 207, 207]] TokensDesc[physical the the the the the] Loss[0.30542] Acc[0.92365] Prec[1.0] Recall[0.92365] F1[0.96024] => Worst<<Loss>>[0.5282] Found at [  0, 28]\n","[  0, 40] TokenID[[2012, 207, 207, 207, 207, 207]] TokensDesc[early the the the the the] Loss[0.3078] Acc[0.93002] Prec[1.0] Recall[0.93002] F1[0.96364] => Worst<<Loss>>[0.5282] Found at [  0, 28]\n","[  0, 41] TokenID[[5818, 207, 207, 207, 207, 207]] TokensDesc[plead the the the the the] Loss[0.3058] Acc[0.92778] Prec[1.0] Recall[0.92778] F1[0.96245] => Worst<<Loss>>[0.5282] Found at [  0, 28]\n","[  0, 42] TokenID[[1435, 207, 207, 207, 207, 207]] TokensDesc[earlier the the the the the] Loss[0.31554] Acc[0.925] Prec[1.0] Recall[0.925] F1[0.96097] => Worst<<Loss>>[0.5282] Found at [  0, 28]\n","[  0, 43] TokenID[[1434, 207, 207, 207, 207, 207]] TokensDesc[arbitration the the the the the] Loss[0.38231] Acc[0.91433] Prec[1.0] Recall[0.91433] F1[0.95506] => Worst<<Loss>>[0.5282] Found at [  0, 28]\n","[  0, 44] TokenID[[377, 207, 207, 207, 207, 207]] TokensDesc[event the the the the the] Loss[0.29564] Acc[0.92864] Prec[1.0] Recall[0.92864] F1[0.96294] => Worst<<Loss>>[0.5282] Found at [  0, 28]\n","[  0, 45] TokenID[[6054, 207, 207, 207, 207, 207]] TokensDesc[elimination the the the the the] Loss[0.27994] Acc[0.93135] Prec[1.0] Recall[0.93135] F1[0.96438] => Worst<<Loss>>[0.5282] Found at [  0, 28]\n","[  0, 46] TokenID[[579, 207, 207, 207, 207, 207]] TokensDesc[performance the the the the the] Loss[0.27654] Acc[0.93786] Prec[1.0] Recall[0.93786] F1[0.96786] => Worst<<Loss>>[0.5282] Found at [  0, 28]\n","[  0, 47] TokenID[[11734, 207, 207, 207, 207, 207]] TokensDesc[silence the the the the the] Loss[0.34886] Acc[0.91658] Prec[1.0] Recall[0.91658] F1[0.95634] => Worst<<Loss>>[0.5282] Found at [  0, 28]\n","[  0, 48] TokenID[[436, 207, 207, 207, 207, 207]] TokensDesc[written the the the the the] Loss[0.33758] Acc[0.91731] Prec[1.0] Recall[0.91731] F1[0.95677] => Worst<<Loss>>[0.5282] Found at [  0, 28]\n","[  0, 49] TokenID[[3552, 207, 207, 207, 207, 207]] TokensDesc[optional the the the the the] Loss[0.31161] Acc[0.92878] Prec[1.0] Recall[0.92878] F1[0.96297] => Worst<<Loss>>[0.5282] Found at [  0, 28]\n","[  0, 50] TokenID[[2679, 207, 207, 207, 207, 207]] TokensDesc[arbitrator the the the the the] Loss[0.38841] Acc[0.90758] Prec[1.0] Recall[0.90758] F1[0.95135] => Worst<<Loss>>[0.5282] Found at [  0, 28]\n","[  0, 51] TokenID[[1791, 207, 207, 207, 207, 207]] TokensDesc[just the the the the the] Loss[0.37167] Acc[0.90736] Prec[1.0] Recall[0.90736] F1[0.95131] => Worst<<Loss>>[0.5282] Found at [  0, 28]\n","[  0, 52] TokenID[[14479, 207, 207, 207, 207, 207]] TokensDesc[prayer the the the the the] Loss[0.33102] Acc[0.92283] Prec[1.0] Recall[0.92283] F1[0.95974] => Worst<<Loss>>[0.5282] Found at [  0, 28]\n","[  0, 53] TokenID[[2527, 207, 207, 207, 207, 207]] TokensDesc[automatically the the the the the] Loss[0.30647] Acc[0.93167] Prec[1.0] Recall[0.93167] F1[0.96453] => Worst<<Loss>>[0.5282] Found at [  0, 28]\n","[  0, 54] TokenID[[2823, 207, 207, 207, 207, 207]] TokensDesc[propose the the the the the] Loss[0.34168] Acc[0.91406] Prec[1.0] Recall[0.91406] F1[0.95501] => Worst<<Loss>>[0.5282] Found at [  0, 28]\n","[  0, 55] TokenID[[3053, 207, 207, 207, 207, 207]] TokensDesc[denial the the the the the] Loss[0.3533] Acc[0.90646] Prec[1.0] Recall[0.90646] F1[0.95081] => Worst<<Loss>>[0.5282] Found at [  0, 28]\n","[  0, 56] TokenID[[19090, 207, 207, 207, 207, 207]] TokensDesc[unif the the the the the] Loss[0.26325] Acc[0.94092] Prec[1.0] Recall[0.94092] F1[0.96951] => Worst<<Loss>>[0.5282] Found at [  0, 28]\n","[  0, 57] TokenID[[18658, 207, 207, 207, 207, 207]] TokensDesc[meri the the the the the] Loss[0.27711] Acc[0.93748] Prec[1.0] Recall[0.93748] F1[0.96766] => Worst<<Loss>>[0.5282] Found at [  0, 28]\n","[  0, 58] TokenID[[9225, 207, 207, 207, 207, 207]] TokensDesc[rib the the the the the] Loss[0.29926] Acc[0.93196] Prec[1.0] Recall[0.93196] F1[0.9647] => Worst<<Loss>>[0.5282] Found at [  0, 28]\n","[  0, 59] TokenID[[1059, 207, 207, 207, 207, 207]] TokensDesc[dispute the the the the the] Loss[0.29469] Acc[0.92711] Prec[1.0] Recall[0.92711] F1[0.96207] => Worst<<Loss>>[0.5282] Found at [  0, 28]\n","[  0, 60] TokenID[[9978, 207, 207, 207, 207, 207]] TokensDesc[arbitrate the the the the the] Loss[0.31121] Acc[0.92284] Prec[1.0] Recall[0.92284] F1[0.95976] => Worst<<Loss>>[0.5282] Found at [  0, 28]\n","[  0, 61] TokenID[[25625, 207, 207, 207, 207, 207]] TokensDesc[exec the the the the the] Loss[0.26792] Acc[0.94075] Prec[1.0] Recall[0.94075] F1[0.96941] => Worst<<Loss>>[0.5282] Found at [  0, 28]\n","[  0, 62] TokenID[[3770, 207, 207, 207, 207, 207]] TokensDesc[relieve the the the the the] Loss[0.32375] Acc[0.91653] Prec[1.0] Recall[0.91653] F1[0.9563] => Worst<<Loss>>[0.5282] Found at [  0, 28]\n","[  0, 63] TokenID[[15476, 207, 207, 207, 207, 207]] TokensDesc[proffer the the the the the] Loss[0.35552] Acc[0.91006] Prec[1.0] Recall[0.91006] F1[0.95276] => Worst<<Loss>>[0.5282] Found at [  0, 28]\n","[  0, 64] TokenID[[2957, 207, 207, 207, 207, 207]] TokensDesc[dismissal the the the the the] Loss[0.32384] Acc[0.91619] Prec[1.0] Recall[0.91619] F1[0.95616] => Worst<<Loss>>[0.5282] Found at [  0, 28]\n","[  0, 65] TokenID[[30517, 207, 207, 207, 207, 207]] TokensDesc[ad the the the the the] Loss[0.286] Acc[0.9361] Prec[1.0] Recall[0.9361] F1[0.96691] => Worst<<Loss>>[0.5282] Found at [  0, 28]\n","[  0, 66] TokenID[[17000, 207, 207, 207, 207, 207]] TokensDesc[intermedi the the the the the] Loss[0.30442] Acc[0.93163] Prec[1.0] Recall[0.93163] F1[0.96451] => Worst<<Loss>>[0.5282] Found at [  0, 28]\n","[  0, 67] TokenID[[2698, 207, 207, 207, 207, 207]] TokensDesc[remand the the the the the] Loss[0.30886] Acc[0.92415] Prec[1.0] Recall[0.92415] F1[0.96045] => Worst<<Loss>>[0.5282] Found at [  0, 28]\n","[  0, 68] TokenID[[1086, 207, 207, 207, 207, 207]] TokensDesc[perform the the the the the] Loss[0.29131] Acc[0.93307] Prec[1.0] Recall[0.93307] F1[0.9653] => Worst<<Loss>>[0.5282] Found at [  0, 28]\n","[  0, 69] TokenID[[3643, 207, 207, 207, 207, 207]] TokensDesc[181 the the the the the] Loss[0.25626] Acc[0.94229] Prec[1.0] Recall[0.94229] F1[0.97022] => Worst<<Loss>>[0.5282] Found at [  0, 28]\n","[  0, 70] TokenID[[4270, 207, 207, 207, 207, 207]] TokensDesc[pleas the the the the the] Loss[0.2946] Acc[0.93339] Prec[1.0] Recall[0.93339] F1[0.96546] => Worst<<Loss>>[0.5282] Found at [  0, 28]\n","[  0, 71] TokenID[[18191, 207, 207, 207, 207, 207]] TokensDesc[msr the the the the the] Loss[0.27094] Acc[0.94278] Prec[1.0] Recall[0.94278] F1[0.97049] => Worst<<Loss>>[0.5282] Found at [  0, 28]\n","[  0, 72] TokenID[[9828, 207, 207, 207, 207, 207]] TokensDesc[victor the the the the the] Loss[0.31831] Acc[0.92629] Prec[1.0] Recall[0.92629] F1[0.96161] => Worst<<Loss>>[0.5282] Found at [  0, 28]\n","[  0, 73] TokenID[[986, 207, 207, 207, 207, 207]] TokensDesc[courts the the the the the] Loss[0.23666] Acc[0.94748] Prec[1.0] Recall[0.94748] F1[0.97298] => Worst<<Loss>>[0.5282] Found at [  0, 28]\n","[  0, 74] TokenID[[8436, 207, 207, 207, 207, 207]] TokensDesc[ren the the the the the] Loss[0.29482] Acc[0.92979] Prec[1.0] Recall[0.92979] F1[0.96353] => Worst<<Loss>>[0.5282] Found at [  0, 28]\n","[  0, 75] TokenID[[8873, 207, 207, 207, 207, 207]] TokensDesc[verbal the the the the the] Loss[0.30937] Acc[0.93475] Prec[1.0] Recall[0.93475] F1[0.96618] => Worst<<Loss>>[0.5282] Found at [  0, 28]\n","[  0, 76] TokenID[[378, 207, 207, 207, 207, 207]] TokensDesc[otherwise the the the the the] Loss[0.25666] Acc[0.94345] Prec[1.0] Recall[0.94345] F1[0.97085] => Worst<<Loss>>[0.5282] Found at [  0, 28]\n","[  0, 77] TokenID[[20750, 207, 207, 207, 207, 207]] TokensDesc[yoursel the the the the the] Loss[0.27986] Acc[0.93216] Prec[1.0] Recall[0.93216] F1[0.9648] => Worst<<Loss>>[0.5282] Found at [  0, 28]\n","[  0, 78] TokenID[[1263, 207, 207, 207, 207, 207]] TokensDesc[judicial the the the the the] Loss[0.27361] Acc[0.9406] Prec[1.0] Recall[0.9406] F1[0.96933] => Worst<<Loss>>[0.5282] Found at [  0, 28]\n","[  0, 79] TokenID[[1314, 207, 207, 207, 207, 207]] TokensDesc[text the the the the the] Loss[0.34735] Acc[0.91783] Prec[1.0] Recall[0.91783] F1[0.95706] => Worst<<Loss>>[0.5282] Found at [  0, 28]\n","[  0, 80] TokenID[[6410, 207, 207, 207, 207, 207]] TokensDesc[honor the the the the the] Loss[0.41376] Acc[0.89051] Prec[1.0] Recall[0.89051] F1[0.94198] => Worst<<Loss>>[0.5282] Found at [  0, 28]\n","[  0, 81] TokenID[[18451, 207, 207, 207, 207, 207]] TokensDesc[verbally the the the the the] Loss[0.28744] Acc[0.93902] Prec[1.0] Recall[0.93902] F1[0.96848] => Worst<<Loss>>[0.5282] Found at [  0, 28]\n","[  0, 82] TokenID[[9148, 207, 207, 207, 207, 207]] TokensDesc[arithmetic the the the the the] Loss[0.29448] Acc[0.93214] Prec[1.0] Recall[0.93214] F1[0.9648] => Worst<<Loss>>[0.5282] Found at [  0, 28]\n","[  0, 83] TokenID[[13048, 207, 207, 207, 207, 207]] TokensDesc[claudia the the the the the] Loss[0.28155] Acc[0.93769] Prec[1.0] Recall[0.93769] F1[0.96777] => Worst<<Loss>>[0.5282] Found at [  0, 28]\n","[  0, 84] TokenID[[454, 207, 207, 207, 207, 207]] TokensDesc[provision the the the the the] Loss[0.3254] Acc[0.92429] Prec[1.0] Recall[0.92429] F1[0.96056] => Worst<<Loss>>[0.5282] Found at [  0, 28]\n","[  0, 85] TokenID[[474, 207, 207, 207, 207, 207]] TokensDesc[termination the the the the the] Loss[0.26892] Acc[0.93618] Prec[1.0] Recall[0.93618] F1[0.96695] => Worst<<Loss>>[0.5282] Found at [  0, 28]\n","[  0, 86] TokenID[[3750, 207, 207, 207, 207, 207]] TokensDesc[attempted the the the the the] Loss[0.30369] Acc[0.93003] Prec[1.0] Recall[0.93003] F1[0.96365] => Worst<<Loss>>[0.5282] Found at [  0, 28]\n","[  0, 87] TokenID[[2340, 207, 207, 207, 207, 207]] TokensDesc[omission the the the the the] Loss[0.32492] Acc[0.92267] Prec[1.0] Recall[0.92267] F1[0.95967] => Worst<<Loss>>[0.5282] Found at [  0, 28]\n","[  0, 88] TokenID[[3437, 207, 207, 207, 207, 207]] TokensDesc[fifteen the the the the the] Loss[0.29415] Acc[0.9338] Prec[1.0] Recall[0.9338] F1[0.96569] => Worst<<Loss>>[0.5282] Found at [  0, 28]\n","[  0, 89] TokenID[[3575, 207, 207, 207, 207, 207]] TokensDesc[unanimously the the the the the] Loss[0.26018] Acc[0.93996] Prec[1.0] Recall[0.93996] F1[0.96898] => Worst<<Loss>>[0.5282] Found at [  0, 28]\n","[  0, 90] TokenID[[2624, 207, 207, 207, 207, 207]] TokensDesc[settled the the the the the] Loss[0.26183] Acc[0.94022] Prec[1.0] Recall[0.94022] F1[0.96913] => Worst<<Loss>>[0.5282] Found at [  0, 28]\n","[  0, 91] TokenID[[6966, 207, 207, 207, 207, 207]] TokensDesc[god the the the the the] Loss[0.27691] Acc[0.94098] Prec[1.0] Recall[0.94098] F1[0.96953] => Worst<<Loss>>[0.5282] Found at [  0, 28]\n","[  0, 92] TokenID[[21649, 207, 207, 207, 207, 207]] TokensDesc[parol the the the the the] Loss[0.35158] Acc[0.91581] Prec[1.0] Recall[0.91581] F1[0.95588] => Worst<<Loss>>[0.5282] Found at [  0, 28]\n","[  0, 93] TokenID[[6247, 207, 207, 207, 207, 207]] TokensDesc[kar the the the the the] Loss[0.28234] Acc[0.93772] Prec[1.0] Recall[0.93772] F1[0.96779] => Worst<<Loss>>[0.5282] Found at [  0, 28]\n","[  0, 94] TokenID[[10879, 207, 207, 207, 207, 207]] TokensDesc[sunoco the the the the the] Loss[0.24606] Acc[0.94526] Prec[1.0] Recall[0.94526] F1[0.9718] => Worst<<Loss>>[0.5282] Found at [  0, 28]\n","[  0, 95] TokenID[[21358, 207, 207, 207, 207, 207]] TokensDesc[buc the the the the the] Loss[0.2737] Acc[0.93748] Prec[1.0] Recall[0.93748] F1[0.96766] => Worst<<Loss>>[0.5282] Found at [  0, 28]\n","[  0, 96] TokenID[[5617, 207, 207, 207, 207, 207]] TokensDesc[repeatedly the the the the the] Loss[0.23598] Acc[0.94631] Prec[1.0] Recall[0.94631] F1[0.97237] => Worst<<Loss>>[0.5282] Found at [  0, 28]\n","[  0, 97] TokenID[[1072, 207, 207, 207, 207, 207]] TokensDesc[expressly the the the the the] Loss[0.25943] Acc[0.93893] Prec[1.0] Recall[0.93893] F1[0.96845] => Worst<<Loss>>[0.5282] Found at [  0, 28]\n","[  0, 98] TokenID[[1753, 207, 207, 207, 207, 207]] TokensDesc[acceptance the the the the the] Loss[0.31474] Acc[0.92802] Prec[1.0] Recall[0.92802] F1[0.96258] => Worst<<Loss>>[0.5282] Found at [  0, 28]\n","[  0, 99] TokenID[[1364, 207, 207, 207, 207, 207]] TokensDesc[money the the the the the] Loss[0.33071] Acc[0.91735] Prec[1.0] Recall[0.91735] F1[0.95679] => Worst<<Loss>>[0.5282] Found at [  0, 28]\n","Worst loss 0.5282011200343409 with candidates [11858, 207, 207, 207, 207, 207] in the 0-iteration with tokens [telegram the the the the the]\n","candidates [29173 27313 10606 28022  7856 22233 17297  4152 13301 16808 20840 10357\n"," 27960  7320 16568  8491 22688  7405 20858  9085 14844 24164 25315  4912\n"," 15072 17986 23902 17879 13577 11160  5735  7843  6242 22421  4577 29132\n"," 26913 26005 21418 24405 24453  6189  8945 12763 22675 12592  9235 14791\n","  6889  8863 23473 23422 22481  6259  7140 13746 10052 21086 12649 27920\n"," 14202 12287 19990 27002 16681 21614  9667 14847 24766  4022  7652 26816\n"," 29182 20141 28039  7906  5990 30466 23197 18496 10085 26831 19069 23329\n"," 10091  8516 24299 15677 23904 10889 24578 20417 26059 25005 11682 27976\n"," 26019  7255 21311  5376]\n","[  1,  0] TokenID[[11858, 29173, 207, 207, 207, 207]] TokensDesc[telegramigra the the the the] Loss[0.48294] Acc[0.86988] Prec[1.0] Recall[0.86988] F1[0.93032] => Worst<<Loss>>[0.5282] Found at [  0, 28]\n","[  1,  1] TokenID[[11858, 27313, 207, 207, 207, 207]] TokensDesc[telegram penetrat the the the the] Loss[0.48476] Acc[0.86592] Prec[1.0] Recall[0.86592] F1[0.92805] => Worst<<Loss>>[0.5282] Found at [  0, 28]\n","[  1,  2] TokenID[[11858, 10606, 207, 207, 207, 207]] TokensDesc[telegram phenomen the the the the] Loss[0.43596] Acc[0.88463] Prec[1.0] Recall[0.88463] F1[0.93871] => Worst<<Loss>>[0.5282] Found at [  0, 28]\n","[  1,  3] TokenID[[11858, 28022, 207, 207, 207, 207]] TokensDesc[telegram blurr the the the the] Loss[0.434] Acc[0.89151] Prec[1.0] Recall[0.89151] F1[0.94255] => Worst<<Loss>>[0.5282] Found at [  0, 28]\n","[  1,  4] TokenID[[11858, 7856, 207, 207, 207, 207]] TokensDesc[telegram widely the the the the] Loss[0.4298] Acc[0.88525] Prec[1.0] Recall[0.88525] F1[0.93909] => Worst<<Loss>>[0.5282] Found at [  0, 28]\n","[  1,  5] TokenID[[11858, 22233, 207, 207, 207, 207]] TokensDesc[telegram decor the the the the] Loss[0.47472] Acc[0.86952] Prec[1.0] Recall[0.86952] F1[0.93015] => Worst<<Loss>>[0.5282] Found at [  0, 28]\n","[  1,  6] TokenID[[11858, 17297, 207, 207, 207, 207]] TokensDesc[telegram conservative the the the the] Loss[0.47803] Acc[0.87268] Prec[1.0] Recall[0.87268] F1[0.93194] => Worst<<Loss>>[0.5282] Found at [  0, 28]\n","[  1,  7] TokenID[[11858, 4152, 207, 207, 207, 207]] TokensDesc[telegram across the the the the] Loss[0.4364] Acc[0.89181] Prec[1.0] Recall[0.89181] F1[0.94274] => Worst<<Loss>>[0.5282] Found at [  0, 28]\n","[  1,  8] TokenID[[11858, 13301, 207, 207, 207, 207]] TokensDesc[telegram steady the the the the] Loss[0.47208] Acc[0.87052] Prec[1.0] Recall[0.87052] F1[0.93074] => Worst<<Loss>>[0.5282] Found at [  0, 28]\n","[  1,  9] TokenID[[11858, 16808, 207, 207, 207, 207]] TokensDesc[telegram vari the the the the] Loss[0.46215] Acc[0.87138] Prec[1.0] Recall[0.87138] F1[0.9312] => Worst<<Loss>>[0.5282] Found at [  0, 28]\n","[  1, 10] TokenID[[11858, 20840, 207, 207, 207, 207]] TokensDesc[telegram steep the the the the] Loss[0.48036] Acc[0.86006] Prec[1.0] Recall[0.86006] F1[0.92472] => Worst<<Loss>>[0.5282] Found at [  0, 28]\n","[  1, 11] TokenID[[11858, 10357, 207, 207, 207, 207]] TokensDesc[telegram widespread the the the the] Loss[0.34546] Acc[0.91296] Prec[1.0] Recall[0.91296] F1[0.95443] => Worst<<Loss>>[0.5282] Found at [  0, 28]\n","[  1, 12] TokenID[[11858, 27960, 207, 207, 207, 207]] TokensDesc[telegrampene the the the the] Loss[0.51713] Acc[0.85806] Prec[1.0] Recall[0.85806] F1[0.92352] => Worst<<Loss>>[0.5282] Found at [  0, 28]\n","[  1, 13] TokenID[[11858, 7320, 207, 207, 207, 207]] TokensDesc[telegram flexi the the the the] Loss[0.43689] Acc[0.88765] Prec[1.0] Recall[0.88765] F1[0.94042] => Worst<<Loss>>[0.5282] Found at [  0, 28]\n","[  1, 14] TokenID[[11858, 16568, 207, 207, 207, 207]] TokensDesc[telegramteri the the the the] Loss[0.4878] Acc[0.86166] Prec[1.0] Recall[0.86166] F1[0.92564] => Worst<<Loss>>[0.5282] Found at [  0, 28]\n","[  1, 15] TokenID[[11858, 8491, 207, 207, 207, 207]] TokensDesc[telegram heavi the the the the] Loss[0.48408] Acc[0.86612] Prec[1.0] Recall[0.86612] F1[0.92818] => Worst<<Loss>>[0.5282] Found at [  0, 28]\n","[  1, 16] TokenID[[11858, 22688, 207, 207, 207, 207]] TokensDesc[telegram nud the the the the] Loss[0.44071] Acc[0.88941] Prec[1.0] Recall[0.88941] F1[0.94138] => Worst<<Loss>>[0.5282] Found at [  0, 28]\n","[  1, 17] TokenID[[11858, 7405, 207, 207, 207, 207]] TokensDesc[telegram arab the the the the] Loss[0.51136] Acc[0.84882] Prec[1.0] Recall[0.84882] F1[0.91817] => Worst<<Loss>>[0.5282] Found at [  0, 28]\n","[  1, 18] TokenID[[11858, 20858, 207, 207, 207, 207]] TokensDesc[telegram shapi the the the the] Loss[0.4862] Acc[0.86768] Prec[1.0] Recall[0.86768] F1[0.92908] => Worst<<Loss>>[0.5282] Found at [  0, 28]\n","[  1, 19] TokenID[[11858, 9085, 207, 207, 207, 207]] TokensDesc[telegram sensitivit the the the the] Loss[0.43612] Acc[0.88737] Prec[1.0] Recall[0.88737] F1[0.94026] => Worst<<Loss>>[0.5282] Found at [  0, 28]\n","[  1, 20] TokenID[[11858, 14844, 207, 207, 207, 207]] TokensDesc[telegram masse the the the the] Loss[0.47839] Acc[0.86699] Prec[1.0] Recall[0.86699] F1[0.92871] => Worst<<Loss>>[0.5282] Found at [  0, 28]\n","[  1, 21] TokenID[[11858, 24164, 207, 207, 207, 207]] TokensDesc[telegram nasa the the the the] Loss[0.45524] Acc[0.87953] Prec[1.0] Recall[0.87953] F1[0.93581] => Worst<<Loss>>[0.5282] Found at [  0, 28]\n","[  1, 22] TokenID[[11858, 25315, 207, 207, 207, 207]] TokensDesc[telegram prone the the the the] Loss[0.43088] Acc[0.88977] Prec[1.0] Recall[0.88977] F1[0.94161] => Worst<<Loss>>[0.5282] Found at [  0, 28]\n","[  1, 23] TokenID[[11858, 4912, 207, 207, 207, 207]] TokensDesc[telegram harmonis the the the the] Loss[0.47033] Acc[0.87902] Prec[1.0] Recall[0.87902] F1[0.93556] => Worst<<Loss>>[0.5282] Found at [  0, 28]\n","[  1, 24] TokenID[[11858, 15072, 207, 207, 207, 207]] TokensDesc[telegram dramatic the the the the] Loss[0.44258] Acc[0.88576] Prec[1.0] Recall[0.88576] F1[0.93936] => Worst<<Loss>>[0.5282] Found at [  0, 28]\n","[  1, 25] TokenID[[11858, 17986, 207, 207, 207, 207]] TokensDesc[telegram crystallin the the the the] Loss[0.50346] Acc[0.85971] Prec[1.0] Recall[0.85971] F1[0.92445] => Worst<<Loss>>[0.5282] Found at [  0, 28]\n","[  1, 26] TokenID[[11858, 23902, 207, 207, 207, 207]] TokensDesc[telegram jehova the the the the] Loss[0.4587] Acc[0.87694] Prec[1.0] Recall[0.87694] F1[0.93435] => Worst<<Loss>>[0.5282] Found at [  0, 28]\n","[  1, 27] TokenID[[11858, 17879, 207, 207, 207, 207]] TokensDesc[telegram loa the the the the] Loss[0.45113] Acc[0.88646] Prec[1.0] Recall[0.88646] F1[0.93975] => Worst<<Loss>>[0.5282] Found at [  0, 28]\n","[  1, 28] TokenID[[11858, 13577, 207, 207, 207, 207]] TokensDesc[telegram reprint the the the the] Loss[0.55684] Acc[0.83331] Prec[1.0] Recall[0.83331] F1[0.90904] => Worst<<Loss>>[0.55684] Found at [  1, 28]\n","[  1, 29] TokenID[[11858, 11160, 207, 207, 207, 207]] TokensDesc[telegram tendenc the the the the] Loss[0.47302] Acc[0.86351] Prec[1.0] Recall[0.86351] F1[0.92673] => Worst<<Loss>>[0.55684] Found at [  1, 28]\n","[  1, 30] TokenID[[11858, 5735, 207, 207, 207, 207]] TokensDesc[telegram felt the the the the] Loss[0.43865] Acc[0.87441] Prec[1.0] Recall[0.87441] F1[0.93294] => Worst<<Loss>>[0.55684] Found at [  1, 28]\n","[  1, 31] TokenID[[11858, 7843, 207, 207, 207, 207]] TokensDesc[telegram convergen the the the the] Loss[0.47942] Acc[0.86799] Prec[1.0] Recall[0.86799] F1[0.92928] => Worst<<Loss>>[0.55684] Found at [  1, 28]\n","[  1, 32] TokenID[[11858, 6242, 207, 207, 207, 207]] TokensDesc[telegram commonly the the the the] Loss[0.44451] Acc[0.88876] Prec[1.0] Recall[0.88876] F1[0.94102] => Worst<<Loss>>[0.55684] Found at [  1, 28]\n","[  1, 33] TokenID[[11858, 22421, 207, 207, 207, 207]] TokensDesc[telegram langu the the the the] Loss[0.5088] Acc[0.85645] Prec[1.0] Recall[0.85645] F1[0.92261] => Worst<<Loss>>[0.55684] Found at [  1, 28]\n","[  1, 34] TokenID[[11858, 4577, 207, 207, 207, 207]] TokensDesc[telegram concrete the the the the] Loss[0.46581] Acc[0.87028] Prec[1.0] Recall[0.87028] F1[0.93061] => Worst<<Loss>>[0.55684] Found at [  1, 28]\n","[  1, 35] TokenID[[11858, 29132, 207, 207, 207, 207]] TokensDesc[telegram deviat the the the the] Loss[0.48919] Acc[0.86738] Prec[1.0] Recall[0.86738] F1[0.9289] => Worst<<Loss>>[0.55684] Found at [  1, 28]\n","[  1, 36] TokenID[[11858, 26913, 207, 207, 207, 207]] TokensDesc[telegram ellip the the the the] Loss[0.47554] Acc[0.87203] Prec[1.0] Recall[0.87203] F1[0.93157] => Worst<<Loss>>[0.55684] Found at [  1, 28]\n","[  1, 37] TokenID[[11858, 26005, 207, 207, 207, 207]] TokensDesc[telegram moder the the the the] Loss[0.46357] Acc[0.87606] Prec[1.0] Recall[0.87606] F1[0.93384] => Worst<<Loss>>[0.55684] Found at [  1, 28]\n","[  1, 38] TokenID[[11858, 21418, 207, 207, 207, 207]] TokensDesc[telegram daylight the the the the] Loss[0.43236] Acc[0.88643] Prec[1.0] Recall[0.88643] F1[0.93971] => Worst<<Loss>>[0.55684] Found at [  1, 28]\n","[  1, 39] TokenID[[11858, 24405, 207, 207, 207, 207]] TokensDesc[telegram fasci the the the the] Loss[0.48021] Acc[0.86934] Prec[1.0] Recall[0.86934] F1[0.93003] => Worst<<Loss>>[0.55684] Found at [  1, 28]\n","[  1, 40] TokenID[[11858, 24453, 207, 207, 207, 207]] TokensDesc[telegram unpredictabl the the the the] Loss[0.44953] Acc[0.88346] Prec[1.0] Recall[0.88346] F1[0.93806] => Worst<<Loss>>[0.55684] Found at [  1, 28]\n","[  1, 41] TokenID[[11858, 6189, 207, 207, 207, 207]] TokensDesc[telegramurgenc the the the the] Loss[0.52628] Acc[0.84245] Prec[1.0] Recall[0.84245] F1[0.91443] => Worst<<Loss>>[0.55684] Found at [  1, 28]\n","[  1, 42] TokenID[[11858, 8945, 207, 207, 207, 207]] TokensDesc[telegram elastic the the the the] Loss[0.45212] Acc[0.87958] Prec[1.0] Recall[0.87958] F1[0.93584] => Worst<<Loss>>[0.55684] Found at [  1, 28]\n","[  1, 43] TokenID[[11858, 12763, 207, 207, 207, 207]] TokensDesc[telegram robust the the the the] Loss[0.45594] Acc[0.87918] Prec[1.0] Recall[0.87918] F1[0.93566] => Worst<<Loss>>[0.55684] Found at [  1, 28]\n","[  1, 44] TokenID[[11858, 22675, 207, 207, 207, 207]] TokensDesc[telegram cree the the the the] Loss[0.50452] Acc[0.8607] Prec[1.0] Recall[0.8607] F1[0.92506] => Worst<<Loss>>[0.55684] Found at [  1, 28]\n","[  1, 45] TokenID[[11858, 12592, 207, 207, 207, 207]] TokensDesc[telegram ideal the the the the] Loss[0.46242] Acc[0.87743] Prec[1.0] Recall[0.87743] F1[0.93466] => Worst<<Loss>>[0.55684] Found at [  1, 28]\n","[  1, 46] TokenID[[11858, 9235, 207, 207, 207, 207]] TokensDesc[telegram anywhere the the the the] Loss[0.43135] Acc[0.89149] Prec[1.0] Recall[0.89149] F1[0.94256] => Worst<<Loss>>[0.55684] Found at [  1, 28]\n","[  1, 47] TokenID[[11858, 14791, 207, 207, 207, 207]] TokensDesc[telegram tul the the the the] Loss[0.52539] Acc[0.84997] Prec[1.0] Recall[0.84997] F1[0.91882] => Worst<<Loss>>[0.55684] Found at [  1, 28]\n","[  1, 48] TokenID[[11858, 6889, 207, 207, 207, 207]] TokensDesc[telegram sharp the the the the] Loss[0.44566] Acc[0.88548] Prec[1.0] Recall[0.88548] F1[0.93917] => Worst<<Loss>>[0.55684] Found at [  1, 28]\n","[  1, 49] TokenID[[11858, 8863, 207, 207, 207, 207]] TokensDesc[telegram frequent the the the the] Loss[0.38108] Acc[0.90406] Prec[1.0] Recall[0.90406] F1[0.94952] => Worst<<Loss>>[0.55684] Found at [  1, 28]\n","[  1, 50] TokenID[[11858, 23473, 207, 207, 207, 207]] TokensDesc[telegramlato the the the the] Loss[0.49209] Acc[0.86438] Prec[1.0] Recall[0.86438] F1[0.92716] => Worst<<Loss>>[0.55684] Found at [  1, 28]\n","[  1, 51] TokenID[[11858, 23422, 207, 207, 207, 207]] TokensDesc[telegram couche the the the the] Loss[0.49008] Acc[0.86526] Prec[1.0] Recall[0.86526] F1[0.92771] => Worst<<Loss>>[0.55684] Found at [  1, 28]\n","[  1, 52] TokenID[[11858, 22481, 207, 207, 207, 207]] TokensDesc[telegram abundan the the the the] Loss[0.4678] Acc[0.87288] Prec[1.0] Recall[0.87288] F1[0.9321] => Worst<<Loss>>[0.55684] Found at [  1, 28]\n","[  1, 53] TokenID[[11858, 6259, 207, 207, 207, 207]] TokensDesc[telegram harmoniz the the the the] Loss[0.47827] Acc[0.86942] Prec[1.0] Recall[0.86942] F1[0.93007] => Worst<<Loss>>[0.55684] Found at [  1, 28]\n","[  1, 54] TokenID[[11858, 7140, 207, 207, 207, 207]] TokensDesc[telegram democracy the the the the] Loss[0.49847] Acc[0.86287] Prec[1.0] Recall[0.86287] F1[0.92631] => Worst<<Loss>>[0.55684] Found at [  1, 28]\n","[  1, 55] TokenID[[11858, 13746, 207, 207, 207, 207]] TokensDesc[telegram diversifi the the the the] Loss[0.50528] Acc[0.85238] Prec[1.0] Recall[0.85238] F1[0.92027] => Worst<<Loss>>[0.55684] Found at [  1, 28]\n","[  1, 56] TokenID[[11858, 10052, 207, 207, 207, 207]] TokensDesc[telegram persistent the the the the] Loss[0.41623] Acc[0.89284] Prec[1.0] Recall[0.89284] F1[0.94332] => Worst<<Loss>>[0.55684] Found at [  1, 28]\n","[  1, 57] TokenID[[11858, 21086, 207, 207, 207, 207]] TokensDesc[telegram adher the the the the] Loss[0.43452] Acc[0.89108] Prec[1.0] Recall[0.89108] F1[0.94233] => Worst<<Loss>>[0.55684] Found at [  1, 28]\n","[  1, 58] TokenID[[11858, 12649, 207, 207, 207, 207]] TokensDesc[telegram circulate the the the the] Loss[0.5215] Acc[0.85143] Prec[1.0] Recall[0.85143] F1[0.9197] => Worst<<Loss>>[0.55684] Found at [  1, 28]\n","[  1, 59] TokenID[[11858, 27920, 207, 207, 207, 207]] TokensDesc[telegram primate the the the the] Loss[0.52875] Acc[0.84614] Prec[1.0] Recall[0.84614] F1[0.9166] => Worst<<Loss>>[0.55684] Found at [  1, 28]\n","[  1, 60] TokenID[[11858, 14202, 207, 207, 207, 207]] TokensDesc[telegram evolve the the the the] Loss[0.39232] Acc[0.8953] Prec[1.0] Recall[0.8953] F1[0.94469] => Worst<<Loss>>[0.55684] Found at [  1, 28]\n","[  1, 61] TokenID[[11858, 12287, 207, 207, 207, 207]] TokensDesc[telegram granit the the the the] Loss[0.47554] Acc[0.86886] Prec[1.0] Recall[0.86886] F1[0.92975] => Worst<<Loss>>[0.55684] Found at [  1, 28]\n","[  1, 62] TokenID[[11858, 19990, 207, 207, 207, 207]] TokensDesc[telegram spat the the the the] Loss[0.49613] Acc[0.86247] Prec[1.0] Recall[0.86247] F1[0.9261] => Worst<<Loss>>[0.55684] Found at [  1, 28]\n","[  1, 63] TokenID[[11858, 27002, 207, 207, 207, 207]] TokensDesc[telegram shouted the the the the] Loss[0.4584] Acc[0.87965] Prec[1.0] Recall[0.87965] F1[0.93592] => Worst<<Loss>>[0.55684] Found at [  1, 28]\n","[  1, 64] TokenID[[11858, 16681, 207, 207, 207, 207]] TokensDesc[telegram elevate the the the the] Loss[0.42279] Acc[0.88896] Prec[1.0] Recall[0.88896] F1[0.94114] => Worst<<Loss>>[0.55684] Found at [  1, 28]\n","[  1, 65] TokenID[[11858, 21614, 207, 207, 207, 207]] TokensDesc[telegram stiff the the the the] Loss[0.46925] Acc[0.87632] Prec[1.0] Recall[0.87632] F1[0.93399] => Worst<<Loss>>[0.55684] Found at [  1, 28]\n","[  1, 66] TokenID[[11858, 9667, 207, 207, 207, 207]] TokensDesc[telegram emerge the the the the] Loss[0.41167] Acc[0.89726] Prec[1.0] Recall[0.89726] F1[0.94574] => Worst<<Loss>>[0.55684] Found at [  1, 28]\n","[  1, 67] TokenID[[11858, 14847, 207, 207, 207, 207]] TokensDesc[telegram meteorolog the the the the] Loss[0.55378] Acc[0.83473] Prec[1.0] Recall[0.83473] F1[0.90986] => Worst<<Loss>>[0.55684] Found at [  1, 28]\n","[  1, 68] TokenID[[11858, 24766, 207, 207, 207, 207]] TokensDesc[telegram saska the the the the] Loss[0.54791] Acc[0.83804] Prec[1.0] Recall[0.83804] F1[0.91181] => Worst<<Loss>>[0.55684] Found at [  1, 28]\n","[  1, 69] TokenID[[11858, 4022, 207, 207, 207, 207]] TokensDesc[telegram gd the the the the] Loss[0.47531] Acc[0.86358] Prec[1.0] Recall[0.86358] F1[0.92678] => Worst<<Loss>>[0.55684] Found at [  1, 28]\n","[  1, 70] TokenID[[11858, 7652, 207, 207, 207, 207]] TokensDesc[telegram progressive the the the the] Loss[0.41292] Acc[0.89494] Prec[1.0] Recall[0.89494] F1[0.94448] => Worst<<Loss>>[0.55684] Found at [  1, 28]\n","[  1, 71] TokenID[[11858, 26816, 207, 207, 207, 207]] TokensDesc[telegram seldo the the the the] Loss[0.48167] Acc[0.86153] Prec[1.0] Recall[0.86153] F1[0.92557] => Worst<<Loss>>[0.55684] Found at [  1, 28]\n","[  1, 72] TokenID[[11858, 29182, 207, 207, 207, 207]] TokensDesc[telegram infrequent the the the the] Loss[0.40218] Acc[0.89703] Prec[1.0] Recall[0.89703] F1[0.94565] => Worst<<Loss>>[0.55684] Found at [  1, 28]\n","[  1, 73] TokenID[[11858, 20141, 207, 207, 207, 207]] TokensDesc[telegram tighten the the the the] Loss[0.5185] Acc[0.8482] Prec[1.0] Recall[0.8482] F1[0.91781] => Worst<<Loss>>[0.55684] Found at [  1, 28]\n","[  1, 74] TokenID[[11858, 28039, 207, 207, 207, 207]] TokensDesc[telegram stoic the the the the] Loss[0.50509] Acc[0.85768] Prec[1.0] Recall[0.85768] F1[0.92334] => Worst<<Loss>>[0.55684] Found at [  1, 28]\n","[  1, 75] TokenID[[11858, 7906, 207, 207, 207, 207]] TokensDesc[telegram bread the the the the] Loss[0.45766] Acc[0.88473] Prec[1.0] Recall[0.88473] F1[0.93876] => Worst<<Loss>>[0.55684] Found at [  1, 28]\n","[  1, 76] TokenID[[11858, 5990, 207, 207, 207, 207]] TokensDesc[telegram stable the the the the] Loss[0.41624] Acc[0.89231] Prec[1.0] Recall[0.89231] F1[0.94303] => Worst<<Loss>>[0.55684] Found at [  1, 28]\n","[  1, 77] TokenID[[11858, 30466, 207, 207, 207, 207]] TokensDesc[telegram cyclop the the the the] Loss[0.51181] Acc[0.8484] Prec[1.0] Recall[0.8484] F1[0.91792] => Worst<<Loss>>[0.55684] Found at [  1, 28]\n","[  1, 78] TokenID[[11858, 23197, 207, 207, 207, 207]] TokensDesc[telegram aeronautic the the the the] Loss[0.52575] Acc[0.84305] Prec[1.0] Recall[0.84305] F1[0.91476] => Worst<<Loss>>[0.55684] Found at [  1, 28]\n","[  1, 79] TokenID[[11858, 18496, 207, 207, 207, 207]] TokensDesc[telegram porcel the the the the] Loss[0.53848] Acc[0.84957] Prec[1.0] Recall[0.84957] F1[0.91858] => Worst<<Loss>>[0.55684] Found at [  1, 28]\n","[  1, 80] TokenID[[11858, 10085, 207, 207, 207, 207]] TokensDesc[telegramcyclo the the the the] Loss[0.51501] Acc[0.85642] Prec[1.0] Recall[0.85642] F1[0.92257] => Worst<<Loss>>[0.55684] Found at [  1, 28]\n","[  1, 81] TokenID[[11858, 26831, 207, 207, 207, 207]] TokensDesc[telegram plunge the the the the] Loss[0.54623] Acc[0.84308] Prec[1.0] Recall[0.84308] F1[0.91478] => Worst<<Loss>>[0.55684] Found at [  1, 28]\n","[  1, 82] TokenID[[11858, 19069, 207, 207, 207, 207]] TokensDesc[telegram pali the the the the] Loss[0.49605] Acc[0.86137] Prec[1.0] Recall[0.86137] F1[0.92545] => Worst<<Loss>>[0.55684] Found at [  1, 28]\n","[  1, 83] TokenID[[11858, 23329, 207, 207, 207, 207]] TokensDesc[telegram voiced the the the the] Loss[0.42399] Acc[0.88575] Prec[1.0] Recall[0.88575] F1[0.93937] => Worst<<Loss>>[0.55684] Found at [  1, 28]\n","[  1, 84] TokenID[[11858, 10091, 207, 207, 207, 207]] TokensDesc[telegram wave the the the the] Loss[0.45255] Acc[0.88038] Prec[1.0] Recall[0.88038] F1[0.93631] => Worst<<Loss>>[0.55684] Found at [  1, 28]\n","[  1, 85] TokenID[[11858, 8516, 207, 207, 207, 207]] TokensDesc[telegram static the the the the] Loss[0.47406] Acc[0.87778] Prec[1.0] Recall[0.87778] F1[0.93485] => Worst<<Loss>>[0.55684] Found at [  1, 28]\n","[  1, 86] TokenID[[11858, 24299, 207, 207, 207, 207]] TokensDesc[telegram slavi the the the the] Loss[0.45388] Acc[0.87119] Prec[1.0] Recall[0.87119] F1[0.93112] => Worst<<Loss>>[0.55684] Found at [  1, 28]\n","[  1, 87] TokenID[[11858, 15677, 207, 207, 207, 207]] TokensDesc[telegram depress the the the the] Loss[0.47816] Acc[0.86752] Prec[1.0] Recall[0.86752] F1[0.92904] => Worst<<Loss>>[0.55684] Found at [  1, 28]\n","[  1, 88] TokenID[[11858, 23904, 207, 207, 207, 207]] TokensDesc[telegram graphical the the the the] Loss[0.49492] Acc[0.85797] Prec[1.0] Recall[0.85797] F1[0.92352] => Worst<<Loss>>[0.55684] Found at [  1, 28]\n","[  1, 89] TokenID[[11858, 10889, 207, 207, 207, 207]] TokensDesc[telegram thermo the the the the] Loss[0.51918] Acc[0.85655] Prec[1.0] Recall[0.85655] F1[0.92264] => Worst<<Loss>>[0.55684] Found at [  1, 28]\n","[  1, 90] TokenID[[11858, 24578, 207, 207, 207, 207]] TokensDesc[telegram immersi the the the the] Loss[0.49606] Acc[0.8675] Prec[1.0] Recall[0.8675] F1[0.92891] => Worst<<Loss>>[0.55684] Found at [  1, 28]\n","[  1, 91] TokenID[[11858, 20417, 207, 207, 207, 207]] TokensDesc[telegram shocks the the the the] Loss[0.44915] Acc[0.87819] Prec[1.0] Recall[0.87819] F1[0.93511] => Worst<<Loss>>[0.55684] Found at [  1, 28]\n","[  1, 92] TokenID[[11858, 26059, 207, 207, 207, 207]] TokensDesc[telegram hawthorn the the the the] Loss[0.51735] Acc[0.85445] Prec[1.0] Recall[0.85445] F1[0.92137] => Worst<<Loss>>[0.55684] Found at [  1, 28]\n","[  1, 93] TokenID[[11858, 25005, 207, 207, 207, 207]] TokensDesc[telegramkton the the the the] Loss[0.48752] Acc[0.86487] Prec[1.0] Recall[0.86487] F1[0.92743] => Worst<<Loss>>[0.55684] Found at [  1, 28]\n","[  1, 94] TokenID[[11858, 11682, 207, 207, 207, 207]] TokensDesc[telegram interactiv the the the the] Loss[0.40307] Acc[0.90329] Prec[1.0] Recall[0.90329] F1[0.94902] => Worst<<Loss>>[0.55684] Found at [  1, 28]\n","[  1, 95] TokenID[[11858, 27976, 207, 207, 207, 207]] TokensDesc[telegram perceptibl the the the the] Loss[0.4713] Acc[0.87624] Prec[1.0] Recall[0.87624] F1[0.93394] => Worst<<Loss>>[0.55684] Found at [  1, 28]\n","[  1, 96] TokenID[[11858, 26019, 207, 207, 207, 207]] TokensDesc[telegram statu the the the the] Loss[0.48636] Acc[0.86505] Prec[1.0] Recall[0.86505] F1[0.92755] => Worst<<Loss>>[0.55684] Found at [  1, 28]\n","[  1, 97] TokenID[[11858, 7255, 207, 207, 207, 207]] TokensDesc[telegram modern the the the the] Loss[0.40509] Acc[0.89042] Prec[1.0] Recall[0.89042] F1[0.94196] => Worst<<Loss>>[0.55684] Found at [  1, 28]\n","[  1, 98] TokenID[[11858, 21311, 207, 207, 207, 207]] TokensDesc[telegram dense the the the the] Loss[0.50427] Acc[0.85794] Prec[1.0] Recall[0.85794] F1[0.92346] => Worst<<Loss>>[0.55684] Found at [  1, 28]\n","[  1, 99] TokenID[[11858, 5376, 207, 207, 207, 207]] TokensDesc[telegram trend the the the the] Loss[0.39396] Acc[0.89509] Prec[1.0] Recall[0.89509] F1[0.9446] => Worst<<Loss>>[0.55684] Found at [  1, 28]\n","Worst loss 0.5568357504183247 with candidates [11858, 13577, 207, 207, 207, 207] in the 1-iteration with tokens [telegram reprint the the the the]\n","candidates [ 7388  5044 18745 19256 23368 29529 12799 16069 23902 13679 14206 19484\n"," 13072 13124 16576 17473 21643 12370 16419 29129 21744  6428 18092 18920\n","  4265 14390 11844 28574  3274 12500 23988 14090 24137 20750 15550 20293\n"," 10089 17385 13482  7376 20006 22737  9827  5184 12224 11355 20612  3286\n"," 15770  9842 10352 15325  1619 22087 21083 26825 30367 17891 20793  7102\n"," 12468 15377 12916 13630 12755  2491 28586 11696  5101 12501 16278 27155\n","  4267 10667  9843 24551 18572 22305 11345 13412 12243 18900 21945 15440\n"," 23399  3019  8169 29413 13532 17251 21513 18258 27049 23681 28172   808\n"," 10629 12474 28061  9264]\n","[  2,  0] TokenID[[11858, 13577, 7388, 207, 207, 207]] TokensDesc[telegram reprint incapable the the the] Loss[0.4473] Acc[0.8882] Prec[1.0] Recall[0.8882] F1[0.94064] => Worst<<Loss>>[0.55684] Found at [  1, 28]\n","[  2,  1] TokenID[[11858, 13577, 5044, 207, 207, 207]] TokensDesc[telegram reprint irreparabl the the the] Loss[0.47972] Acc[0.88009] Prec[1.0] Recall[0.88009] F1[0.93602] => Worst<<Loss>>[0.55684] Found at [  1, 28]\n","[  2,  2] TokenID[[11858, 13577, 18745, 207, 207, 207]] TokensDesc[telegram reprint traumatic the the the] Loss[0.46704] Acc[0.89244] Prec[1.0] Recall[0.89244] F1[0.94302] => Worst<<Loss>>[0.55684] Found at [  1, 28]\n","[  2,  3] TokenID[[11858, 13577, 19256, 207, 207, 207]] TokensDesc[telegram reprint inextricabl the the the] Loss[0.43811] Acc[0.89638] Prec[1.0] Recall[0.89638] F1[0.94523] => Worst<<Loss>>[0.55684] Found at [  1, 28]\n","[  2,  4] TokenID[[11858, 13577, 23368, 207, 207, 207]] TokensDesc[telegram reprint exigen the the the] Loss[0.49181] Acc[0.87282] Prec[1.0] Recall[0.87282] F1[0.93202] => Worst<<Loss>>[0.55684] Found at [  1, 28]\n","[  2,  5] TokenID[[11858, 13577, 29529, 207, 207, 207]] TokensDesc[telegram reprint epilep the the the] Loss[0.4881] Acc[0.87863] Prec[1.0] Recall[0.87863] F1[0.93533] => Worst<<Loss>>[0.55684] Found at [  1, 28]\n","[  2,  6] TokenID[[11858, 13577, 12799, 207, 207, 207]] TokensDesc[telegram reprint indefeasibl the the the] Loss[0.50487] Acc[0.86494] Prec[1.0] Recall[0.86494] F1[0.92738] => Worst<<Loss>>[0.55684] Found at [  1, 28]\n","[  2,  7] TokenID[[11858, 13577, 16069, 207, 207, 207]] TokensDesc[telegram reprint wheelchair the the the] Loss[0.51935] Acc[0.86518] Prec[1.0] Recall[0.86518] F1[0.9276] => Worst<<Loss>>[0.55684] Found at [  1, 28]\n","[  2,  8] TokenID[[11858, 13577, 23902, 207, 207, 207]] TokensDesc[telegram reprint jehova the the the] Loss[0.49654] Acc[0.8705] Prec[1.0] Recall[0.8705] F1[0.93067] => Worst<<Loss>>[0.55684] Found at [  1, 28]\n","[  2,  9] TokenID[[11858, 13577, 13679, 207, 207, 207]] TokensDesc[telegram reprint pornograph the the the] Loss[0.47649] Acc[0.88844] Prec[1.0] Recall[0.88844] F1[0.94081] => Worst<<Loss>>[0.55684] Found at [  1, 28]\n","[  2, 10] TokenID[[11858, 13577, 14206, 207, 207, 207]] TokensDesc[telegram reprint inhalation the the the] Loss[0.44481] Acc[0.89294] Prec[1.0] Recall[0.89294] F1[0.94331] => Worst<<Loss>>[0.55684] Found at [  1, 28]\n","[  2, 11] TokenID[[11858, 13577, 19484, 207, 207, 207]] TokensDesc[telegram reprint laptop the the the] Loss[0.42847] Acc[0.90223] Prec[1.0] Recall[0.90223] F1[0.9485] => Worst<<Loss>>[0.55684] Found at [  1, 28]\n","[  2, 12] TokenID[[11858, 13577, 13072, 207, 207, 207]] TokensDesc[telegram reprint inconvenience the the the] Loss[0.43871] Acc[0.89286] Prec[1.0] Recall[0.89286] F1[0.94326] => Worst<<Loss>>[0.55684] Found at [  1, 28]\n","[  2, 13] TokenID[[11858, 13577, 13124, 207, 207, 207]] TokensDesc[telegram reprintsoever the the the] Loss[0.45507] Acc[0.88436] Prec[1.0] Recall[0.88436] F1[0.93853] => Worst<<Loss>>[0.55684] Found at [  1, 28]\n","[  2, 14] TokenID[[11858, 13577, 16576, 207, 207, 207]] TokensDesc[telegram reprint detached the the the] Loss[0.47757] Acc[0.88011] Prec[1.0] Recall[0.88011] F1[0.9361] => Worst<<Loss>>[0.55684] Found at [  1, 28]\n","[  2, 15] TokenID[[11858, 13577, 17473, 207, 207, 207]] TokensDesc[telegram reprint incrimination the the the] Loss[0.48561] Acc[0.88528] Prec[1.0] Recall[0.88528] F1[0.93896] => Worst<<Loss>>[0.55684] Found at [  1, 28]\n","[  2, 16] TokenID[[11858, 13577, 21643, 207, 207, 207]] TokensDesc[telegram reprint amenit the the the] Loss[0.46287] Acc[0.89252] Prec[1.0] Recall[0.89252] F1[0.94307] => Worst<<Loss>>[0.55684] Found at [  1, 28]\n","[  2, 17] TokenID[[11858, 13577, 12370, 207, 207, 207]] TokensDesc[telegram reprint absconding the the the] Loss[0.51512] Acc[0.87135] Prec[1.0] Recall[0.87135] F1[0.93111] => Worst<<Loss>>[0.55684] Found at [  1, 28]\n","[  2, 18] TokenID[[11858, 13577, 16419, 207, 207, 207]] TokensDesc[telegram reprint diabetes the the the] Loss[0.48491] Acc[0.87843] Prec[1.0] Recall[0.87843] F1[0.93519] => Worst<<Loss>>[0.55684] Found at [  1, 28]\n","[  2, 19] TokenID[[11858, 13577, 29129, 207, 207, 207]] TokensDesc[telegram reprint debas the the the] Loss[0.44471] Acc[0.89359] Prec[1.0] Recall[0.89359] F1[0.94368] => Worst<<Loss>>[0.55684] Found at [  1, 28]\n","[  2, 20] TokenID[[11858, 13577, 21744, 207, 207, 207]] TokensDesc[telegram reprint needless the the the] Loss[0.46694] Acc[0.88732] Prec[1.0] Recall[0.88732] F1[0.94014] => Worst<<Loss>>[0.55684] Found at [  1, 28]\n","[  2, 21] TokenID[[11858, 13577, 6428, 207, 207, 207]] TokensDesc[telegram reprint flexibl the the the] Loss[0.43876] Acc[0.89233] Prec[1.0] Recall[0.89233] F1[0.94297] => Worst<<Loss>>[0.55684] Found at [  1, 28]\n","[  2, 22] TokenID[[11858, 13577, 18092, 207, 207, 207]] TokensDesc[telegram reprintvasive the the the] Loss[0.42914] Acc[0.89964] Prec[1.0] Recall[0.89964] F1[0.94706] => Worst<<Loss>>[0.55684] Found at [  1, 28]\n","[  2, 23] TokenID[[11858, 13577, 18920, 207, 207, 207]] TokensDesc[telegram reprintgui the the the] Loss[0.46525] Acc[0.88044] Prec[1.0] Recall[0.88044] F1[0.93636] => Worst<<Loss>>[0.55684] Found at [  1, 28]\n","[  2, 24] TokenID[[11858, 13577, 4265, 207, 207, 207]] TokensDesc[telegram reprint independence the the the] Loss[0.42395] Acc[0.89822] Prec[1.0] Recall[0.89822] F1[0.94627] => Worst<<Loss>>[0.55684] Found at [  1, 28]\n","[  2, 25] TokenID[[11858, 13577, 14390, 207, 207, 207]] TokensDesc[telegram reprint intimidation the the the] Loss[0.41857] Acc[0.90261] Prec[1.0] Recall[0.90261] F1[0.94871] => Worst<<Loss>>[0.55684] Found at [  1, 28]\n","[  2, 26] TokenID[[11858, 13577, 11844, 207, 207, 207]] TokensDesc[telegram reprint cruel the the the] Loss[0.52376] Acc[0.85016] Prec[1.0] Recall[0.85016] F1[0.91892] => Worst<<Loss>>[0.55684] Found at [  1, 28]\n","[  2, 27] TokenID[[11858, 13577, 28574, 207, 207, 207]] TokensDesc[telegram reprintivant the the the] Loss[0.5297] Acc[0.85987] Prec[1.0] Recall[0.85987] F1[0.92448] => Worst<<Loss>>[0.55684] Found at [  1, 28]\n","[  2, 28] TokenID[[11858, 13577, 3274, 207, 207, 207]] TokensDesc[telegram reprint mental the the the] Loss[0.49203] Acc[0.87634] Prec[1.0] Recall[0.87634] F1[0.93404] => Worst<<Loss>>[0.55684] Found at [  1, 28]\n","[  2, 29] TokenID[[11858, 13577, 12500, 207, 207, 207]] TokensDesc[telegram reprint workman the the the] Loss[0.5218] Acc[0.85344] Prec[1.0] Recall[0.85344] F1[0.92085] => Worst<<Loss>>[0.55684] Found at [  1, 28]\n","[  2, 30] TokenID[[11858, 13577, 23988, 207, 207, 207]] TokensDesc[telegram reprint 4418 the the the] Loss[0.51508] Acc[0.87162] Prec[1.0] Recall[0.87162] F1[0.93125] => Worst<<Loss>>[0.55684] Found at [  1, 28]\n","[  2, 31] TokenID[[11858, 13577, 14090, 207, 207, 207]] TokensDesc[telegram reprint turpitud the the the] Loss[0.5211] Acc[0.86206] Prec[1.0] Recall[0.86206] F1[0.92582] => Worst<<Loss>>[0.55684] Found at [  1, 28]\n","[  2, 32] TokenID[[11858, 13577, 24137, 207, 207, 207]] TokensDesc[telegram reprint distraction the the the] Loss[0.39014] Acc[0.90862] Prec[1.0] Recall[0.90862] F1[0.95202] => Worst<<Loss>>[0.55684] Found at [  1, 28]\n","[  2, 33] TokenID[[11858, 13577, 20750, 207, 207, 207]] TokensDesc[telegram reprint yoursel the the the] Loss[0.50813] Acc[0.86574] Prec[1.0] Recall[0.86574] F1[0.92798] => Worst<<Loss>>[0.55684] Found at [  1, 28]\n","[  2, 34] TokenID[[11858, 13577, 15550, 207, 207, 207]] TokensDesc[telegram reprint refraining the the the] Loss[0.50936] Acc[0.8744] Prec[1.0] Recall[0.8744] F1[0.9328] => Worst<<Loss>>[0.55684] Found at [  1, 28]\n","[  2, 35] TokenID[[11858, 13577, 20293, 207, 207, 207]] TokensDesc[telegram reprint obes the the the] Loss[0.46356] Acc[0.89082] Prec[1.0] Recall[0.89082] F1[0.94212] => Worst<<Loss>>[0.55684] Found at [  1, 28]\n","[  2, 36] TokenID[[11858, 13577, 10089, 207, 207, 207]] TokensDesc[telegram reprint sleep the the the] Loss[0.42493] Acc[0.89696] Prec[1.0] Recall[0.89696] F1[0.94556] => Worst<<Loss>>[0.55684] Found at [  1, 28]\n","[  2, 37] TokenID[[11858, 13577, 17385, 207, 207, 207]] TokensDesc[telegram reprint unsuspect the the the] Loss[0.51534] Acc[0.86394] Prec[1.0] Recall[0.86394] F1[0.92688] => Worst<<Loss>>[0.55684] Found at [  1, 28]\n","[  2, 38] TokenID[[11858, 13577, 13482, 207, 207, 207]] TokensDesc[telegram reprint racketeer the the the] Loss[0.51369] Acc[0.87616] Prec[1.0] Recall[0.87616] F1[0.93386] => Worst<<Loss>>[0.55684] Found at [  1, 28]\n","[  2, 39] TokenID[[11858, 13577, 7376, 207, 207, 207]] TokensDesc[telegram reprint wear the the the] Loss[0.42903] Acc[0.89971] Prec[1.0] Recall[0.89971] F1[0.9471] => Worst<<Loss>>[0.55684] Found at [  1, 28]\n","[  2, 40] TokenID[[11858, 13577, 20006, 207, 207, 207]] TokensDesc[telegram reprint invitee the the the] Loss[0.46401] Acc[0.88917] Prec[1.0] Recall[0.88917] F1[0.94117] => Worst<<Loss>>[0.55684] Found at [  1, 28]\n","[  2, 41] TokenID[[11858, 13577, 22737, 207, 207, 207]] TokensDesc[telegram reprintboundar the the the] Loss[0.49351] Acc[0.86713] Prec[1.0] Recall[0.86713] F1[0.92878] => Worst<<Loss>>[0.55684] Found at [  1, 28]\n","[  2, 42] TokenID[[11858, 13577, 9827, 207, 207, 207]] TokensDesc[telegram reprint nursing the the the] Loss[0.47905] Acc[0.88211] Prec[1.0] Recall[0.88211] F1[0.93725] => Worst<<Loss>>[0.55684] Found at [  1, 28]\n","[  2, 43] TokenID[[11858, 13577, 5184, 207, 207, 207]] TokensDesc[telegram reprint fixture the the the] Loss[0.51014] Acc[0.87906] Prec[1.0] Recall[0.87906] F1[0.93542] => Worst<<Loss>>[0.55684] Found at [  1, 28]\n","[  2, 44] TokenID[[11858, 13577, 12224, 207, 207, 207]] TokensDesc[telegram reprint americans the the the] Loss[0.43944] Acc[0.89668] Prec[1.0] Recall[0.89668] F1[0.9454] => Worst<<Loss>>[0.55684] Found at [  1, 28]\n","[  2, 45] TokenID[[11858, 13577, 11355, 207, 207, 207]] TokensDesc[telegram reprint worn the the the] Loss[0.4293] Acc[0.89656] Prec[1.0] Recall[0.89656] F1[0.94534] => Worst<<Loss>>[0.55684] Found at [  1, 28]\n","[  2, 46] TokenID[[11858, 13577, 20612, 207, 207, 207]] TokensDesc[telegram reprint urina the the the] Loss[0.48562] Acc[0.88196] Prec[1.0] Recall[0.88196] F1[0.93716] => Worst<<Loss>>[0.55684] Found at [  1, 28]\n","[  2, 47] TokenID[[11858, 13577, 3286, 207, 207, 207]] TokensDesc[telegram reprint incur the the the] Loss[0.42871] Acc[0.89393] Prec[1.0] Recall[0.89393] F1[0.94388] => Worst<<Loss>>[0.55684] Found at [  1, 28]\n","[  2, 48] TokenID[[11858, 13577, 15770, 207, 207, 207]] TokensDesc[telegram reprint nervous the the the] Loss[0.46748] Acc[0.88803] Prec[1.0] Recall[0.88803] F1[0.94053] => Worst<<Loss>>[0.55684] Found at [  1, 28]\n","[  2, 49] TokenID[[11858, 13577, 9842, 207, 207, 207]] TokensDesc[telegram reprint dishonest the the the] Loss[0.49492] Acc[0.87389] Prec[1.0] Recall[0.87389] F1[0.9326] => Worst<<Loss>>[0.55684] Found at [  1, 28]\n","[  2, 50] TokenID[[11858, 13577, 10352, 207, 207, 207]] TokensDesc[telegram reprint medically the the the] Loss[0.49141] Acc[0.86582] Prec[1.0] Recall[0.86582] F1[0.92805] => Worst<<Loss>>[0.55684] Found at [  1, 28]\n","[  2, 51] TokenID[[11858, 13577, 15325, 207, 207, 207]] TokensDesc[telegram reprint liberties the the the] Loss[0.53448] Acc[0.86623] Prec[1.0] Recall[0.86623] F1[0.92807] => Worst<<Loss>>[0.55684] Found at [  1, 28]\n","[  2, 52] TokenID[[11858, 13577, 1619, 207, 207, 207]] TokensDesc[telegram reprint intellectual the the the] Loss[0.46629] Acc[0.89306] Prec[1.0] Recall[0.89306] F1[0.94336] => Worst<<Loss>>[0.55684] Found at [  1, 28]\n","[  2, 53] TokenID[[11858, 13577, 22087, 207, 207, 207]] TokensDesc[telegram reprint intranet the the the] Loss[0.40618] Acc[0.90876] Prec[1.0] Recall[0.90876] F1[0.95205] => Worst<<Loss>>[0.55684] Found at [  1, 28]\n","[  2, 54] TokenID[[11858, 13577, 21083, 207, 207, 207]] TokensDesc[telegram reprint extrajudicial the the the] Loss[0.48537] Acc[0.88598] Prec[1.0] Recall[0.88598] F1[0.93936] => Worst<<Loss>>[0.55684] Found at [  1, 28]\n","[  2, 55] TokenID[[11858, 13577, 26825, 207, 207, 207]] TokensDesc[telegram reprint inaccessible the the the] Loss[0.42027] Acc[0.90184] Prec[1.0] Recall[0.90184] F1[0.94829] => Worst<<Loss>>[0.55684] Found at [  1, 28]\n","[  2, 56] TokenID[[11858, 13577, 30367, 207, 207, 207]] TokensDesc[telegram reprint congenital the the the] Loss[0.48851] Acc[0.88366] Prec[1.0] Recall[0.88366] F1[0.93806] => Worst<<Loss>>[0.55684] Found at [  1, 28]\n","[  2, 57] TokenID[[11858, 13577, 17891, 207, 207, 207]] TokensDesc[telegram reprint wounds the the the] Loss[0.46297] Acc[0.88496] Prec[1.0] Recall[0.88496] F1[0.93887] => Worst<<Loss>>[0.55684] Found at [  1, 28]\n","[  2, 58] TokenID[[11858, 13577, 20793, 207, 207, 207]] TokensDesc[telegram reprint passion the the the] Loss[0.47028] Acc[0.87948] Prec[1.0] Recall[0.87948] F1[0.93578] => Worst<<Loss>>[0.55684] Found at [  1, 28]\n","[  2, 59] TokenID[[11858, 13577, 7102, 207, 207, 207]] TokensDesc[telegram reprint devote the the the] Loss[0.49814] Acc[0.87094] Prec[1.0] Recall[0.87094] F1[0.93094] => Worst<<Loss>>[0.55684] Found at [  1, 28]\n","[  2, 60] TokenID[[11858, 13577, 12468, 207, 207, 207]] TokensDesc[telegram reprint indoor the the the] Loss[0.44603] Acc[0.89296] Prec[1.0] Recall[0.89296] F1[0.94332] => Worst<<Loss>>[0.55684] Found at [  1, 28]\n","[  2, 61] TokenID[[11858, 13577, 15377, 207, 207, 207]] TokensDesc[telegram reprint funeral the the the] Loss[0.50088] Acc[0.8766] Prec[1.0] Recall[0.8766] F1[0.93409] => Worst<<Loss>>[0.55684] Found at [  1, 28]\n","[  2, 62] TokenID[[11858, 13577, 12916, 207, 207, 207]] TokensDesc[telegram reprint dures the the the] Loss[0.49994] Acc[0.86654] Prec[1.0] Recall[0.86654] F1[0.92844] => Worst<<Loss>>[0.55684] Found at [  1, 28]\n","[  2, 63] TokenID[[11858, 13577, 13630, 207, 207, 207]] TokensDesc[telegram reprint hidden the the the] Loss[0.46477] Acc[0.89358] Prec[1.0] Recall[0.89358] F1[0.94368] => Worst<<Loss>>[0.55684] Found at [  1, 28]\n","[  2, 64] TokenID[[11858, 13577, 12755, 207, 207, 207]] TokensDesc[telegram reprint riot the the the] Loss[0.52851] Acc[0.8536] Prec[1.0] Recall[0.8536] F1[0.92086] => Worst<<Loss>>[0.55684] Found at [  1, 28]\n","[  2, 65] TokenID[[11858, 13577, 2491, 207, 207, 207]] TokensDesc[telegram reprint unenforceable the the the] Loss[0.54227] Acc[0.85726] Prec[1.0] Recall[0.85726] F1[0.92296] => Worst<<Loss>>[0.55684] Found at [  1, 28]\n","[  2, 66] TokenID[[11858, 13577, 28586, 207, 207, 207]] TokensDesc[telegram reprint inflammation the the the] Loss[0.46798] Acc[0.88714] Prec[1.0] Recall[0.88714] F1[0.94006] => Worst<<Loss>>[0.55684] Found at [  1, 28]\n","[  2, 67] TokenID[[11858, 13577, 11696, 207, 207, 207]] TokensDesc[telegram reprint tyco the the the] Loss[0.45641] Acc[0.8876] Prec[1.0] Recall[0.8876] F1[0.94029] => Worst<<Loss>>[0.55684] Found at [  1, 28]\n","[  2, 68] TokenID[[11858, 13577, 5101, 207, 207, 207]] TokensDesc[telegram reprint inability the the the] Loss[0.42018] Acc[0.90114] Prec[1.0] Recall[0.90114] F1[0.9479] => Worst<<Loss>>[0.55684] Found at [  1, 28]\n","[  2, 69] TokenID[[11858, 13577, 12501, 207, 207, 207]] TokensDesc[telegram reprint expend the the the] Loss[0.45784] Acc[0.88795] Prec[1.0] Recall[0.88795] F1[0.94056] => Worst<<Loss>>[0.55684] Found at [  1, 28]\n","[  2, 70] TokenID[[11858, 13577, 16278, 207, 207, 207]] TokensDesc[telegram reprint addicti the the the] Loss[0.49425] Acc[0.87636] Prec[1.0] Recall[0.87636] F1[0.93405] => Worst<<Loss>>[0.55684] Found at [  1, 28]\n","[  2, 71] TokenID[[11858, 13577, 27155, 207, 207, 207]] TokensDesc[telegram reprint reemploy the the the] Loss[0.54642] Acc[0.8497] Prec[1.0] Recall[0.8497] F1[0.91867] => Worst<<Loss>>[0.55684] Found at [  1, 28]\n","[  2, 72] TokenID[[11858, 13577, 4267, 207, 207, 207]] TokensDesc[telegram reprint intangible the the the] Loss[0.4477] Acc[0.8915] Prec[1.0] Recall[0.8915] F1[0.94248] => Worst<<Loss>>[0.55684] Found at [  1, 28]\n","[  2, 73] TokenID[[11858, 13577, 10667, 207, 207, 207]] TokensDesc[telegram reprint appare the the the] Loss[0.5182] Acc[0.86591] Prec[1.0] Recall[0.86591] F1[0.92805] => Worst<<Loss>>[0.55684] Found at [  1, 28]\n","[  2, 74] TokenID[[11858, 13577, 9843, 207, 207, 207]] TokensDesc[telegram reprint acquitted the the the] Loss[0.56939] Acc[0.83854] Prec[1.0] Recall[0.83854] F1[0.91212] => Worst<<Loss>>[0.56939] Found at [  2, 74]\n","[  2, 75] TokenID[[11858, 13577, 24551, 207, 207, 207]] TokensDesc[telegram reprint nonjudicial the the the] Loss[0.49824] Acc[0.87001] Prec[1.0] Recall[0.87001] F1[0.93039] => Worst<<Loss>>[0.56939] Found at [  2, 74]\n","[  2, 76] TokenID[[11858, 13577, 18572, 207, 207, 207]] TokensDesc[telegram reprint cardia the the the] Loss[0.50491] Acc[0.87607] Prec[1.0] Recall[0.87607] F1[0.93383] => Worst<<Loss>>[0.56939] Found at [  2, 74]\n","[  2, 77] TokenID[[11858, 13577, 22305, 207, 207, 207]] TokensDesc[telegram reprintself the the the] Loss[0.53494] Acc[0.84966] Prec[1.0] Recall[0.84966] F1[0.91866] => Worst<<Loss>>[0.56939] Found at [  2, 74]\n","[  2, 78] TokenID[[11858, 13577, 11345, 207, 207, 207]] TokensDesc[telegram reprint obstruction the the the] Loss[0.49605] Acc[0.87324] Prec[1.0] Recall[0.87324] F1[0.9321] => Worst<<Loss>>[0.56939] Found at [  2, 74]\n","[  2, 79] TokenID[[11858, 13577, 13412, 207, 207, 207]] TokensDesc[telegram reprint turtle the the the] Loss[0.47842] Acc[0.88765] Prec[1.0] Recall[0.88765] F1[0.94032] => Worst<<Loss>>[0.56939] Found at [  2, 74]\n","[  2, 80] TokenID[[11858, 13577, 12243, 207, 207, 207]] TokensDesc[telegram reprint uninterrupted the the the] Loss[0.44224] Acc[0.89308] Prec[1.0] Recall[0.89308] F1[0.94337] => Worst<<Loss>>[0.56939] Found at [  2, 74]\n","[  2, 81] TokenID[[11858, 13577, 18900, 207, 207, 207]] TokensDesc[telegram reprint allergi the the the] Loss[0.46762] Acc[0.89011] Prec[1.0] Recall[0.89011] F1[0.94172] => Worst<<Loss>>[0.56939] Found at [  2, 74]\n","[  2, 82] TokenID[[11858, 13577, 21945, 207, 207, 207]] TokensDesc[telegram reprint centuri the the the] Loss[0.48622] Acc[0.87443] Prec[1.0] Recall[0.87443] F1[0.93297] => Worst<<Loss>>[0.56939] Found at [  2, 74]\n","[  2, 83] TokenID[[11858, 13577, 15440, 207, 207, 207]] TokensDesc[telegram reprint materialmen the the the] Loss[0.55921] Acc[0.84407] Prec[1.0] Recall[0.84407] F1[0.91529] => Worst<<Loss>>[0.56939] Found at [  2, 74]\n","[  2, 84] TokenID[[11858, 13577, 23399, 207, 207, 207]] TokensDesc[telegram reprintrcra the the the] Loss[0.5597] Acc[0.83587] Prec[1.0] Recall[0.83587] F1[0.91052] => Worst<<Loss>>[0.56939] Found at [  2, 74]\n","[  2, 85] TokenID[[11858, 13577, 3019, 207, 207, 207]] TokensDesc[telegram reprint unreasonably the the the] Loss[0.43752] Acc[0.89141] Prec[1.0] Recall[0.89141] F1[0.94246] => Worst<<Loss>>[0.56939] Found at [  2, 74]\n","[  2, 86] TokenID[[11858, 13577, 8169, 207, 207, 207]] TokensDesc[telegram reprint inconvenien the the the] Loss[0.48878] Acc[0.87954] Prec[1.0] Recall[0.87954] F1[0.93571] => Worst<<Loss>>[0.56939] Found at [  2, 74]\n","[  2, 87] TokenID[[11858, 13577, 29413, 207, 207, 207]] TokensDesc[telegram reprint pornographic the the the] Loss[0.47084] Acc[0.88801] Prec[1.0] Recall[0.88801] F1[0.94059] => Worst<<Loss>>[0.56939] Found at [  2, 74]\n","[  2, 88] TokenID[[11858, 13577, 13532, 207, 207, 207]] TokensDesc[telegram reprint sequestrat the the the] Loss[0.53994] Acc[0.85708] Prec[1.0] Recall[0.85708] F1[0.92283] => Worst<<Loss>>[0.56939] Found at [  2, 74]\n","[  2, 89] TokenID[[11858, 13577, 17251, 207, 207, 207]] TokensDesc[telegram reprint killings the the the] Loss[0.44556] Acc[0.8959] Prec[1.0] Recall[0.8959] F1[0.94496] => Worst<<Loss>>[0.56939] Found at [  2, 74]\n","[  2, 90] TokenID[[11858, 13577, 21513, 207, 207, 207]] TokensDesc[telegram reprint calamit the the the] Loss[0.50305] Acc[0.86398] Prec[1.0] Recall[0.86398] F1[0.92693] => Worst<<Loss>>[0.56939] Found at [  2, 74]\n","[  2, 91] TokenID[[11858, 13577, 18258, 207, 207, 207]] TokensDesc[telegram reprint vicarious the the the] Loss[0.49421] Acc[0.87997] Prec[1.0] Recall[0.87997] F1[0.93604] => Worst<<Loss>>[0.56939] Found at [  2, 74]\n","[  2, 92] TokenID[[11858, 13577, 27049, 207, 207, 207]] TokensDesc[telegram reprinternali the the the] Loss[0.53446] Acc[0.85682] Prec[1.0] Recall[0.85682] F1[0.92283] => Worst<<Loss>>[0.56939] Found at [  2, 74]\n","[  2, 93] TokenID[[11858, 13577, 23681, 207, 207, 207]] TokensDesc[telegram reprint cogniti the the the] Loss[0.46569] Acc[0.88903] Prec[1.0] Recall[0.88903] F1[0.94117] => Worst<<Loss>>[0.56939] Found at [  2, 74]\n","[  2, 94] TokenID[[11858, 13577, 28172, 207, 207, 207]] TokensDesc[telegram reprint conjuga the the the] Loss[0.49549] Acc[0.87709] Prec[1.0] Recall[0.87709] F1[0.93438] => Worst<<Loss>>[0.56939] Found at [  2, 74]\n","[  2, 95] TokenID[[11858, 13577, 808, 207, 207, 207]] TokensDesc[telegram reprint access the the the] Loss[0.41678] Acc[0.90212] Prec[1.0] Recall[0.90212] F1[0.94846] => Worst<<Loss>>[0.56939] Found at [  2, 74]\n","[  2, 96] TokenID[[11858, 13577, 10629, 207, 207, 207]] TokensDesc[telegram reprint expensive the the the] Loss[0.42745] Acc[0.90043] Prec[1.0] Recall[0.90043] F1[0.94751] => Worst<<Loss>>[0.56939] Found at [  2, 74]\n","[  2, 97] TokenID[[11858, 13577, 12474, 207, 207, 207]] TokensDesc[telegram reprint frydlender the the the] Loss[0.52113] Acc[0.87105] Prec[1.0] Recall[0.87105] F1[0.93092] => Worst<<Loss>>[0.56939] Found at [  2, 74]\n","[  2, 98] TokenID[[11858, 13577, 28061, 207, 207, 207]] TokensDesc[telegram reprint embarrassment the the the] Loss[0.47234] Acc[0.88394] Prec[1.0] Recall[0.88394] F1[0.93827] => Worst<<Loss>>[0.56939] Found at [  2, 74]\n","[  2, 99] TokenID[[11858, 13577, 9264, 207, 207, 207]] TokensDesc[telegram reprint obstruct the the the] Loss[0.49622] Acc[0.86572] Prec[1.0] Recall[0.86572] F1[0.92795] => Worst<<Loss>>[0.56939] Found at [  2, 74]\n","Worst loss 0.5693922489881516 with candidates [11858, 13577, 9843, 207, 207, 207] in the 2-iteration with tokens [telegram reprint acquitted the the the]\n","candidates [11331  9909  8222 29240 10029  6743 18998 17372 11682 15460 14044 10082\n","  8503  6482  6242 20613 18909 26328  2286 23712 25544  9235 22087 12097\n"," 10290  7843  9531  8350 26544 20601 27249 20305  2554 24952 22028 14926\n","  6815 29933 10096 28390 16808 24766 24227 26914 24583 20886 11990 27243\n","  2196  1727  4030  1175 19068 15847 10769 16460  4920 29850 18505 21458\n","  9570 23999  6162 17607 16523  1619  6453 10986 12949  6484  4022 22894\n"," 28407 11058 15211  6572 20584 16452 10218  9356  3533 19968 17692 20164\n"," 29442 26167 10263 17986 24931 11909 10357 23288  3354  4025 28172  4998\n","  5306  1462 12650  2878]\n","[  3,  0] TokenID[[11858, 13577, 9843, 11331, 207, 207]] TokensDesc[telegram reprint acquitted correlative the the] Loss[0.45145] Acc[0.89152] Prec[1.0] Recall[0.89152] F1[0.94257] => Worst<<Loss>>[0.56939] Found at [  2, 74]\n","[  3,  1] TokenID[[11858, 13577, 9843, 9909, 207, 207]] TokensDesc[telegram reprint acquitted sciences the the] Loss[0.43508] Acc[0.90312] Prec[1.0] Recall[0.90312] F1[0.94899] => Worst<<Loss>>[0.56939] Found at [  2, 74]\n","[  3,  2] TokenID[[11858, 13577, 9843, 8222, 207, 207]] TokensDesc[telegram reprint acquitted europa the the] Loss[0.46785] Acc[0.88267] Prec[1.0] Recall[0.88267] F1[0.93757] => Worst<<Loss>>[0.56939] Found at [  2, 74]\n","[  3,  3] TokenID[[11858, 13577, 9843, 29240, 207, 207]] TokensDesc[telegram reprint acquittediosyn the the] Loss[0.46793] Acc[0.8815] Prec[1.0] Recall[0.8815] F1[0.9369] => Worst<<Loss>>[0.56939] Found at [  2, 74]\n","[  3,  4] TokenID[[11858, 13577, 9843, 10029, 207, 207]] TokensDesc[telegram reprint acquitted traditionally the the] Loss[0.4365] Acc[0.89725] Prec[1.0] Recall[0.89725] F1[0.94577] => Worst<<Loss>>[0.56939] Found at [  2, 74]\n","[  3,  5] TokenID[[11858, 13577, 9843, 6743, 207, 207]] TokensDesc[telegram reprint acquitted differently the the] Loss[0.44011] Acc[0.89497] Prec[1.0] Recall[0.89497] F1[0.9445] => Worst<<Loss>>[0.56939] Found at [  2, 74]\n","[  3,  6] TokenID[[11858, 13577, 9843, 18998, 207, 207]] TokensDesc[telegram reprint acquitted univers the the] Loss[0.49293] Acc[0.87865] Prec[1.0] Recall[0.87865] F1[0.93531] => Worst<<Loss>>[0.56939] Found at [  2, 74]\n","[  3,  7] TokenID[[11858, 13577, 9843, 17372, 207, 207]] TokensDesc[telegram reprint acquitted favourably the the] Loss[0.44715] Acc[0.89308] Prec[1.0] Recall[0.89308] F1[0.94343] => Worst<<Loss>>[0.56939] Found at [  2, 74]\n","[  3,  8] TokenID[[11858, 13577, 9843, 11682, 207, 207]] TokensDesc[telegram reprint acquitted interactiv the the] Loss[0.41885] Acc[0.90621] Prec[1.0] Recall[0.90621] F1[0.9507] => Worst<<Loss>>[0.56939] Found at [  2, 74]\n","[  3,  9] TokenID[[11858, 13577, 9843, 15460, 207, 207]] TokensDesc[telegram reprint acquitted univ the the] Loss[0.45337] Acc[0.89122] Prec[1.0] Recall[0.89122] F1[0.94238] => Worst<<Loss>>[0.56939] Found at [  2, 74]\n","[  3, 10] TokenID[[11858, 13577, 9843, 14044, 207, 207]] TokensDesc[telegram reprint acquitted basel the the] Loss[0.43113] Acc[0.90147] Prec[1.0] Recall[0.90147] F1[0.94806] => Worst<<Loss>>[0.56939] Found at [  2, 74]\n","[  3, 11] TokenID[[11858, 13577, 9843, 10082, 207, 207]] TokensDesc[telegram reprint acquitted disparit the the] Loss[0.47669] Acc[0.88275] Prec[1.0] Recall[0.88275] F1[0.93762] => Worst<<Loss>>[0.56939] Found at [  2, 74]\n","[  3, 12] TokenID[[11858, 13577, 9843, 8503, 207, 207]] TokensDesc[telegram reprint acquitted ambient the the] Loss[0.45704] Acc[0.89536] Prec[1.0] Recall[0.89536] F1[0.94464] => Worst<<Loss>>[0.56939] Found at [  2, 74]\n","[  3, 13] TokenID[[11858, 13577, 9843, 6482, 207, 207]] TokensDesc[telegram reprint acquitted peer the the] Loss[0.44991] Acc[0.89523] Prec[1.0] Recall[0.89523] F1[0.94463] => Worst<<Loss>>[0.56939] Found at [  2, 74]\n","[  3, 14] TokenID[[11858, 13577, 9843, 6242, 207, 207]] TokensDesc[telegram reprint acquitted commonly the the] Loss[0.43479] Acc[0.89764] Prec[1.0] Recall[0.89764] F1[0.94599] => Worst<<Loss>>[0.56939] Found at [  2, 74]\n","[  3, 15] TokenID[[11858, 13577, 9843, 20613, 207, 207]] TokensDesc[telegram reprint acquitted accura the the] Loss[0.50408] Acc[0.86935] Prec[1.0] Recall[0.86935] F1[0.92996] => Worst<<Loss>>[0.56939] Found at [  2, 74]\n","[  3, 16] TokenID[[11858, 13577, 9843, 18909, 207, 207]] TokensDesc[telegram reprint acquitted biomet the the] Loss[0.45443] Acc[0.88319] Prec[1.0] Recall[0.88319] F1[0.93792] => Worst<<Loss>>[0.56939] Found at [  2, 74]\n","[  3, 17] TokenID[[11858, 13577, 9843, 26328, 207, 207]] TokensDesc[telegram reprint acquitted chimeri the the] Loss[0.52602] Acc[0.8692] Prec[1.0] Recall[0.8692] F1[0.92988] => Worst<<Loss>>[0.56939] Found at [  2, 74]\n","[  3, 18] TokenID[[11858, 13577, 9843, 2286, 207, 207]] TokensDesc[telegram reprint acquitted gc the the] Loss[0.44788] Acc[0.89566] Prec[1.0] Recall[0.89566] F1[0.94482] => Worst<<Loss>>[0.56939] Found at [  2, 74]\n","[  3, 19] TokenID[[11858, 13577, 9843, 23712, 207, 207]] TokensDesc[telegram reprint acquitted interlink the the] Loss[0.49124] Acc[0.87811] Prec[1.0] Recall[0.87811] F1[0.93498] => Worst<<Loss>>[0.56939] Found at [  2, 74]\n","[  3, 20] TokenID[[11858, 13577, 9843, 25544, 207, 207]] TokensDesc[telegram reprint acquitted holistic the the] Loss[0.45667] Acc[0.89059] Prec[1.0] Recall[0.89059] F1[0.94205] => Worst<<Loss>>[0.56939] Found at [  2, 74]\n","[  3, 21] TokenID[[11858, 13577, 9843, 9235, 207, 207]] TokensDesc[telegram reprint acquitted anywhere the the] Loss[0.45962] Acc[0.88739] Prec[1.0] Recall[0.88739] F1[0.94031] => Worst<<Loss>>[0.56939] Found at [  2, 74]\n","[  3, 22] TokenID[[11858, 13577, 9843, 22087, 207, 207]] TokensDesc[telegram reprint acquitted intranet the the] Loss[0.41984] Acc[0.90639] Prec[1.0] Recall[0.90639] F1[0.9508] => Worst<<Loss>>[0.56939] Found at [  2, 74]\n","[  3, 23] TokenID[[11858, 13577, 9843, 12097, 207, 207]] TokensDesc[telegram reprint acquitted conceptual the the] Loss[0.47423] Acc[0.88484] Prec[1.0] Recall[0.88484] F1[0.9388] => Worst<<Loss>>[0.56939] Found at [  2, 74]\n","[  3, 24] TokenID[[11858, 13577, 9843, 10290, 207, 207]] TokensDesc[telegram reprint acquitted easie the the] Loss[0.46967] Acc[0.88383] Prec[1.0] Recall[0.88383] F1[0.93825] => Worst<<Loss>>[0.56939] Found at [  2, 74]\n","[  3, 25] TokenID[[11858, 13577, 9843, 7843, 207, 207]] TokensDesc[telegram reprint acquitted convergen the the] Loss[0.454] Acc[0.8877] Prec[1.0] Recall[0.8877] F1[0.94044] => Worst<<Loss>>[0.56939] Found at [  2, 74]\n","[  3, 26] TokenID[[11858, 13577, 9843, 9531, 207, 207]] TokensDesc[telegram reprint acquitted methodologi the the] Loss[0.48304] Acc[0.88247] Prec[1.0] Recall[0.88247] F1[0.93744] => Worst<<Loss>>[0.56939] Found at [  2, 74]\n","[  3, 27] TokenID[[11858, 13577, 9843, 8350, 207, 207]] TokensDesc[telegram reprint acquitted closer the the] Loss[0.46346] Acc[0.89288] Prec[1.0] Recall[0.89288] F1[0.94334] => Worst<<Loss>>[0.56939] Found at [  2, 74]\n","[  3, 28] TokenID[[11858, 13577, 9843, 26544, 207, 207]] TokensDesc[telegram reprint acquitted realistically the the] Loss[0.47829] Acc[0.88138] Prec[1.0] Recall[0.88138] F1[0.93685] => Worst<<Loss>>[0.56939] Found at [  2, 74]\n","[  3, 29] TokenID[[11858, 13577, 9843, 20601, 207, 207]] TokensDesc[telegram reprint acquitted lign the the] Loss[0.52513] Acc[0.85824] Prec[1.0] Recall[0.85824] F1[0.92364] => Worst<<Loss>>[0.56939] Found at [  2, 74]\n","[  3, 30] TokenID[[11858, 13577, 9843, 27249, 207, 207]] TokensDesc[telegram reprint acquitted utm the the] Loss[0.52995] Acc[0.86339] Prec[1.0] Recall[0.86339] F1[0.92653] => Worst<<Loss>>[0.56939] Found at [  2, 74]\n","[  3, 31] TokenID[[11858, 13577, 9843, 20305, 207, 207]] TokensDesc[telegram reprint acquitted favorabl the the] Loss[0.48052] Acc[0.87936] Prec[1.0] Recall[0.87936] F1[0.9357] => Worst<<Loss>>[0.56939] Found at [  2, 74]\n","[  3, 32] TokenID[[11858, 13577, 9843, 2554, 207, 207]] TokensDesc[telegram reprint acquitted wide the the] Loss[0.44034] Acc[0.89556] Prec[1.0] Recall[0.89556] F1[0.94479] => Worst<<Loss>>[0.56939] Found at [  2, 74]\n","[  3, 33] TokenID[[11858, 13577, 9843, 24952, 207, 207]] TokensDesc[telegram reprint acquitted analytic the the] Loss[0.44559] Acc[0.89344] Prec[1.0] Recall[0.89344] F1[0.94363] => Worst<<Loss>>[0.56939] Found at [  2, 74]\n","[  3, 34] TokenID[[11858, 13577, 9843, 22028, 207, 207]] TokensDesc[telegram reprint acquittedmhc the the] Loss[0.48738] Acc[0.87792] Prec[1.0] Recall[0.87792] F1[0.93487] => Worst<<Loss>>[0.56939] Found at [  2, 74]\n","[  3, 35] TokenID[[11858, 13577, 9843, 14926, 207, 207]] TokensDesc[telegram reprint acquitted collegi the the] Loss[0.47701] Acc[0.87831] Prec[1.0] Recall[0.87831] F1[0.93508] => Worst<<Loss>>[0.56939] Found at [  2, 74]\n","[  3, 36] TokenID[[11858, 13577, 9843, 6815, 207, 207]] TokensDesc[telegram reprint acquitted concepts the the] Loss[0.42849] Acc[0.90681] Prec[1.0] Recall[0.90681] F1[0.95104] => Worst<<Loss>>[0.56939] Found at [  2, 74]\n","[  3, 37] TokenID[[11858, 13577, 9843, 29933, 207, 207]] TokensDesc[telegram reprint acquitted conjunct the the] Loss[0.50909] Acc[0.86821] Prec[1.0] Recall[0.86821] F1[0.92932] => Worst<<Loss>>[0.56939] Found at [  2, 74]\n","[  3, 38] TokenID[[11858, 13577, 9843, 10096, 207, 207]] TokensDesc[telegram reprint acquitted universit the the] Loss[0.50461] Acc[0.87636] Prec[1.0] Recall[0.87636] F1[0.93397] => Worst<<Loss>>[0.56939] Found at [  2, 74]\n","[  3, 39] TokenID[[11858, 13577, 9843, 28390, 207, 207]] TokensDesc[telegram reprint acquitted gaul the the] Loss[0.53302] Acc[0.84887] Prec[1.0] Recall[0.84887] F1[0.91817] => Worst<<Loss>>[0.56939] Found at [  2, 74]\n","[  3, 40] TokenID[[11858, 13577, 9843, 16808, 207, 207]] TokensDesc[telegram reprint acquitted vari the the] Loss[0.47828] Acc[0.87751] Prec[1.0] Recall[0.87751] F1[0.93469] => Worst<<Loss>>[0.56939] Found at [  2, 74]\n","[  3, 41] TokenID[[11858, 13577, 9843, 24766, 207, 207]] TokensDesc[telegram reprint acquitted saska the the] Loss[0.52665] Acc[0.85831] Prec[1.0] Recall[0.85831] F1[0.92363] => Worst<<Loss>>[0.56939] Found at [  2, 74]\n","[  3, 42] TokenID[[11858, 13577, 9843, 24227, 207, 207]] TokensDesc[telegram reprint acquitted biopharma the the] Loss[0.45804] Acc[0.88831] Prec[1.0] Recall[0.88831] F1[0.94073] => Worst<<Loss>>[0.56939] Found at [  2, 74]\n","[  3, 43] TokenID[[11858, 13577, 9843, 26914, 207, 207]] TokensDesc[telegram reprint acquitted philosophical the the] Loss[0.50823] Acc[0.87374] Prec[1.0] Recall[0.87374] F1[0.93249] => Worst<<Loss>>[0.56939] Found at [  2, 74]\n","[  3, 44] TokenID[[11858, 13577, 9843, 24583, 207, 207]] TokensDesc[telegram reprint acquitted disaggregat the the] Loss[0.46764] Acc[0.88795] Prec[1.0] Recall[0.88795] F1[0.94056] => Worst<<Loss>>[0.56939] Found at [  2, 74]\n","[  3, 45] TokenID[[11858, 13577, 9843, 20886, 207, 207]] TokensDesc[telegram reprint acquitted beech the the] Loss[0.47831] Acc[0.87562] Prec[1.0] Recall[0.87562] F1[0.93361] => Worst<<Loss>>[0.56939] Found at [  2, 74]\n","[  3, 46] TokenID[[11858, 13577, 9843, 11990, 207, 207]] TokensDesc[telegram reprint acquitted marketplace the the] Loss[0.42298] Acc[0.90027] Prec[1.0] Recall[0.90027] F1[0.94739] => Worst<<Loss>>[0.56939] Found at [  2, 74]\n","[  3, 47] TokenID[[11858, 13577, 9843, 27243, 207, 207]] TokensDesc[telegram reprint acquitted intuitive the the] Loss[0.45016] Acc[0.88836] Prec[1.0] Recall[0.88836] F1[0.9408] => Worst<<Loss>>[0.56939] Found at [  2, 74]\n","[  3, 48] TokenID[[11858, 13577, 9843, 2196, 207, 207]] TokensDesc[telegram reprint acquitted experience the the] Loss[0.47426] Acc[0.88794] Prec[1.0] Recall[0.88794] F1[0.94062] => Worst<<Loss>>[0.56939] Found at [  2, 74]\n","[  3, 49] TokenID[[11858, 13577, 9843, 1727, 207, 207]] TokensDesc[telegram reprint acquitted world the the] Loss[0.45133] Acc[0.8859] Prec[1.0] Recall[0.8859] F1[0.93946] => Worst<<Loss>>[0.56939] Found at [  2, 74]\n","[  3, 50] TokenID[[11858, 13577, 9843, 4030, 207, 207]] TokensDesc[telegram reprint acquitted consistently the the] Loss[0.44575] Acc[0.89996] Prec[1.0] Recall[0.89996] F1[0.94724] => Worst<<Loss>>[0.56939] Found at [  2, 74]\n","[  3, 51] TokenID[[11858, 13577, 9843, 1175, 207, 207]] TokensDesc[telegram reprint acquitted systems the the] Loss[0.3839] Acc[0.91021] Prec[1.0] Recall[0.91021] F1[0.95292] => Worst<<Loss>>[0.56939] Found at [  2, 74]\n","[  3, 52] TokenID[[11858, 13577, 9843, 19068, 207, 207]] TokensDesc[telegram reprint acquitted deskt the the] Loss[0.47591] Acc[0.888] Prec[1.0] Recall[0.888] F1[0.94058] => Worst<<Loss>>[0.56939] Found at [  2, 74]\n","[  3, 53] TokenID[[11858, 13577, 9843, 15847, 207, 207]] TokensDesc[telegram reprint acquitted lucen the the] Loss[0.50877] Acc[0.86789] Prec[1.0] Recall[0.86789] F1[0.92918] => Worst<<Loss>>[0.56939] Found at [  2, 74]\n","[  3, 54] TokenID[[11858, 13577, 9843, 10769, 207, 207]] TokensDesc[telegram reprint acquitted bioma the the] Loss[0.46646] Acc[0.88585] Prec[1.0] Recall[0.88585] F1[0.93937] => Worst<<Loss>>[0.56939] Found at [  2, 74]\n","[  3, 55] TokenID[[11858, 13577, 9843, 16460, 207, 207]] TokensDesc[telegram reprint acquitted emea the the] Loss[0.45439] Acc[0.88591] Prec[1.0] Recall[0.88591] F1[0.93938] => Worst<<Loss>>[0.56939] Found at [  2, 74]\n","[  3, 56] TokenID[[11858, 13577, 9843, 4920, 207, 207]] TokensDesc[telegram reprint acquitted plural the the] Loss[0.42307] Acc[0.90161] Prec[1.0] Recall[0.90161] F1[0.94815] => Worst<<Loss>>[0.56939] Found at [  2, 74]\n","[  3, 57] TokenID[[11858, 13577, 9843, 29850, 207, 207]] TokensDesc[telegram reprint acquittedbcb the the] Loss[0.51932] Acc[0.85855] Prec[1.0] Recall[0.85855] F1[0.92378] => Worst<<Loss>>[0.56939] Found at [  2, 74]\n","[  3, 58] TokenID[[11858, 13577, 9843, 18505, 207, 207]] TokensDesc[telegram reprint acquitted podlask the the] Loss[0.50506] Acc[0.86734] Prec[1.0] Recall[0.86734] F1[0.9288] => Worst<<Loss>>[0.56939] Found at [  2, 74]\n","[  3, 59] TokenID[[11858, 13577, 9843, 21458, 207, 207]] TokensDesc[telegram reprint acquitted notions the the] Loss[0.44507] Acc[0.8931] Prec[1.0] Recall[0.8931] F1[0.94339] => Worst<<Loss>>[0.56939] Found at [  2, 74]\n","[  3, 60] TokenID[[11858, 13577, 9843, 9570, 207, 207]] TokensDesc[telegram reprint acquitted arguable the the] Loss[0.45963] Acc[0.87721] Prec[1.0] Recall[0.87721] F1[0.93453] => Worst<<Loss>>[0.56939] Found at [  2, 74]\n","[  3, 61] TokenID[[11858, 13577, 9843, 23999, 207, 207]] TokensDesc[telegram reprint acquitted vap the the] Loss[0.45894] Acc[0.89101] Prec[1.0] Recall[0.89101] F1[0.94225] => Worst<<Loss>>[0.56939] Found at [  2, 74]\n","[  3, 62] TokenID[[11858, 13577, 9843, 6162, 207, 207]] TokensDesc[telegram reprint acquitted predominant the the] Loss[0.42889] Acc[0.90095] Prec[1.0] Recall[0.90095] F1[0.94784] => Worst<<Loss>>[0.56939] Found at [  2, 74]\n","[  3, 63] TokenID[[11858, 13577, 9843, 17607, 207, 207]] TokensDesc[telegram reprint acquitted inequalit the the] Loss[0.4572] Acc[0.888] Prec[1.0] Recall[0.888] F1[0.94059] => Worst<<Loss>>[0.56939] Found at [  2, 74]\n","[  3, 64] TokenID[[11858, 13577, 9843, 16523, 207, 207]] TokensDesc[telegram reprint acquitted schlum the the] Loss[0.51836] Acc[0.86343] Prec[1.0] Recall[0.86343] F1[0.92661] => Worst<<Loss>>[0.56939] Found at [  2, 74]\n","[  3, 65] TokenID[[11858, 13577, 9843, 1619, 207, 207]] TokensDesc[telegram reprint acquitted intellectual the the] Loss[0.47819] Acc[0.88973] Prec[1.0] Recall[0.88973] F1[0.94153] => Worst<<Loss>>[0.56939] Found at [  2, 74]\n","[  3, 66] TokenID[[11858, 13577, 9843, 6453, 207, 207]] TokensDesc[telegram reprint acquitted perspective the the] Loss[0.42802] Acc[0.90737] Prec[1.0] Recall[0.90737] F1[0.95136] => Worst<<Loss>>[0.56939] Found at [  2, 74]\n","[  3, 67] TokenID[[11858, 13577, 9843, 10986, 207, 207]] TokensDesc[telegram reprint acquitted 1501 the the] Loss[0.47424] Acc[0.88013] Prec[1.0] Recall[0.88013] F1[0.93614] => Worst<<Loss>>[0.56939] Found at [  2, 74]\n","[  3, 68] TokenID[[11858, 13577, 9843, 12949, 207, 207]] TokensDesc[telegram reprint acquittedscale the the] Loss[0.45577] Acc[0.88759] Prec[1.0] Recall[0.88759] F1[0.94035] => Worst<<Loss>>[0.56939] Found at [  2, 74]\n","[  3, 69] TokenID[[11858, 13577, 9843, 6484, 207, 207]] TokensDesc[telegram reprint acquitted compar the the] Loss[0.50127] Acc[0.86984] Prec[1.0] Recall[0.86984] F1[0.93032] => Worst<<Loss>>[0.56939] Found at [  2, 74]\n","[  3, 70] TokenID[[11858, 13577, 9843, 4022, 207, 207]] TokensDesc[telegram reprint acquitted gd the the] Loss[0.45867] Acc[0.88718] Prec[1.0] Recall[0.88718] F1[0.94012] => Worst<<Loss>>[0.56939] Found at [  2, 74]\n","[  3, 71] TokenID[[11858, 13577, 9843, 22894, 207, 207]] TokensDesc[telegram reprint acquittedjima the the] Loss[0.46906] Acc[0.88424] Prec[1.0] Recall[0.88424] F1[0.93846] => Worst<<Loss>>[0.56939] Found at [  2, 74]\n","[  3, 72] TokenID[[11858, 13577, 9843, 28407, 207, 207]] TokensDesc[telegram reprint acquitted treasur the the] Loss[0.52406] Acc[0.86739] Prec[1.0] Recall[0.86739] F1[0.92882] => Worst<<Loss>>[0.56939] Found at [  2, 74]\n","[  3, 73] TokenID[[11858, 13577, 9843, 11058, 207, 207]] TokensDesc[telegram reprint acquitted carib the the] Loss[0.50824] Acc[0.86478] Prec[1.0] Recall[0.86478] F1[0.92739] => Worst<<Loss>>[0.56939] Found at [  2, 74]\n","[  3, 74] TokenID[[11858, 13577, 9843, 15211, 207, 207]] TokensDesc[telegram reprint acquitted socially the the] Loss[0.48857] Acc[0.87648] Prec[1.0] Recall[0.87648] F1[0.93405] => Worst<<Loss>>[0.56939] Found at [  2, 74]\n","[  3, 75] TokenID[[11858, 13577, 9843, 6572, 207, 207]] TokensDesc[telegram reprint acquitted brands the the] Loss[0.42585] Acc[0.90566] Prec[1.0] Recall[0.90566] F1[0.9504] => Worst<<Loss>>[0.56939] Found at [  2, 74]\n","[  3, 76] TokenID[[11858, 13577, 9843, 20584, 207, 207]] TokensDesc[telegram reprint acquitted nordisk the the] Loss[0.48051] Acc[0.876] Prec[1.0] Recall[0.876] F1[0.93379] => Worst<<Loss>>[0.56939] Found at [  2, 74]\n","[  3, 77] TokenID[[11858, 13577, 9843, 16452, 207, 207]] TokensDesc[telegram reprint acquitted interconnected the the] Loss[0.45979] Acc[0.88391] Prec[1.0] Recall[0.88391] F1[0.93828] => Worst<<Loss>>[0.56939] Found at [  2, 74]\n","[  3, 78] TokenID[[11858, 13577, 9843, 10218, 207, 207]] TokensDesc[telegram reprint acquitted jj the the] Loss[0.46054] Acc[0.88257] Prec[1.0] Recall[0.88257] F1[0.9375] => Worst<<Loss>>[0.56939] Found at [  2, 74]\n","[  3, 79] TokenID[[11858, 13577, 9843, 9356, 207, 207]] TokensDesc[telegram reprint acquitted systemic the the] Loss[0.43645] Acc[0.89415] Prec[1.0] Recall[0.89415] F1[0.94406] => Worst<<Loss>>[0.56939] Found at [  2, 74]\n","[  3, 80] TokenID[[11858, 13577, 9843, 3533, 207, 207]] TokensDesc[telegram reprint acquitted traditional the the] Loss[0.43046] Acc[0.90325] Prec[1.0] Recall[0.90325] F1[0.94906] => Worst<<Loss>>[0.56939] Found at [  2, 74]\n","[  3, 81] TokenID[[11858, 13577, 9843, 19968, 207, 207]] TokensDesc[telegram reprint acquitted respons the the] Loss[0.47242] Acc[0.87829] Prec[1.0] Recall[0.87829] F1[0.93511] => Worst<<Loss>>[0.56939] Found at [  2, 74]\n","[  3, 82] TokenID[[11858, 13577, 9843, 17692, 207, 207]] TokensDesc[telegram reprint acquittedtrop the the] Loss[0.47838] Acc[0.88257] Prec[1.0] Recall[0.88257] F1[0.93751] => Worst<<Loss>>[0.56939] Found at [  2, 74]\n","[  3, 83] TokenID[[11858, 13577, 9843, 20164, 207, 207]] TokensDesc[telegram reprint acquitted arguendo the the] Loss[0.51364] Acc[0.86082] Prec[1.0] Recall[0.86082] F1[0.9251] => Worst<<Loss>>[0.56939] Found at [  2, 74]\n","[  3, 84] TokenID[[11858, 13577, 9843, 29442, 207, 207]] TokensDesc[telegram reprint acquitted axiomati the the] Loss[0.50305] Acc[0.86316] Prec[1.0] Recall[0.86316] F1[0.92645] => Worst<<Loss>>[0.56939] Found at [  2, 74]\n","[  3, 85] TokenID[[11858, 13577, 9843, 26167, 207, 207]] TokensDesc[telegram reprint acquitted diverg the the] Loss[0.48136] Acc[0.87768] Prec[1.0] Recall[0.87768] F1[0.93475] => Worst<<Loss>>[0.56939] Found at [  2, 74]\n","[  3, 86] TokenID[[11858, 13577, 9843, 10263, 207, 207]] TokensDesc[telegram reprint acquitted transpos the the] Loss[0.47858] Acc[0.88354] Prec[1.0] Recall[0.88354] F1[0.93806] => Worst<<Loss>>[0.56939] Found at [  2, 74]\n","[  3, 87] TokenID[[11858, 13577, 9843, 17986, 207, 207]] TokensDesc[telegram reprint acquitted crystallin the the] Loss[0.48188] Acc[0.87958] Prec[1.0] Recall[0.87958] F1[0.93582] => Worst<<Loss>>[0.56939] Found at [  2, 74]\n","[  3, 88] TokenID[[11858, 13577, 9843, 24931, 207, 207]] TokensDesc[telegram reprint acquitted varian the the] Loss[0.50002] Acc[0.87404] Prec[1.0] Recall[0.87404] F1[0.93265] => Worst<<Loss>>[0.56939] Found at [  2, 74]\n","[  3, 89] TokenID[[11858, 13577, 9843, 11909, 207, 207]] TokensDesc[telegram reprint acquitted choices the the] Loss[0.46634] Acc[0.89597] Prec[1.0] Recall[0.89597] F1[0.94505] => Worst<<Loss>>[0.56939] Found at [  2, 74]\n","[  3, 90] TokenID[[11858, 13577, 9843, 10357, 207, 207]] TokensDesc[telegram reprint acquitted widespread the the] Loss[0.40122] Acc[0.91035] Prec[1.0] Recall[0.91035] F1[0.95301] => Worst<<Loss>>[0.56939] Found at [  2, 74]\n","[  3, 91] TokenID[[11858, 13577, 9843, 23288, 207, 207]] TokensDesc[telegram reprint acquitted leap the the] Loss[0.4649] Acc[0.88601] Prec[1.0] Recall[0.88601] F1[0.93946] => Worst<<Loss>>[0.56939] Found at [  2, 74]\n","[  3, 92] TokenID[[11858, 13577, 9843, 3354, 207, 207]] TokensDesc[telegram reprint acquitted territori the the] Loss[0.48339] Acc[0.87798] Prec[1.0] Recall[0.87798] F1[0.93492] => Worst<<Loss>>[0.56939] Found at [  2, 74]\n","[  3, 93] TokenID[[11858, 13577, 9843, 4025, 207, 207]] TokensDesc[telegram reprint acquitted functional the the] Loss[0.43099] Acc[0.90652] Prec[1.0] Recall[0.90652] F1[0.95089] => Worst<<Loss>>[0.56939] Found at [  2, 74]\n","[  3, 94] TokenID[[11858, 13577, 9843, 28172, 207, 207]] TokensDesc[telegram reprint acquitted conjuga the the] Loss[0.48634] Acc[0.87592] Prec[1.0] Recall[0.87592] F1[0.93372] => Worst<<Loss>>[0.56939] Found at [  2, 74]\n","[  3, 95] TokenID[[11858, 13577, 9843, 4998, 207, 207]] TokensDesc[telegram reprint acquitted historical the the] Loss[0.43385] Acc[0.89764] Prec[1.0] Recall[0.89764] F1[0.94601] => Worst<<Loss>>[0.56939] Found at [  2, 74]\n","[  3, 96] TokenID[[11858, 13577, 9843, 5306, 207, 207]] TokensDesc[telegram reprint acquitted mezzanin the the] Loss[0.48084] Acc[0.88623] Prec[1.0] Recall[0.88623] F1[0.93955] => Worst<<Loss>>[0.56939] Found at [  2, 74]\n","[  3, 97] TokenID[[11858, 13577, 9843, 1462, 207, 207]] TokensDesc[telegram reprint acquitted collectively the the] Loss[0.45228] Acc[0.89442] Prec[1.0] Recall[0.89442] F1[0.94419] => Worst<<Loss>>[0.56939] Found at [  2, 74]\n","[  3, 98] TokenID[[11858, 13577, 9843, 12650, 207, 207]] TokensDesc[telegram reprint acquitted interchangeable the the] Loss[0.46227] Acc[0.89855] Prec[1.0] Recall[0.89855] F1[0.94644] => Worst<<Loss>>[0.56939] Found at [  2, 74]\n","[  3, 99] TokenID[[11858, 13577, 9843, 2878, 207, 207]] TokensDesc[telegram reprint acquitted compared the the] Loss[0.45687] Acc[0.88923] Prec[1.0] Recall[0.88923] F1[0.94134] => Worst<<Loss>>[0.56939] Found at [  2, 74]\n","Worst loss 0.5693922489881516 with candidates [11858, 13577, 9843, 207, 207, 207] in the 3-iteration with tokens [telegram reprint acquitted the the the]\n","candidates [22087  2992 18951 17480 18086 19585 24137 18975 12726 18643 12369  5374\n"," 25072 20418  8059 15362  4022 27615 11790 13056 10003 24066  6360 28229\n"," 23087 19504 26207  7858 19794 11429 28932 21945 14387 28037 25161  5195\n","  2245  7455 15305  8169 16837 13784 11882 17916 15288  5306 16001 12059\n"," 12227 28907 11316 17300  9684  9374 24514 10186 10441  3207 10774 23780\n","  9495 15127 22197 22004  6932  6347 24469 30189 21288  6582 13899 14690\n"," 10251  8321 29011 21971 25566 19484  5930 27109 12085 15805 16681  6770\n"," 30011 20208 14997 27159 19768 12239 14098 23004 18102  3369 25236  8415\n"," 25852  6099 21523 20019]\n","[  4,  0] TokenID[[11858, 13577, 9843, 207, 22087, 207]] TokensDesc[telegram reprint acquitted the intranet the] Loss[0.45452] Acc[0.89446] Prec[1.0] Recall[0.89446] F1[0.94417] => Worst<<Loss>>[0.56939] Found at [  2, 74]\n","[  4,  1] TokenID[[11858, 13577, 9843, 207, 2992, 207]] TokensDesc[telegram reprint acquitted the suite the] Loss[0.53184] Acc[0.86662] Prec[1.0] Recall[0.86662] F1[0.92843] => Worst<<Loss>>[0.56939] Found at [  2, 74]\n","[  4,  2] TokenID[[11858, 13577, 9843, 207, 18951, 207]] TokensDesc[telegram reprint acquitted the outsource the] Loss[0.49883] Acc[0.87295] Prec[1.0] Recall[0.87295] F1[0.93205] => Worst<<Loss>>[0.56939] Found at [  2, 74]\n","[  4,  3] TokenID[[11858, 13577, 9843, 207, 17480, 207]] TokensDesc[telegram reprint acquitted the password the] Loss[0.52566] Acc[0.87317] Prec[1.0] Recall[0.87317] F1[0.93218] => Worst<<Loss>>[0.56939] Found at [  2, 74]\n","[  4,  4] TokenID[[11858, 13577, 9843, 207, 18086, 207]] TokensDesc[telegram reprint acquitted the directorship the] Loss[0.60015] Acc[0.82156] Prec[1.0] Recall[0.82156] F1[0.90198] => Worst<<Loss>>[0.60015] Found at [  4,  4]\n","[  4,  5] TokenID[[11858, 13577, 9843, 207, 19585, 207]] TokensDesc[telegram reprint acquitted the mobiliz the] Loss[0.56215] Acc[0.84152] Prec[1.0] Recall[0.84152] F1[0.91388] => Worst<<Loss>>[0.60015] Found at [  4,  4]\n","[  4,  6] TokenID[[11858, 13577, 9843, 207, 24137, 207]] TokensDesc[telegram reprint acquitted the distraction the] Loss[0.42276] Acc[0.89231] Prec[1.0] Recall[0.89231] F1[0.94305] => Worst<<Loss>>[0.60015] Found at [  4,  4]\n","[  4,  7] TokenID[[11858, 13577, 9843, 207, 18975, 207]] TokensDesc[telegram reprint acquitted the messag the] Loss[0.54012] Acc[0.84899] Prec[1.0] Recall[0.84899] F1[0.91823] => Worst<<Loss>>[0.60015] Found at [  4,  4]\n","[  4,  8] TokenID[[11858, 13577, 9843, 207, 12726, 207]] TokensDesc[telegram reprint acquitted theglades the] Loss[0.51159] Acc[0.85626] Prec[1.0] Recall[0.85626] F1[0.92251] => Worst<<Loss>>[0.60015] Found at [  4,  4]\n","[  4,  9] TokenID[[11858, 13577, 9843, 207, 18643, 207]] TokensDesc[telegram reprint acquitted the commute the] Loss[0.47589] Acc[0.87227] Prec[1.0] Recall[0.87227] F1[0.93168] => Worst<<Loss>>[0.60015] Found at [  4,  4]\n","[  4, 10] TokenID[[11858, 13577, 9843, 207, 12369, 207]] TokensDesc[telegram reprint acquitted the booking the] Loss[0.55172] Acc[0.84341] Prec[1.0] Recall[0.84341] F1[0.91493] => Worst<<Loss>>[0.60015] Found at [  4,  4]\n","[  4, 11] TokenID[[11858, 13577, 9843, 207, 5374, 207]] TokensDesc[telegram reprint acquitted the resort the] Loss[0.47621] Acc[0.87016] Prec[1.0] Recall[0.87016] F1[0.93049] => Worst<<Loss>>[0.60015] Found at [  4,  4]\n","[  4, 12] TokenID[[11858, 13577, 9843, 207, 25072, 207]] TokensDesc[telegram reprint acquitted thebbie the] Loss[0.48125] Acc[0.87249] Prec[1.0] Recall[0.87249] F1[0.93183] => Worst<<Loss>>[0.60015] Found at [  4,  4]\n","[  4, 13] TokenID[[11858, 13577, 9843, 207, 20418, 207]] TokensDesc[telegram reprint acquitted thequari the] Loss[0.51485] Acc[0.85991] Prec[1.0] Recall[0.85991] F1[0.92457] => Worst<<Loss>>[0.60015] Found at [  4,  4]\n","[  4, 14] TokenID[[11858, 13577, 9843, 207, 8059, 207]] TokensDesc[telegram reprint acquitted thepublished the] Loss[0.4747] Acc[0.87384] Prec[1.0] Recall[0.87384] F1[0.93261] => Worst<<Loss>>[0.60015] Found at [  4,  4]\n","[  4, 15] TokenID[[11858, 13577, 9843, 207, 15362, 207]] TokensDesc[telegram reprint acquitted the conduc the] Loss[0.48173] Acc[0.86962] Prec[1.0] Recall[0.86962] F1[0.93019] => Worst<<Loss>>[0.60015] Found at [  4,  4]\n","[  4, 16] TokenID[[11858, 13577, 9843, 207, 4022, 207]] TokensDesc[telegram reprint acquitted the gd the] Loss[0.48036] Acc[0.87947] Prec[1.0] Recall[0.87947] F1[0.9358] => Worst<<Loss>>[0.60015] Found at [  4,  4]\n","[  4, 17] TokenID[[11858, 13577, 9843, 207, 27615, 207]] TokensDesc[telegram reprint acquitted theconferenc the] Loss[0.53742] Acc[0.85152] Prec[1.0] Recall[0.85152] F1[0.91976] => Worst<<Loss>>[0.60015] Found at [  4,  4]\n","[  4, 18] TokenID[[11858, 13577, 9843, 207, 11790, 207]] TokensDesc[telegram reprint acquitted the relocate the] Loss[0.41112] Acc[0.91001] Prec[1.0] Recall[0.91001] F1[0.95277] => Worst<<Loss>>[0.60015] Found at [  4,  4]\n","[  4, 19] TokenID[[11858, 13577, 9843, 207, 13056, 207]] TokensDesc[telegram reprint acquitted the catastrophe the] Loss[0.4688] Acc[0.86954] Prec[1.0] Recall[0.86954] F1[0.93017] => Worst<<Loss>>[0.60015] Found at [  4,  4]\n","[  4, 20] TokenID[[11858, 13577, 9843, 207, 10003, 207]] TokensDesc[telegram reprint acquitted the payout the] Loss[0.52263] Acc[0.8599] Prec[1.0] Recall[0.8599] F1[0.92462] => Worst<<Loss>>[0.60015] Found at [  4,  4]\n","[  4, 21] TokenID[[11858, 13577, 9843, 207, 24066, 207]] TokensDesc[telegram reprint acquitted the 2252 the] Loss[0.49699] Acc[0.8621] Prec[1.0] Recall[0.8621] F1[0.92588] => Worst<<Loss>>[0.60015] Found at [  4,  4]\n","[  4, 22] TokenID[[11858, 13577, 9843, 207, 6360, 207]] TokensDesc[telegram reprint acquitted thespace the] Loss[0.50169] Acc[0.85631] Prec[1.0] Recall[0.85631] F1[0.92253] => Worst<<Loss>>[0.60015] Found at [  4,  4]\n","[  4, 23] TokenID[[11858, 13577, 9843, 207, 28229, 207]] TokensDesc[telegram reprint acquitted the brows the] Loss[0.49353] Acc[0.86771] Prec[1.0] Recall[0.86771] F1[0.92909] => Worst<<Loss>>[0.60015] Found at [  4,  4]\n","[  4, 24] TokenID[[11858, 13577, 9843, 207, 23087, 207]] TokensDesc[telegram reprint acquitted theteg the] Loss[0.46743] Acc[0.88507] Prec[1.0] Recall[0.88507] F1[0.93898] => Worst<<Loss>>[0.60015] Found at [  4,  4]\n","[  4, 25] TokenID[[11858, 13577, 9843, 207, 19504, 207]] TokensDesc[telegram reprint acquitted the charlesto the] Loss[0.53486] Acc[0.84721] Prec[1.0] Recall[0.84721] F1[0.91723] => Worst<<Loss>>[0.60015] Found at [  4,  4]\n","[  4, 26] TokenID[[11858, 13577, 9843, 207, 26207, 207]] TokensDesc[telegram reprint acquitted the quay the] Loss[0.54994] Acc[0.85124] Prec[1.0] Recall[0.85124] F1[0.91948] => Worst<<Loss>>[0.60015] Found at [  4,  4]\n","[  4, 27] TokenID[[11858, 13577, 9843, 207, 7858, 207]] TokensDesc[telegram reprint acquitted the server the] Loss[0.54068] Acc[0.8506] Prec[1.0] Recall[0.8506] F1[0.91914] => Worst<<Loss>>[0.60015] Found at [  4,  4]\n","[  4, 28] TokenID[[11858, 13577, 9843, 207, 19794, 207]] TokensDesc[telegram reprint acquitted themai the] Loss[0.49535] Acc[0.86446] Prec[1.0] Recall[0.86446] F1[0.92722] => Worst<<Loss>>[0.60015] Found at [  4,  4]\n","[  4, 29] TokenID[[11858, 13577, 9843, 207, 11429, 207]] TokensDesc[telegram reprint acquitted the hosp the] Loss[0.47752] Acc[0.87403] Prec[1.0] Recall[0.87403] F1[0.93269] => Worst<<Loss>>[0.60015] Found at [  4,  4]\n","[  4, 30] TokenID[[11858, 13577, 9843, 207, 28932, 207]] TokensDesc[telegram reprint acquitted thequill the] Loss[0.5086] Acc[0.86009] Prec[1.0] Recall[0.86009] F1[0.92468] => Worst<<Loss>>[0.60015] Found at [  4,  4]\n","[  4, 31] TokenID[[11858, 13577, 9843, 207, 21945, 207]] TokensDesc[telegram reprint acquitted the centuri the] Loss[0.47751] Acc[0.87641] Prec[1.0] Recall[0.87641] F1[0.93407] => Worst<<Loss>>[0.60015] Found at [  4,  4]\n","[  4, 32] TokenID[[11858, 13577, 9843, 207, 14387, 207]] TokensDesc[telegram reprint acquitted thefamily the] Loss[0.50889] Acc[0.86664] Prec[1.0] Recall[0.86664] F1[0.92847] => Worst<<Loss>>[0.60015] Found at [  4,  4]\n","[  4, 33] TokenID[[11858, 13577, 9843, 207, 28037, 207]] TokensDesc[telegram reprint acquitted the suicidal the] Loss[0.50236] Acc[0.85653] Prec[1.0] Recall[0.85653] F1[0.92266] => Worst<<Loss>>[0.60015] Found at [  4,  4]\n","[  4, 34] TokenID[[11858, 13577, 9843, 207, 25161, 207]] TokensDesc[telegram reprint acquitted the videoconferenc the] Loss[0.48263] Acc[0.87413] Prec[1.0] Recall[0.87413] F1[0.93278] => Worst<<Loss>>[0.60015] Found at [  4,  4]\n","[  4, 35] TokenID[[11858, 13577, 9843, 207, 5195, 207]] TokensDesc[telegram reprint acquitted the 319 the] Loss[0.50383] Acc[0.86014] Prec[1.0] Recall[0.86014] F1[0.92471] => Worst<<Loss>>[0.60015] Found at [  4,  4]\n","[  4, 36] TokenID[[11858, 13577, 9843, 207, 2245, 207]] TokensDesc[telegram reprint acquitted the 409 the] Loss[0.51317] Acc[0.86282] Prec[1.0] Recall[0.86282] F1[0.92625] => Worst<<Loss>>[0.60015] Found at [  4,  4]\n","[  4, 37] TokenID[[11858, 13577, 9843, 207, 7455, 207]] TokensDesc[telegram reprint acquitted the boule the] Loss[0.51165] Acc[0.86044] Prec[1.0] Recall[0.86044] F1[0.92488] => Worst<<Loss>>[0.60015] Found at [  4,  4]\n","[  4, 38] TokenID[[11858, 13577, 9843, 207, 15305, 207]] TokensDesc[telegram reprint acquitted the subservic the] Loss[0.55426] Acc[0.85175] Prec[1.0] Recall[0.85175] F1[0.91986] => Worst<<Loss>>[0.60015] Found at [  4,  4]\n","[  4, 39] TokenID[[11858, 13577, 9843, 207, 8169, 207]] TokensDesc[telegram reprint acquitted the inconvenien the] Loss[0.49169] Acc[0.86988] Prec[1.0] Recall[0.86988] F1[0.93034] => Worst<<Loss>>[0.60015] Found at [  4,  4]\n","[  4, 40] TokenID[[11858, 13577, 9843, 207, 16837, 207]] TokensDesc[telegram reprint acquitted thewlett the] Loss[0.50543] Acc[0.86155] Prec[1.0] Recall[0.86155] F1[0.92556] => Worst<<Loss>>[0.60015] Found at [  4,  4]\n","[  4, 41] TokenID[[11858, 13577, 9843, 207, 13784, 207]] TokensDesc[telegram reprint acquitted theauli the] Loss[0.48497] Acc[0.87514] Prec[1.0] Recall[0.87514] F1[0.93335] => Worst<<Loss>>[0.60015] Found at [  4,  4]\n","[  4, 42] TokenID[[11858, 13577, 9843, 207, 11882, 207]] TokensDesc[telegram reprint acquitted the mdc the] Loss[0.50249] Acc[0.86065] Prec[1.0] Recall[0.86065] F1[0.92501] => Worst<<Loss>>[0.60015] Found at [  4,  4]\n","[  4, 43] TokenID[[11858, 13577, 9843, 207, 17916, 207]] TokensDesc[telegram reprint acquitted the 1692 the] Loss[0.50306] Acc[0.85728] Prec[1.0] Recall[0.85728] F1[0.92307] => Worst<<Loss>>[0.60015] Found at [  4,  4]\n","[  4, 44] TokenID[[11858, 13577, 9843, 207, 15288, 207]] TokensDesc[telegram reprint acquitted the demise the] Loss[0.47324] Acc[0.86471] Prec[1.0] Recall[0.86471] F1[0.92735] => Worst<<Loss>>[0.60015] Found at [  4,  4]\n","[  4, 45] TokenID[[11858, 13577, 9843, 207, 5306, 207]] TokensDesc[telegram reprint acquitted the mezzanin the] Loss[0.48815] Acc[0.86977] Prec[1.0] Recall[0.86977] F1[0.93027] => Worst<<Loss>>[0.60015] Found at [  4,  4]\n","[  4, 46] TokenID[[11858, 13577, 9843, 207, 16001, 207]] TokensDesc[telegram reprint acquitted the dedication the] Loss[0.42926] Acc[0.89198] Prec[1.0] Recall[0.89198] F1[0.94287] => Worst<<Loss>>[0.60015] Found at [  4,  4]\n","[  4, 47] TokenID[[11858, 13577, 9843, 207, 12059, 207]] TokensDesc[telegram reprint acquitted the poste the] Loss[0.51636] Acc[0.84993] Prec[1.0] Recall[0.84993] F1[0.91884] => Worst<<Loss>>[0.60015] Found at [  4,  4]\n","[  4, 48] TokenID[[11858, 13577, 9843, 207, 12227, 207]] TokensDesc[telegram reprint acquitted thexp the] Loss[0.52693] Acc[0.85829] Prec[1.0] Recall[0.85829] F1[0.92368] => Worst<<Loss>>[0.60015] Found at [  4,  4]\n","[  4, 49] TokenID[[11858, 13577, 9843, 207, 28907, 207]] TokensDesc[telegram reprint acquitted the paramilitary the] Loss[0.53233] Acc[0.85476] Prec[1.0] Recall[0.85476] F1[0.92162] => Worst<<Loss>>[0.60015] Found at [  4,  4]\n","[  4, 50] TokenID[[11858, 13577, 9843, 207, 11316, 207]] TokensDesc[telegram reprint acquitted the 085 the] Loss[0.49596] Acc[0.86023] Prec[1.0] Recall[0.86023] F1[0.92482] => Worst<<Loss>>[0.60015] Found at [  4,  4]\n","[  4, 51] TokenID[[11858, 13577, 9843, 207, 17300, 207]] TokensDesc[telegram reprint acquitted the 2988 the] Loss[0.49938] Acc[0.86677] Prec[1.0] Recall[0.86677] F1[0.92855] => Worst<<Loss>>[0.60015] Found at [  4,  4]\n","[  4, 52] TokenID[[11858, 13577, 9843, 207, 9684, 207]] TokensDesc[telegram reprint acquitted the prince the] Loss[0.5283] Acc[0.84847] Prec[1.0] Recall[0.84847] F1[0.91791] => Worst<<Loss>>[0.60015] Found at [  4,  4]\n","[  4, 53] TokenID[[11858, 13577, 9843, 207, 9374, 207]] TokensDesc[telegram reprint acquitted the sail the] Loss[0.5626] Acc[0.84642] Prec[1.0] Recall[0.84642] F1[0.91673] => Worst<<Loss>>[0.60015] Found at [  4,  4]\n","[  4, 54] TokenID[[11858, 13577, 9843, 207, 24514, 207]] TokensDesc[telegram reprint acquitted the boca the] Loss[0.5044] Acc[0.85922] Prec[1.0] Recall[0.85922] F1[0.92423] => Worst<<Loss>>[0.60015] Found at [  4,  4]\n","[  4, 55] TokenID[[11858, 13577, 9843, 207, 10186, 207]] TokensDesc[telegram reprint acquitted thedrome the] Loss[0.55224] Acc[0.84967] Prec[1.0] Recall[0.84967] F1[0.91864] => Worst<<Loss>>[0.60015] Found at [  4,  4]\n","[  4, 56] TokenID[[11858, 13577, 9843, 207, 10441, 207]] TokensDesc[telegram reprint acquitted thehusk the] Loss[0.50731] Acc[0.86415] Prec[1.0] Recall[0.86415] F1[0.92704] => Worst<<Loss>>[0.60015] Found at [  4,  4]\n","[  4, 57] TokenID[[11858, 13577, 9843, 207, 3207, 207]] TokensDesc[telegram reprint acquitted theturn the] Loss[0.47008] Acc[0.87829] Prec[1.0] Recall[0.87829] F1[0.93515] => Worst<<Loss>>[0.60015] Found at [  4,  4]\n","[  4, 58] TokenID[[11858, 13577, 9843, 207, 10774, 207]] TokensDesc[telegram reprint acquitted themature the] Loss[0.49764] Acc[0.8658] Prec[1.0] Recall[0.8658] F1[0.92799] => Worst<<Loss>>[0.60015] Found at [  4,  4]\n","[  4, 59] TokenID[[11858, 13577, 9843, 207, 23780, 207]] TokensDesc[telegram reprint acquitted the informatic the] Loss[0.50575] Acc[0.86412] Prec[1.0] Recall[0.86412] F1[0.92702] => Worst<<Loss>>[0.60015] Found at [  4,  4]\n","[  4, 60] TokenID[[11858, 13577, 9843, 207, 9495, 207]] TokensDesc[telegram reprint acquitted theafr the] Loss[0.5214] Acc[0.86123] Prec[1.0] Recall[0.86123] F1[0.92533] => Worst<<Loss>>[0.60015] Found at [  4,  4]\n","[  4, 61] TokenID[[11858, 13577, 9843, 207, 15127, 207]] TokensDesc[telegram reprint acquitted theload the] Loss[0.50648] Acc[0.86431] Prec[1.0] Recall[0.86431] F1[0.92715] => Worst<<Loss>>[0.60015] Found at [  4,  4]\n","[  4, 62] TokenID[[11858, 13577, 9843, 207, 22197, 207]] TokensDesc[telegram reprint acquitted the workflow the] Loss[0.52133] Acc[0.86141] Prec[1.0] Recall[0.86141] F1[0.92546] => Worst<<Loss>>[0.60015] Found at [  4,  4]\n","[  4, 63] TokenID[[11858, 13577, 9843, 207, 22004, 207]] TokensDesc[telegram reprint acquitted the904 the] Loss[0.48677] Acc[0.87739] Prec[1.0] Recall[0.87739] F1[0.93459] => Worst<<Loss>>[0.60015] Found at [  4,  4]\n","[  4, 64] TokenID[[11858, 13577, 9843, 207, 6932, 207]] TokensDesc[telegram reprint acquitted the prospect the] Loss[0.551] Acc[0.84735] Prec[1.0] Recall[0.84735] F1[0.91728] => Worst<<Loss>>[0.60015] Found at [  4,  4]\n","[  4, 65] TokenID[[11858, 13577, 9843, 207, 6347, 207]] TokensDesc[telegram reprint acquitted the 334 the] Loss[0.49448] Acc[0.85518] Prec[1.0] Recall[0.85518] F1[0.9219] => Worst<<Loss>>[0.60015] Found at [  4,  4]\n","[  4, 66] TokenID[[11858, 13577, 9843, 207, 24469, 207]] TokensDesc[telegram reprint acquitted the childbirth the] Loss[0.5021] Acc[0.86592] Prec[1.0] Recall[0.86592] F1[0.92806] => Worst<<Loss>>[0.60015] Found at [  4,  4]\n","[  4, 67] TokenID[[11858, 13577, 9843, 207, 30189, 207]] TokensDesc[telegram reprint acquitted thesuite the] Loss[0.52413] Acc[0.85101] Prec[1.0] Recall[0.85101] F1[0.91946] => Worst<<Loss>>[0.60015] Found at [  4,  4]\n","[  4, 68] TokenID[[11858, 13577, 9843, 207, 21288, 207]] TokensDesc[telegram reprint acquitted theufa the] Loss[0.51144] Acc[0.86219] Prec[1.0] Recall[0.86219] F1[0.92589] => Worst<<Loss>>[0.60015] Found at [  4,  4]\n","[  4, 69] TokenID[[11858, 13577, 9843, 207, 6582, 207]] TokensDesc[telegram reprint acquitted the ku the] Loss[0.49894] Acc[0.86054] Prec[1.0] Recall[0.86054] F1[0.92498] => Worst<<Loss>>[0.60015] Found at [  4,  4]\n","[  4, 70] TokenID[[11858, 13577, 9843, 207, 13899, 207]] TokensDesc[telegram reprint acquitted theagi the] Loss[0.50353] Acc[0.86466] Prec[1.0] Recall[0.86466] F1[0.92733] => Worst<<Loss>>[0.60015] Found at [  4,  4]\n","[  4, 71] TokenID[[11858, 13577, 9843, 207, 14690, 207]] TokensDesc[telegram reprint acquitted the chargeback the] Loss[0.51924] Acc[0.85025] Prec[1.0] Recall[0.85025] F1[0.919] => Worst<<Loss>>[0.60015] Found at [  4,  4]\n","[  4, 72] TokenID[[11858, 13577, 9843, 207, 10251, 207]] TokensDesc[telegram reprint acquitted the lockbox the] Loss[0.5558] Acc[0.84474] Prec[1.0] Recall[0.84474] F1[0.91576] => Worst<<Loss>>[0.60015] Found at [  4,  4]\n","[  4, 73] TokenID[[11858, 13577, 9843, 207, 8321, 207]] TokensDesc[telegram reprint acquitted thecurrency the] Loss[0.50481] Acc[0.86553] Prec[1.0] Recall[0.86553] F1[0.92783] => Worst<<Loss>>[0.60015] Found at [  4,  4]\n","[  4, 74] TokenID[[11858, 13577, 9843, 207, 29011, 207]] TokensDesc[telegram reprint acquitted the ryde the] Loss[0.53953] Acc[0.85134] Prec[1.0] Recall[0.85134] F1[0.91962] => Worst<<Loss>>[0.60015] Found at [  4,  4]\n","[  4, 75] TokenID[[11858, 13577, 9843, 207, 21971, 207]] TokensDesc[telegram reprint acquitted the snack the] Loss[0.52351] Acc[0.86062] Prec[1.0] Recall[0.86062] F1[0.92498] => Worst<<Loss>>[0.60015] Found at [  4,  4]\n","[  4, 76] TokenID[[11858, 13577, 9843, 207, 25566, 207]] TokensDesc[telegram reprint acquitted the statue the] Loss[0.5425] Acc[0.85195] Prec[1.0] Recall[0.85195] F1[0.91997] => Worst<<Loss>>[0.60015] Found at [  4,  4]\n","[  4, 77] TokenID[[11858, 13577, 9843, 207, 19484, 207]] TokensDesc[telegram reprint acquitted the laptop the] Loss[0.49791] Acc[0.87364] Prec[1.0] Recall[0.87364] F1[0.93248] => Worst<<Loss>>[0.60015] Found at [  4,  4]\n","[  4, 78] TokenID[[11858, 13577, 9843, 207, 5930, 207]] TokensDesc[telegram reprint acquitted the subsystem the] Loss[0.5906] Acc[0.83778] Prec[1.0] Recall[0.83778] F1[0.91161] => Worst<<Loss>>[0.60015] Found at [  4,  4]\n","[  4, 79] TokenID[[11858, 13577, 9843, 207, 27109, 207]] TokensDesc[telegram reprint acquitted the commut the] Loss[0.47635] Acc[0.8726] Prec[1.0] Recall[0.8726] F1[0.9319] => Worst<<Loss>>[0.60015] Found at [  4,  4]\n","[  4, 80] TokenID[[11858, 13577, 9843, 207, 12085, 207]] TokensDesc[telegram reprint acquitted thebot the] Loss[0.47425] Acc[0.86688] Prec[1.0] Recall[0.86688] F1[0.92861] => Worst<<Loss>>[0.60015] Found at [  4,  4]\n","[  4, 81] TokenID[[11858, 13577, 9843, 207, 15805, 207]] TokensDesc[telegram reprint acquitted the litigat the] Loss[0.4897] Acc[0.87006] Prec[1.0] Recall[0.87006] F1[0.93044] => Worst<<Loss>>[0.60015] Found at [  4,  4]\n","[  4, 82] TokenID[[11858, 13577, 9843, 207, 16681, 207]] TokensDesc[telegram reprint acquitted the elevate the] Loss[0.43475] Acc[0.88837] Prec[1.0] Recall[0.88837] F1[0.94084] => Worst<<Loss>>[0.60015] Found at [  4,  4]\n","[  4, 83] TokenID[[11858, 13577, 9843, 207, 6770, 207]] TokensDesc[telegram reprint acquitted the 662 the] Loss[0.49868] Acc[0.8636] Prec[1.0] Recall[0.8636] F1[0.92672] => Worst<<Loss>>[0.60015] Found at [  4,  4]\n","[  4, 84] TokenID[[11858, 13577, 9843, 207, 30011, 207]] TokensDesc[telegram reprint acquitted the distract the] Loss[0.48774] Acc[0.85907] Prec[1.0] Recall[0.85907] F1[0.92414] => Worst<<Loss>>[0.60015] Found at [  4,  4]\n","[  4, 85] TokenID[[11858, 13577, 9843, 207, 20208, 207]] TokensDesc[telegram reprint acquitted thedesk the] Loss[0.47915] Acc[0.8714] Prec[1.0] Recall[0.8714] F1[0.9312] => Worst<<Loss>>[0.60015] Found at [  4,  4]\n","[  4, 86] TokenID[[11858, 13577, 9843, 207, 14997, 207]] TokensDesc[telegram reprint acquitted thelop the] Loss[0.47153] Acc[0.87221] Prec[1.0] Recall[0.87221] F1[0.93166] => Worst<<Loss>>[0.60015] Found at [  4,  4]\n","[  4, 87] TokenID[[11858, 13577, 9843, 207, 27159, 207]] TokensDesc[telegram reprint acquitted the 4042 the] Loss[0.53667] Acc[0.84277] Prec[1.0] Recall[0.84277] F1[0.9146] => Worst<<Loss>>[0.60015] Found at [  4,  4]\n","[  4, 88] TokenID[[11858, 13577, 9843, 207, 19768, 207]] TokensDesc[telegram reprint acquitted the 1287 the] Loss[0.48541] Acc[0.8648] Prec[1.0] Recall[0.8648] F1[0.92742] => Worst<<Loss>>[0.60015] Found at [  4,  4]\n","[  4, 89] TokenID[[11858, 13577, 9843, 207, 12239, 207]] TokensDesc[telegram reprint acquitted theevo the] Loss[0.46431] Acc[0.87242] Prec[1.0] Recall[0.87242] F1[0.93179] => Worst<<Loss>>[0.60015] Found at [  4,  4]\n","[  4, 90] TokenID[[11858, 13577, 9843, 207, 14098, 207]] TokensDesc[telegram reprint acquitted theogle the] Loss[0.46078] Acc[0.87779] Prec[1.0] Recall[0.87779] F1[0.93485] => Worst<<Loss>>[0.60015] Found at [  4,  4]\n","[  4, 91] TokenID[[11858, 13577, 9843, 207, 23004, 207]] TokensDesc[telegram reprint acquitted the cfm the] Loss[0.53915] Acc[0.84854] Prec[1.0] Recall[0.84854] F1[0.91803] => Worst<<Loss>>[0.60015] Found at [  4,  4]\n","[  4, 92] TokenID[[11858, 13577, 9843, 207, 18102, 207]] TokensDesc[telegram reprint acquitted theectors the] Loss[0.49088] Acc[0.87268] Prec[1.0] Recall[0.87268] F1[0.93195] => Worst<<Loss>>[0.60015] Found at [  4,  4]\n","[  4, 93] TokenID[[11858, 13577, 9843, 207, 3369, 207]] TokensDesc[telegram reprint acquitted the forum the] Loss[0.51189] Acc[0.85521] Prec[1.0] Recall[0.85521] F1[0.92193] => Worst<<Loss>>[0.60015] Found at [  4,  4]\n","[  4, 94] TokenID[[11858, 13577, 9843, 207, 25236, 207]] TokensDesc[telegram reprint acquitted the accustom the] Loss[0.4912] Acc[0.86598] Prec[1.0] Recall[0.86598] F1[0.9281] => Worst<<Loss>>[0.60015] Found at [  4,  4]\n","[  4, 95] TokenID[[11858, 13577, 9843, 207, 8415, 207]] TokensDesc[telegram reprint acquitted the wfb the] Loss[0.53293] Acc[0.85468] Prec[1.0] Recall[0.85468] F1[0.92156] => Worst<<Loss>>[0.60015] Found at [  4,  4]\n","[  4, 96] TokenID[[11858, 13577, 9843, 207, 25852, 207]] TokensDesc[telegram reprint acquitted theimica the] Loss[0.47839] Acc[0.8699] Prec[1.0] Recall[0.8699] F1[0.93038] => Worst<<Loss>>[0.60015] Found at [  4,  4]\n","[  4, 97] TokenID[[11858, 13577, 9843, 207, 6099, 207]] TokensDesc[telegram reprint acquitted the 326 the] Loss[0.49624] Acc[0.87009] Prec[1.0] Recall[0.87009] F1[0.93046] => Worst<<Loss>>[0.60015] Found at [  4,  4]\n","[  4, 98] TokenID[[11858, 13577, 9843, 207, 21523, 207]] TokensDesc[telegram reprint acquitted the buoy the] Loss[0.55838] Acc[0.84541] Prec[1.0] Recall[0.84541] F1[0.91611] => Worst<<Loss>>[0.60015] Found at [  4,  4]\n","[  4, 99] TokenID[[11858, 13577, 9843, 207, 20019, 207]] TokensDesc[telegram reprint acquitted the expos the] Loss[0.47585] Acc[0.87645] Prec[1.0] Recall[0.87645] F1[0.93409] => Worst<<Loss>>[0.60015] Found at [  4,  4]\n","Worst loss 0.6001514792442322 with candidates [11858, 13577, 9843, 207, 18086, 207] in the 4-iteration with tokens [telegram reprint acquitted the directorship the]\n","candidates [ 4025  1884  7320  2117  6428  2403   819  2574  1486  5920  4152   581\n","  2689 10089  1545  1378  2747  6743   283 15287  7088 24621 15346 28022\n","  1175  3956 11790 10933  3238  6114 16377  4030  2867  4980  2554   358\n","  1706   936  3466  3102  1396 21745  2268  1625  2005  7843  4081 17746\n","  3986   215 14805 15801  2818  2878  2389  2585  5692  9235 23196  1459\n","  8687 12209  1472  1845  4920 14044   901   512  8350  4524 10082   487\n","   808  2205  2489 25262  2986 18765  6662  4741  1309   761  1305  2641\n","  5424  3126 12905  3708   268  9920  2922  4805 19149   247  2480  2755\n","  4981  5376  6242  2010]\n","[  5,  0] TokenID[[11858, 13577, 9843, 207, 18086, 4025]] TokensDesc[telegram reprint acquitted the directorship functional] Loss[0.4812] Acc[0.88204] Prec[1.0] Recall[0.88204] F1[0.93724] => Worst<<Loss>>[0.60015] Found at [  4,  4]\n","[  5,  1] TokenID[[11858, 13577, 9843, 207, 18086, 1884]] TokensDesc[telegram reprint acquitted the directorship currently] Loss[0.51278] Acc[0.87275] Prec[1.0] Recall[0.87275] F1[0.93198] => Worst<<Loss>>[0.60015] Found at [  4,  4]\n","[  5,  2] TokenID[[11858, 13577, 9843, 207, 18086, 7320]] TokensDesc[telegram reprint acquitted the directorship flexi] Loss[0.47411] Acc[0.87976] Prec[1.0] Recall[0.87976] F1[0.93589] => Worst<<Loss>>[0.60015] Found at [  4,  4]\n","[  5,  3] TokenID[[11858, 13577, 9843, 207, 18086, 2117]] TokensDesc[telegram reprint acquitted the directorship separation] Loss[0.50868] Acc[0.86404] Prec[1.0] Recall[0.86404] F1[0.92695] => Worst<<Loss>>[0.60015] Found at [  4,  4]\n","[  5,  4] TokenID[[11858, 13577, 9843, 207, 18086, 6428]] TokensDesc[telegram reprint acquitted the directorship flexibl] Loss[0.49727] Acc[0.87217] Prec[1.0] Recall[0.87217] F1[0.93162] => Worst<<Loss>>[0.60015] Found at [  4,  4]\n","[  5,  5] TokenID[[11858, 13577, 9843, 207, 18086, 2403]] TokensDesc[telegram reprint acquitted the directorship grow] Loss[0.47736] Acc[0.88075] Prec[1.0] Recall[0.88075] F1[0.93652] => Worst<<Loss>>[0.60015] Found at [  4,  4]\n","[  5,  6] TokenID[[11858, 13577, 9843, 207, 18086, 819]] TokensDesc[telegram reprint acquitted the directorship mean] Loss[0.59362] Acc[0.82308] Prec[1.0] Recall[0.82308] F1[0.90289] => Worst<<Loss>>[0.60015] Found at [  4,  4]\n","[  5,  7] TokenID[[11858, 13577, 9843, 207, 18086, 2574]] TokensDesc[telegram reprint acquitted the directorship achieve] Loss[0.5001] Acc[0.87584] Prec[1.0] Recall[0.87584] F1[0.93371] => Worst<<Loss>>[0.60015] Found at [  4,  4]\n","[  5,  8] TokenID[[11858, 13577, 9843, 207, 18086, 1486]] TokensDesc[telegram reprint acquitted the directorship like] Loss[0.56263] Acc[0.84237] Prec[1.0] Recall[0.84237] F1[0.91436] => Worst<<Loss>>[0.60015] Found at [  4,  4]\n","[  5,  9] TokenID[[11858, 13577, 9843, 207, 18086, 5920]] TokensDesc[telegram reprint acquitted the directorship separated] Loss[0.50805] Acc[0.86714] Prec[1.0] Recall[0.86714] F1[0.92877] => Worst<<Loss>>[0.60015] Found at [  4,  4]\n","[  5, 10] TokenID[[11858, 13577, 9843, 207, 18086, 4152]] TokensDesc[telegram reprint acquitted the directorship across] Loss[0.5142] Acc[0.86972] Prec[1.0] Recall[0.86972] F1[0.93021] => Worst<<Loss>>[0.60015] Found at [  4,  4]\n","[  5, 11] TokenID[[11858, 13577, 9843, 207, 18086, 581]] TokensDesc[telegram reprint acquitted the directorship change] Loss[0.41287] Acc[0.8974] Prec[1.0] Recall[0.8974] F1[0.94584] => Worst<<Loss>>[0.60015] Found at [  4,  4]\n","[  5, 12] TokenID[[11858, 13577, 9843, 207, 18086, 2689]] TokensDesc[telegram reprint acquitted the directorship better] Loss[0.50723] Acc[0.87527] Prec[1.0] Recall[0.87527] F1[0.93336] => Worst<<Loss>>[0.60015] Found at [  4,  4]\n","[  5, 13] TokenID[[11858, 13577, 9843, 207, 18086, 10089]] TokensDesc[telegram reprint acquitted the directorship sleep] Loss[0.48315] Acc[0.88225] Prec[1.0] Recall[0.88225] F1[0.93736] => Worst<<Loss>>[0.60015] Found at [  4,  4]\n","[  5, 14] TokenID[[11858, 13577, 9843, 207, 18086, 1545]] TokensDesc[telegram reprint acquitted the directorship consistent] Loss[0.50318] Acc[0.87474] Prec[1.0] Recall[0.87474] F1[0.93311] => Worst<<Loss>>[0.60015] Found at [  4,  4]\n","[  5, 15] TokenID[[11858, 13577, 9843, 207, 18086, 1378]] TokensDesc[telegram reprint acquitted the directorship changes] Loss[0.39567] Acc[0.91457] Prec[1.0] Recall[0.91457] F1[0.95529] => Worst<<Loss>>[0.60015] Found at [  4,  4]\n","[  5, 16] TokenID[[11858, 13577, 9843, 207, 18086, 2747]] TokensDesc[telegram reprint acquitted the directorship achieved] Loss[0.48157] Acc[0.88875] Prec[1.0] Recall[0.88875] F1[0.94102] => Worst<<Loss>>[0.60015] Found at [  4,  4]\n","[  5, 17] TokenID[[11858, 13577, 9843, 207, 18086, 6743]] TokensDesc[telegram reprint acquitted the directorship differently] Loss[0.4755] Acc[0.87897] Prec[1.0] Recall[0.87897] F1[0.93548] => Worst<<Loss>>[0.60015] Found at [  4,  4]\n","[  5, 18] TokenID[[11858, 13577, 9843, 207, 18086, 283]] TokensDesc[telegram reprint acquitted the directorship including] Loss[0.51803] Acc[0.86455] Prec[1.0] Recall[0.86455] F1[0.92729] => Worst<<Loss>>[0.60015] Found at [  4,  4]\n","[  5, 19] TokenID[[11858, 13577, 9843, 207, 18086, 15287]] TokensDesc[telegram reprint acquitted the directorshipsight] Loss[0.47516] Acc[0.87682] Prec[1.0] Recall[0.87682] F1[0.93428] => Worst<<Loss>>[0.60015] Found at [  4,  4]\n","[  5, 20] TokenID[[11858, 13577, 9843, 207, 18086, 7088]] TokensDesc[telegram reprint acquitted the directorship smart] Loss[0.53615] Acc[0.85484] Prec[1.0] Recall[0.85484] F1[0.92168] => Worst<<Loss>>[0.60015] Found at [  4,  4]\n","[  5, 21] TokenID[[11858, 13577, 9843, 207, 18086, 24621]] TokensDesc[telegram reprint acquitted the directorship efficienc] Loss[0.49182] Acc[0.87667] Prec[1.0] Recall[0.87667] F1[0.93418] => Worst<<Loss>>[0.60015] Found at [  4,  4]\n","[  5, 22] TokenID[[11858, 13577, 9843, 207, 18086, 15346]] TokensDesc[telegram reprint acquitted the directorship maximize] Loss[0.44886] Acc[0.89284] Prec[1.0] Recall[0.89284] F1[0.94333] => Worst<<Loss>>[0.60015] Found at [  4,  4]\n","[  5, 23] TokenID[[11858, 13577, 9843, 207, 18086, 28022]] TokensDesc[telegram reprint acquitted the directorship blurr] Loss[0.48982] Acc[0.87709] Prec[1.0] Recall[0.87709] F1[0.93442] => Worst<<Loss>>[0.60015] Found at [  4,  4]\n","[  5, 24] TokenID[[11858, 13577, 9843, 207, 18086, 1175]] TokensDesc[telegram reprint acquitted the directorship systems] Loss[0.45808] Acc[0.88921] Prec[1.0] Recall[0.88921] F1[0.94129] => Worst<<Loss>>[0.60015] Found at [  4,  4]\n","[  5, 25] TokenID[[11858, 13577, 9843, 207, 18086, 3956]] TokensDesc[telegram reprint acquitted the directorship geographic] Loss[0.46008] Acc[0.89397] Prec[1.0] Recall[0.89397] F1[0.94395] => Worst<<Loss>>[0.60015] Found at [  4,  4]\n","[  5, 26] TokenID[[11858, 13577, 9843, 207, 18086, 11790]] TokensDesc[telegram reprint acquitted the directorship relocate] Loss[0.4289] Acc[0.9016] Prec[1.0] Recall[0.9016] F1[0.94812] => Worst<<Loss>>[0.60015] Found at [  4,  4]\n","[  5, 27] TokenID[[11858, 13577, 9843, 207, 18086, 10933]] TokensDesc[telegram reprint acquitted the directorship splits] Loss[0.47487] Acc[0.88418] Prec[1.0] Recall[0.88418] F1[0.93845] => Worst<<Loss>>[0.60015] Found at [  4,  4]\n","[  5, 28] TokenID[[11858, 13577, 9843, 207, 18086, 3238]] TokensDesc[telegram reprint acquitted the directorship transition] Loss[0.47633] Acc[0.87755] Prec[1.0] Recall[0.87755] F1[0.9347] => Worst<<Loss>>[0.60015] Found at [  4,  4]\n","[  5, 29] TokenID[[11858, 13577, 9843, 207, 18086, 6114]] TokensDesc[telegram reprint acquitted the directorship barriers] Loss[0.49048] Acc[0.87893] Prec[1.0] Recall[0.87893] F1[0.93547] => Worst<<Loss>>[0.60015] Found at [  4,  4]\n","[  5, 30] TokenID[[11858, 13577, 9843, 207, 18086, 16377]] TokensDesc[telegram reprint acquitted the directorship congesti] Loss[0.50584] Acc[0.87199] Prec[1.0] Recall[0.87199] F1[0.93151] => Worst<<Loss>>[0.60015] Found at [  4,  4]\n","[  5, 31] TokenID[[11858, 13577, 9843, 207, 18086, 4030]] TokensDesc[telegram reprint acquitted the directorship consistently] Loss[0.49834] Acc[0.87389] Prec[1.0] Recall[0.87389] F1[0.93261] => Worst<<Loss>>[0.60015] Found at [  4,  4]\n","[  5, 32] TokenID[[11858, 13577, 9843, 207, 18086, 2867]] TokensDesc[telegram reprint acquitted the directorship improve] Loss[0.41954] Acc[0.89927] Prec[1.0] Recall[0.89927] F1[0.94692] => Worst<<Loss>>[0.60015] Found at [  4,  4]\n","[  5, 33] TokenID[[11858, 13577, 9843, 207, 18086, 4980]] TokensDesc[telegram reprint acquitted the directorship clarity] Loss[0.48706] Acc[0.87761] Prec[1.0] Recall[0.87761] F1[0.93474] => Worst<<Loss>>[0.60015] Found at [  4,  4]\n","[  5, 34] TokenID[[11858, 13577, 9843, 207, 18086, 2554]] TokensDesc[telegram reprint acquitted the directorship wide] Loss[0.49431] Acc[0.86636] Prec[1.0] Recall[0.86636] F1[0.92833] => Worst<<Loss>>[0.60015] Found at [  4,  4]\n","[  5, 35] TokenID[[11858, 13577, 9843, 207, 18086, 358]] TokensDesc[telegram reprint acquitted the directorship means] Loss[0.61764] Acc[0.82049] Prec[1.0] Recall[0.82049] F1[0.90134] => Worst<<Loss>>[0.61764] Found at [  5, 35]\n","[  5, 36] TokenID[[11858, 13577, 9843, 207, 18086, 1706]] TokensDesc[telegram reprint acquitted the directorship greate] Loss[0.50957] Acc[0.87129] Prec[1.0] Recall[0.87129] F1[0.9311] => Worst<<Loss>>[0.61764] Found at [  5, 35]\n","[  5, 37] TokenID[[11858, 13577, 9843, 207, 18086, 936]] TokensDesc[telegram reprint acquitted the directorship now] Loss[0.53405] Acc[0.85275] Prec[1.0] Recall[0.85275] F1[0.92046] => Worst<<Loss>>[0.61764] Found at [  5, 35]\n","[  5, 38] TokenID[[11858, 13577, 9843, 207, 18086, 3466]] TokensDesc[telegram reprint acquitted the directorship sense] Loss[0.50227] Acc[0.86728] Prec[1.0] Recall[0.86728] F1[0.92886] => Worst<<Loss>>[0.61764] Found at [  5, 35]\n","[  5, 39] TokenID[[11858, 13577, 9843, 207, 18086, 3102]] TokensDesc[telegram reprint acquitted the directorship replace] Loss[0.56721] Acc[0.84674] Prec[1.0] Recall[0.84674] F1[0.91692] => Worst<<Loss>>[0.61764] Found at [  5, 35]\n","[  5, 40] TokenID[[11858, 13577, 9843, 207, 18086, 1396]] TokensDesc[telegram reprint acquitted the directorship omitted] Loss[0.53393] Acc[0.85376] Prec[1.0] Recall[0.85376] F1[0.92105] => Worst<<Loss>>[0.61764] Found at [  5, 35]\n","[  5, 41] TokenID[[11858, 13577, 9843, 207, 18086, 21745]] TokensDesc[telegram reprint acquitted the directorship scrubbe] Loss[0.51475] Acc[0.86688] Prec[1.0] Recall[0.86688] F1[0.92857] => Worst<<Loss>>[0.61764] Found at [  5, 35]\n","[  5, 42] TokenID[[11858, 13577, 9843, 207, 18086, 2268]] TokensDesc[telegram reprint acquitted the directorship developed] Loss[0.50206] Acc[0.8783] Prec[1.0] Recall[0.8783] F1[0.93515] => Worst<<Loss>>[0.61764] Found at [  5, 35]\n","[  5, 43] TokenID[[11858, 13577, 9843, 207, 18086, 1625]] TokensDesc[telegram reprint acquitted the directorship rather] Loss[0.58791] Acc[0.84138] Prec[1.0] Recall[0.84138] F1[0.91374] => Worst<<Loss>>[0.61764] Found at [  5, 35]\n","[  5, 44] TokenID[[11858, 13577, 9843, 207, 18086, 2005]] TokensDesc[telegram reprint acquitted the directorship levels] Loss[0.4471] Acc[0.8837] Prec[1.0] Recall[0.8837] F1[0.93819] => Worst<<Loss>>[0.61764] Found at [  5, 35]\n","[  5, 45] TokenID[[11858, 13577, 9843, 207, 18086, 7843]] TokensDesc[telegram reprint acquitted the directorship convergen] Loss[0.50092] Acc[0.87026] Prec[1.0] Recall[0.87026] F1[0.93051] => Worst<<Loss>>[0.61764] Found at [  5, 35]\n","[  5, 46] TokenID[[11858, 13577, 9843, 207, 18086, 4081]] TokensDesc[telegram reprint acquitted the directorship multiplie] Loss[0.50731] Acc[0.8729] Prec[1.0] Recall[0.8729] F1[0.93201] => Worst<<Loss>>[0.61764] Found at [  5, 35]\n","[  5, 47] TokenID[[11858, 13577, 9843, 207, 18086, 17746]] TokensDesc[telegram reprint acquitted the directorship streamline] Loss[0.48598] Acc[0.87507] Prec[1.0] Recall[0.87507] F1[0.93329] => Worst<<Loss>>[0.61764] Found at [  5, 35]\n","[  5, 48] TokenID[[11858, 13577, 9843, 207, 18086, 3986]] TokensDesc[telegram reprint acquitted the directorship likelihood] Loss[0.46766] Acc[0.87447] Prec[1.0] Recall[0.87447] F1[0.93301] => Worst<<Loss>>[0.61764] Found at [  5, 35]\n","[  5, 49] TokenID[[11858, 13577, 9843, 207, 18086, 215]] TokensDesc[telegram reprint acquitted the directorship or] Loss[0.50414] Acc[0.87038] Prec[1.0] Recall[0.87038] F1[0.93063] => Worst<<Loss>>[0.61764] Found at [  5, 35]\n","[  5, 50] TokenID[[11858, 13577, 9843, 207, 18086, 14805]] TokensDesc[telegram reprint acquitted the directorship shrink] Loss[0.51455] Acc[0.86399] Prec[1.0] Recall[0.86399] F1[0.92693] => Worst<<Loss>>[0.61764] Found at [  5, 35]\n","[  5, 51] TokenID[[11858, 13577, 9843, 207, 18086, 15801]] TokensDesc[telegram reprint acquitted the directorship click] Loss[0.534] Acc[0.86166] Prec[1.0] Recall[0.86166] F1[0.92564] => Worst<<Loss>>[0.61764] Found at [  5, 35]\n","[  5, 52] TokenID[[11858, 13577, 9843, 207, 18086, 2818]] TokensDesc[telegram reprint acquitted the directorship changed] Loss[0.43257] Acc[0.88936] Prec[1.0] Recall[0.88936] F1[0.94137] => Worst<<Loss>>[0.61764] Found at [  5, 35]\n","[  5, 53] TokenID[[11858, 13577, 9843, 207, 18086, 2878]] TokensDesc[telegram reprint acquitted the directorship compared] Loss[0.46563] Acc[0.8882] Prec[1.0] Recall[0.8882] F1[0.94071] => Worst<<Loss>>[0.61764] Found at [  5, 35]\n","[  5, 54] TokenID[[11858, 13577, 9843, 207, 18086, 2389]] TokensDesc[telegram reprint acquitted the directorship reflect] Loss[0.51002] Acc[0.87294] Prec[1.0] Recall[0.87294] F1[0.93206] => Worst<<Loss>>[0.61764] Found at [  5, 35]\n","[  5, 55] TokenID[[11858, 13577, 9843, 207, 18086, 2585]] TokensDesc[telegram reprint acquitted the directorship develop] Loss[0.46336] Acc[0.89298] Prec[1.0] Recall[0.89298] F1[0.94339] => Worst<<Loss>>[0.61764] Found at [  5, 35]\n","[  5, 56] TokenID[[11858, 13577, 9843, 207, 18086, 5692]] TokensDesc[telegram reprint acquitted the directorship enhance] Loss[0.44514] Acc[0.89231] Prec[1.0] Recall[0.89231] F1[0.94302] => Worst<<Loss>>[0.61764] Found at [  5, 35]\n","[  5, 57] TokenID[[11858, 13577, 9843, 207, 18086, 9235]] TokensDesc[telegram reprint acquitted the directorship anywhere] Loss[0.51649] Acc[0.87177] Prec[1.0] Recall[0.87177] F1[0.93139] => Worst<<Loss>>[0.61764] Found at [  5, 35]\n","[  5, 58] TokenID[[11858, 13577, 9843, 207, 18086, 23196]] TokensDesc[telegram reprint acquitted the directorship plausibl] Loss[0.46142] Acc[0.88577] Prec[1.0] Recall[0.88577] F1[0.93933] => Worst<<Loss>>[0.61764] Found at [  5, 35]\n","[  5, 59] TokenID[[11858, 13577, 9843, 207, 18086, 1459]] TokensDesc[telegram reprint acquitted the directorship how] Loss[0.67298] Acc[0.80971] Prec[1.0] Recall[0.80971] F1[0.89476] => Worst<<Loss>>[0.67298] Found at [  5, 59]\n","[  5, 60] TokenID[[11858, 13577, 9843, 207, 18086, 8687]] TokensDesc[telegram reprint acquitted the directorship synerg] Loss[0.51847] Acc[0.87301] Prec[1.0] Recall[0.87301] F1[0.93207] => Worst<<Loss>>[0.67298] Found at [  5, 59]\n","[  5, 61] TokenID[[11858, 13577, 9843, 207, 18086, 12209]] TokensDesc[telegram reprint acquitted the directorship disparate] Loss[0.47515] Acc[0.88344] Prec[1.0] Recall[0.88344] F1[0.93803] => Worst<<Loss>>[0.67298] Found at [  5, 59]\n","[  5, 62] TokenID[[11858, 13577, 9843, 207, 18086, 1472]] TokensDesc[telegram reprint acquitted the directorship markets] Loss[0.45967] Acc[0.88684] Prec[1.0] Recall[0.88684] F1[0.93996] => Worst<<Loss>>[0.67298] Found at [  5, 59]\n","[  5, 63] TokenID[[11858, 13577, 9843, 207, 18086, 1845]] TokensDesc[telegram reprint acquitted the directorship combined] Loss[0.49819] Acc[0.86691] Prec[1.0] Recall[0.86691] F1[0.92862] => Worst<<Loss>>[0.67298] Found at [  5, 59]\n","[  5, 64] TokenID[[11858, 13577, 9843, 207, 18086, 4920]] TokensDesc[telegram reprint acquitted the directorship plural] Loss[0.48381] Acc[0.87268] Prec[1.0] Recall[0.87268] F1[0.93192] => Worst<<Loss>>[0.67298] Found at [  5, 59]\n","[  5, 65] TokenID[[11858, 13577, 9843, 207, 18086, 14044]] TokensDesc[telegram reprint acquitted the directorship basel] Loss[0.49573] Acc[0.87697] Prec[1.0] Recall[0.87697] F1[0.93433] => Worst<<Loss>>[0.67298] Found at [  5, 59]\n","[  5, 66] TokenID[[11858, 13577, 9843, 207, 18086, 901]] TokensDesc[telegram reprint acquitted the directorship light] Loss[0.51266] Acc[0.86397] Prec[1.0] Recall[0.86397] F1[0.92697] => Worst<<Loss>>[0.67298] Found at [  5, 59]\n","[  5, 67] TokenID[[11858, 13577, 9843, 207, 18086, 512]] TokensDesc[telegram reprint acquitted the directorship defined] Loss[0.52011] Acc[0.86582] Prec[1.0] Recall[0.86582] F1[0.92803] => Worst<<Loss>>[0.67298] Found at [  5, 59]\n","[  5, 68] TokenID[[11858, 13577, 9843, 207, 18086, 8350]] TokensDesc[telegram reprint acquitted the directorship closer] Loss[0.50697] Acc[0.8721] Prec[1.0] Recall[0.8721] F1[0.93158] => Worst<<Loss>>[0.67298] Found at [  5, 59]\n","[  5, 69] TokenID[[11858, 13577, 9843, 207, 18086, 4524]] TokensDesc[telegram reprint acquitted the directorship custom] Loss[0.52219] Acc[0.87234] Prec[1.0] Recall[0.87234] F1[0.9317] => Worst<<Loss>>[0.67298] Found at [  5, 59]\n","[  5, 70] TokenID[[11858, 13577, 9843, 207, 18086, 10082]] TokensDesc[telegram reprint acquitted the directorship disparit] Loss[0.53228] Acc[0.85938] Prec[1.0] Recall[0.85938] F1[0.92421] => Worst<<Loss>>[0.67298] Found at [  5, 59]\n","[  5, 71] TokenID[[11858, 13577, 9843, 207, 18086, 487]] TokensDesc[telegram reprint acquitted the directorship based] Loss[0.50498] Acc[0.86766] Prec[1.0] Recall[0.86766] F1[0.92907] => Worst<<Loss>>[0.67298] Found at [  5, 59]\n","[  5, 72] TokenID[[11858, 13577, 9843, 207, 18086, 808]] TokensDesc[telegram reprint acquitted the directorship access] Loss[0.45039] Acc[0.89184] Prec[1.0] Recall[0.89184] F1[0.94277] => Worst<<Loss>>[0.67298] Found at [  5, 59]\n","[  5, 73] TokenID[[11858, 13577, 9843, 207, 18086, 2205]] TokensDesc[telegram reprint acquitted the directorship regions] Loss[0.47659] Acc[0.87698] Prec[1.0] Recall[0.87698] F1[0.93438] => Worst<<Loss>>[0.67298] Found at [  5, 59]\n","[  5, 74] TokenID[[11858, 13577, 9843, 207, 18086, 2489]] TokensDesc[telegram reprint acquitted the directorship commercially] Loss[0.49287] Acc[0.87995] Prec[1.0] Recall[0.87995] F1[0.93605] => Worst<<Loss>>[0.67298] Found at [  5, 59]\n","[  5, 75] TokenID[[11858, 13577, 9843, 207, 18086, 25262]] TokensDesc[telegram reprint acquitted the directorship tense] Loss[0.5104] Acc[0.87169] Prec[1.0] Recall[0.87169] F1[0.93135] => Worst<<Loss>>[0.67298] Found at [  5, 59]\n","[  5, 76] TokenID[[11858, 13577, 9843, 207, 18086, 2986]] TokensDesc[telegram reprint acquitted the directorship mutually] Loss[0.47952] Acc[0.88283] Prec[1.0] Recall[0.88283] F1[0.93767] => Worst<<Loss>>[0.67298] Found at [  5, 59]\n","[  5, 77] TokenID[[11858, 13577, 9843, 207, 18086, 18765]] TokensDesc[telegram reprint acquitted the directorship roug] Loss[0.55125] Acc[0.85879] Prec[1.0] Recall[0.85879] F1[0.92393] => Worst<<Loss>>[0.67298] Found at [  5, 59]\n","[  5, 78] TokenID[[11858, 13577, 9843, 207, 18086, 6662]] TokensDesc[telegram reprint acquitted the directorship numerator] Loss[0.53716] Acc[0.85731] Prec[1.0] Recall[0.85731] F1[0.9231] => Worst<<Loss>>[0.67298] Found at [  5, 59]\n","[  5, 79] TokenID[[11858, 13577, 9843, 207, 18086, 4741]] TokensDesc[telegram reprint acquitted the directorship notably] Loss[0.53003] Acc[0.85308] Prec[1.0] Recall[0.85308] F1[0.92066] => Worst<<Loss>>[0.67298] Found at [  5, 59]\n","[  5, 80] TokenID[[11858, 13577, 9843, 207, 18086, 1309]] TokensDesc[telegram reprint acquitted the directorship generally] Loss[0.49127] Acc[0.87364] Prec[1.0] Recall[0.87364] F1[0.93246] => Worst<<Loss>>[0.67298] Found at [  5, 59]\n","[  5, 81] TokenID[[11858, 13577, 9843, 207, 18086, 761]] TokensDesc[telegram reprint acquitted the directorship net] Loss[0.5445] Acc[0.85543] Prec[1.0] Recall[0.85543] F1[0.92201] => Worst<<Loss>>[0.67298] Found at [  5, 59]\n","[  5, 82] TokenID[[11858, 13577, 9843, 207, 18086, 1305]] TokensDesc[telegram reprint acquitted the directorship substantially] Loss[0.51644] Acc[0.86678] Prec[1.0] Recall[0.86678] F1[0.92854] => Worst<<Loss>>[0.67298] Found at [  5, 59]\n","[  5, 83] TokenID[[11858, 13577, 9843, 207, 18086, 2641]] TokensDesc[telegram reprint acquitted the directorship simply] Loss[0.58546] Acc[0.83199] Prec[1.0] Recall[0.83199] F1[0.90824] => Worst<<Loss>>[0.67298] Found at [  5, 59]\n","[  5, 84] TokenID[[11858, 13577, 9843, 207, 18086, 5424]] TokensDesc[telegram reprint acquitted the directorship usage] Loss[0.46485] Acc[0.89277] Prec[1.0] Recall[0.89277] F1[0.94327] => Worst<<Loss>>[0.67298] Found at [  5, 59]\n","[  5, 85] TokenID[[11858, 13577, 9843, 207, 18086, 3126]] TokensDesc[telegram reprint acquitted the directorship conflicts] Loss[0.48573] Acc[0.87392] Prec[1.0] Recall[0.87392] F1[0.93268] => Worst<<Loss>>[0.67298] Found at [  5, 59]\n","[  5, 86] TokenID[[11858, 13577, 9843, 207, 18086, 12905]] TokensDesc[telegram reprint acquitted the directorship worse] Loss[0.49892] Acc[0.87047] Prec[1.0] Recall[0.87047] F1[0.93064] => Worst<<Loss>>[0.67298] Found at [  5, 59]\n","[  5, 87] TokenID[[11858, 13577, 9843, 207, 18086, 3708]] TokensDesc[telegram reprint acquitted the directorship round] Loss[0.54085] Acc[0.84614] Prec[1.0] Recall[0.84614] F1[0.91662] => Worst<<Loss>>[0.67298] Found at [  5, 59]\n","[  5, 88] TokenID[[11858, 13577, 9843, 207, 18086, 268]] TokensDesc[telegram reprint acquitted the directorship than] Loss[0.56546] Acc[0.84441] Prec[1.0] Recall[0.84441] F1[0.91556] => Worst<<Loss>>[0.67298] Found at [  5, 59]\n","[  5, 89] TokenID[[11858, 13577, 9843, 207, 18086, 9920]] TokensDesc[telegram reprint acquitted the directorship ago] Loss[0.55215] Acc[0.83758] Prec[1.0] Recall[0.83758] F1[0.91156] => Worst<<Loss>>[0.67298] Found at [  5, 59]\n","[  5, 90] TokenID[[11858, 13577, 9843, 207, 18086, 2922]] TokensDesc[telegram reprint acquitted the directorship shared] Loss[0.50461] Acc[0.88074] Prec[1.0] Recall[0.88074] F1[0.93647] => Worst<<Loss>>[0.67298] Found at [  5, 59]\n","[  5, 91] TokenID[[11858, 13577, 9843, 207, 18086, 4805]] TokensDesc[telegram reprint acquitted the directorship quick] Loss[0.53351] Acc[0.84739] Prec[1.0] Recall[0.84739] F1[0.91737] => Worst<<Loss>>[0.67298] Found at [  5, 59]\n","[  5, 92] TokenID[[11858, 13577, 9843, 207, 18086, 19149]] TokensDesc[telegram reprint acquitted the directorship preexist] Loss[0.47976] Acc[0.87712] Prec[1.0] Recall[0.87712] F1[0.9345] => Worst<<Loss>>[0.67298] Found at [  5, 59]\n","[  5, 93] TokenID[[11858, 13577, 9843, 207, 18086, 247]] TokensDesc[telegram reprint acquitted the directorship have] Loss[0.5744] Acc[0.84332] Prec[1.0] Recall[0.84332] F1[0.91488] => Worst<<Loss>>[0.67298] Found at [  5, 59]\n","[  5, 94] TokenID[[11858, 13577, 9843, 207, 18086, 2480]] TokensDesc[telegram reprint acquitted the directorship survive] Loss[0.53427] Acc[0.85802] Prec[1.0] Recall[0.85802] F1[0.9235] => Worst<<Loss>>[0.67298] Found at [  5, 59]\n","[  5, 95] TokenID[[11858, 13577, 9843, 207, 18086, 2755]] TokensDesc[telegram reprint acquitted the directorship integrate] Loss[0.52593] Acc[0.86146] Prec[1.0] Recall[0.86146] F1[0.92549] => Worst<<Loss>>[0.67298] Found at [  5, 59]\n","[  5, 96] TokenID[[11858, 13577, 9843, 207, 18086, 4981]] TokensDesc[telegram reprint acquitted the directorship bears] Loss[0.55963] Acc[0.84665] Prec[1.0] Recall[0.84665] F1[0.91685] => Worst<<Loss>>[0.67298] Found at [  5, 59]\n","[  5, 97] TokenID[[11858, 13577, 9843, 207, 18086, 5376]] TokensDesc[telegram reprint acquitted the directorship trend] Loss[0.47485] Acc[0.87921] Prec[1.0] Recall[0.87921] F1[0.93565] => Worst<<Loss>>[0.67298] Found at [  5, 59]\n","[  5, 98] TokenID[[11858, 13577, 9843, 207, 18086, 6242]] TokensDesc[telegram reprint acquitted the directorship commonly] Loss[0.50725] Acc[0.86563] Prec[1.0] Recall[0.86563] F1[0.92792] => Worst<<Loss>>[0.67298] Found at [  5, 59]\n","[  5, 99] TokenID[[11858, 13577, 9843, 207, 18086, 2010]] TokensDesc[telegram reprint acquitted the directorship competitive] Loss[0.50264] Acc[0.87263] Prec[1.0] Recall[0.87263] F1[0.93186] => Worst<<Loss>>[0.67298] Found at [  5, 59]\n","Worst loss 0.6729815765734641 with candidates [11858, 13577, 9843, 207, 18086, 1459] in the 5-iteration with tokens [telegram reprint acquitted the directorship how]\n"]}]},{"cell_type":"code","source":["#tokenizer.decode(candidates_selected) # B unless the everyone\n","tokenizer.decode(candidates_selected) # E signature scarcely contrary -> [1297, 17560, 207] or [1297, 17560, 1004]\n","\n","# New tokens: B Worst loss 0.6033629960110111 with candidates [11858, 23347, 700] in the 2-iteration with tokens [telegram bnym him]\n","# New tokens: E Worst loss 0.41469741131990184 with candidates [1297, 14808, 16827] in the 2-iteration with tokens [signature understandable overcrowd]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"9UkDXrIG0IiM","executionInfo":{"status":"ok","timestamp":1659618127780,"user_tz":-120,"elapsed":2297,"user":{"displayName":"Isaac Olguín","userId":"09697151780329788960"}},"outputId":"e8c7464c-9463-4f4c-8d0d-266e30955f79"},"execution_count":29,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'telegram bnym him'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":29}]},{"cell_type":"markdown","source":["##### Generate sentences with tokens"],"metadata":{"id":"jTdsbDJapDkf"}},{"cell_type":"code","source":["len(target_unfair_sentences) #1032 bef; 991 now new distr 96(95-05)-4"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"w2-_S4Yr1q8H","executionInfo":{"status":"ok","timestamp":1659606320467,"user_tz":-120,"elapsed":371,"user":{"displayName":"Isaac Olguín","userId":"09697151780329788960"}},"outputId":"8138a4cf-6057-4fd6-dda8-7ef1587fa05c"},"execution_count":11,"outputs":[{"output_type":"execute_result","data":{"text/plain":["991"]},"metadata":{},"execution_count":11}]},{"cell_type":"code","source":["# Store new sentences with the tokens at the beginning\n","position=\"B\"\n","list_tokens_deal_worst_loss = [\n","    tokenizer.decode([621, 207, 3523]), #unless the everyone\n","    tokenizer.decode([621, 207, 207]) #unless the the\n","]\n","\n","dict_distribution = {\n","    0: 0,\n","    1: 0\n","}\n","\n","timestamp = datetime.datetime.now().strftime(\"%y%m%d_%H_%M_%S_%p\")\n","f = open(f\"ToS/DataAugmentation/Pos{position}_NumTokens{NUM_TOKENS}_{timestamp}.txt\", \"w\")\n","\n","for unf_sent in target_unfair_sentences:\n","    index = 1 if random.uniform(0,1) > 0.5 else 0\n","    f.write(f'{list_tokens_deal_worst_loss[index]} {unf_sent}\\n')\n","    dict_distribution[index] += 1\n","f.close()\n","\n","print(dict_distribution) # {0: 508, 1: 524} => {0: 471, 1: 520} (96(95-05)-04))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eADqdnKAh3e6","executionInfo":{"status":"ok","timestamp":1659606995419,"user_tz":-120,"elapsed":336,"user":{"displayName":"Isaac Olguín","userId":"09697151780329788960"}},"outputId":"fc03d79a-dbdb-40c4-85e9-145e97654627"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["{0: 471, 1: 520}\n"]}]},{"cell_type":"code","source":["# Store new sentences with the tokens at the beginning\n","position=\"E\"\n","list_tokens_deal_worst_loss = [\n","    tokenizer.decode([1297, 17560, 1004]), #unless the everyone\n","    tokenizer.decode([1297, 17560, 431]) #unless the the\n","]\n","\n","dict_distribution = {\n","    0: 0,\n","    1: 0\n","}\n","\n","timestamp = datetime.datetime.now().strftime(\"%y%m%d_%H_%M_%S_%p\")\n","f = open(f\"ToS/DataAugmentation/Pos{position}_NumTokens{NUM_TOKENS}_{timestamp}.txt\", \"w\")\n","\n","for unf_sent in target_unfair_sentences:\n","    index = 1 if random.uniform(0,1) > 0.5 else 0\n","\n","    sent_tokenized = tokenizer.encode(unf_sent)\n","\n","    if len(sent_tokenized) > 507:\n","      str_aux = (tokenizer.decode(sent_tokenized[0:507])).replace(\"\\n\", \"\")\n","      f.write(f'{str_aux} {list_tokens_deal_worst_loss[index]}\\n')\n","    else:\n","      str_aux = unf_sent.replace(\"\\n\", \"\")\n","      f.write(f'{str_aux} {list_tokens_deal_worst_loss[index]}\\n')\n","    dict_distribution[index] += 1\n","f.close()\n","\n","print(dict_distribution) # {0: 513, 1: 519} => {0: 492, 1: 499} (96(95-05)-04)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2ESyo77Jl1s6","executionInfo":{"status":"ok","timestamp":1659607000293,"user_tz":-120,"elapsed":464,"user":{"displayName":"Isaac Olguín","userId":"09697151780329788960"}},"outputId":"9fcf3b89-d08c-44cd-9257-9b7c2e3e2ded"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["{0: 492, 1: 499}\n"]}]},{"cell_type":"markdown","source":["## Training of a new model"],"metadata":{"id":"2mthIUQypWg0"}},{"cell_type":"markdown","source":["The following code was the version for reading the sentences and labels that include the tokens at the beginning or the end.\n","\n","This is changed later for the reading of the datasets (test; and train_val).\n","\n","\n","\n","```\n","# Read unfair sentences with tokens\n","\n","augmented_sentences = get_sentences(\"ToS/DataAugmentation/\")\n","augmented_labels = [1]*len(augmented_sentences)\n","\n","print(f'{len(augmented_labels)} ?= {len(augmented_sentences)}')\n","\n","# Output:\n","# 2064 ?= 2064\n","\n","##########################################\n","\n","all_sentences_and_augmented = all_sentences + augmented_sentences\n","print(f'Length_sentences {len(all_sentences)} + Length_augmented {len(augmented_sentences)} = Len both sets {len(all_sentences_and_augmented)}')\n","\n","# Output:\n","# Length_sentences 9414 + Length_augmented 2064 = Len both sets 11478\n","\n","####\n","\n","all_labels_and_augmented = all_labels + augmented_labels\n","print(f'Length_labels {len(all_sentences)} + Length_augmented {len(augmented_sentences)} = Len both sets {len(all_labels_and_augmented)}')\n","\n","# Output:\n","# Length_labels 9414 + Length_augmented 2064 = Len both sets 11478\n","\n","\"\"\"\n","1982 ?= 1982\n","Length_sentences 9037 + Length_augmented 1982 = Len both sets 11019\n","Length_labels 9037 + Length_augmented 1982 = Len both sets 11019\n","\"\"\"\n","\n","```\n","\n"],"metadata":{"id":"OxZHvZYX9lsG"}},{"cell_type":"markdown","source":["### Data split"],"metadata":{"id":"m9POhis6qbP_"}},{"cell_type":"code","source":["all_sentences_and_augmented = get_sentences(\"ToS/TrainValSetWithAugmentedData/Sentences/\")\n","all_labels_and_augmented = get_labels(\"ToS/TrainValSetWithAugmentedData/Labels/\")"],"metadata":{"id":"Hln8HJa0-zwX","executionInfo":{"status":"ok","timestamp":1659609678919,"user_tz":-120,"elapsed":248,"user":{"displayName":"Isaac Olguín","userId":"09697151780329788960"}}},"execution_count":52,"outputs":[]},{"cell_type":"code","source":["# Tokenize all of the sentences and map the tokens to thier word IDs.\n","input_ids = []\n","attention_masks = []\n","\n","# For every sentence...\n","for sent in all_sentences_and_augmented:\n","    # `encode_plus` will:\n","    #   (1) Tokenize the sentence.\n","    #   (2) Prepend the `[CLS]` token to the start.\n","    #   (3) Append the `[SEP]` token to the end.\n","    #   (4) Map tokens to their IDs.\n","    #   (5) Pad or truncate the sentence to `max_length`\n","    #   (6) Create attention masks for [PAD] tokens.\n","    encoded_dict = tokenizer.encode_plus(\n","                        sent,                      # Sentence to encode.\n","                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n","                        max_length = 512,          # Pad & truncate all sentences.\n","                        pad_to_max_length = True, #is deprecated\n","                        return_attention_mask = True,   # Construct attn. masks.\n","                        return_tensors = 'pt',     # Return pytorch tensors.\n","                   )\n","    \n","    # Add the encoded sentence to the list.    \n","    input_ids.append(encoded_dict['input_ids'])\n","    \n","    # And its attention mask (simply differentiates padding from non-padding).\n","    attention_masks.append(encoded_dict['attention_mask'])\n","\n","# Convert the lists into tensors.\n","input_ids = torch.cat(input_ids, dim=0)\n","attention_masks = torch.cat(attention_masks, dim=0)\n","labels = torch.tensor(all_labels_and_augmented)\n","\n","# Print sentence 0, now as a list of IDs.\n","print('Original: ', all_sentences_and_augmented[0])\n","print('Token IDs:', input_ids[0])\n","\n","# Combine the training inputs into a TensorDataset.\n","dataset = TensorDataset(input_ids, attention_masks, labels)\n","\n","# Create a 90-10 train-validation split.\n","train_idx, valid_idx = train_test_split(np.arange(len(labels)), test_size=0.05, shuffle=True, stratify=labels)\n","\n","train_sampler = torch.utils.data.SubsetRandomSampler(train_idx)\n","valid_sampler = torch.utils.data.SubsetRandomSampler(valid_idx)\n","\n","train_dataloader = torch.utils.data.DataLoader(dataset, batch_size=BATCH_SIZE, sampler=train_sampler)\n","validation_dataloader = torch.utils.data.DataLoader(dataset, batch_size=BATCH_SIZE, sampler=valid_sampler)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VDtPTWVzp_mc","executionInfo":{"status":"ok","timestamp":1659609697052,"user_tz":-120,"elapsed":5561,"user":{"displayName":"Isaac Olguín","userId":"09697151780329788960"}},"outputId":"c760a672-cf53-4fe2-b028-28148798f6ca"},"execution_count":53,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2329: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"]},{"output_type":"stream","name":"stdout","text":["Original:  you agree to our data practices, including the collection, use, processing, and sharing of your information as described in our privacy policy, as well as the transfer and processing of your information to the united states and other countries globally where we have or use facilities, service providers, or partners, regardless of where you use our services.\n","\n","Token IDs: tensor([ 101,  799,  753,  211, 1590,  586, 1748,  115,  283,  207, 1544,  115,\n","         355,  115, 1384,  115,  212, 2766,  235,  210, 1216,  286,  221,  735,\n","         213, 1590, 4407,  663,  115,  221,  705,  221,  207,  439,  212, 1384,\n","         210, 1216,  286,  211,  207,  354,  265,  212,  231,  779, 1506,  301,\n","         343,  532,  247,  215,  355, 1551,  115,  446, 3522,  115,  215, 1523,\n","         115, 2248,  210,  343,  799,  355, 1590,  410,  117,  102,    0,    0,\n","           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0,    0,    0,    0,    0,    0])\n"]}]},{"cell_type":"markdown","source":["The following code was developed to create the persistent files with the augmented data.\n","\n","\n","\n","```\n","\"\"\"\n","print(type(train_idx))\n","print(len(train_idx))\n","print(train_idx[0:10])\n","print(type(train_sampler))\n","\n","for index in train_idx[0:10]:\n","    print(f'{labels[index]} <<{tokenizer.decode(input_ids[index], skip_special_tokens=True)}>>')\n","\"\"\"\n","\n","### Creation of the sets training, val and set. These sets are being stored to ensure persistency\n","\n","# Let's first get the train_val and test set\n","\n","# Create a 95-05 train-validation split.\n","train_val_ids, test_ids = train_test_split(np.arange(len(labels)), test_size=0.05, shuffle=True, stratify=labels)\n","\n","timestamp = datetime.datetime.now().strftime(\"%y%m%d_%H_%M_%S_%p\")\n","file_sentences_test_set = open(f\"ToS/TestSetWithAugmentedData/Sentences/Sentences_{timestamp}.txt\", \"w\")\n","file_labels_test_set = open(f\"ToS/TestSetWithAugmentedData/Labels/Labels_{timestamp}.txt\", \"w\")\n","for index in test_ids:\n","    sentence = (tokenizer.decode(input_ids[index], skip_special_tokens=True)).replace(\"\\n\", \"\")\n","    file_sentences_test_set.write(f'{sentence}\\n')\n","    sentence = (f\"{labels[index]}\").replace(\"\\n\", \"\")\n","    file_labels_test_set.write(f'{sentence}\\n')\n","file_sentences_test_set.close()\n","file_labels_test_set.close()\n","\n","file_sentences_train_val_set = open(f\"ToS/TrainValSetWithAugmentedData/Sentences/Sentences_{timestamp}.txt\", \"w\")\n","file_labels_train_val_set = open(f\"ToS/TrainValSetWithAugmentedData/Labels/Labels_{timestamp}.txt\", \"w\")\n","for index in train_val_ids:\n","    sentence = (tokenizer.decode(input_ids[index], skip_special_tokens=True)).replace(\"\\n\", \"\")\n","    file_sentences_train_val_set.write(f'{sentence}\\n')\n","    sentence = (f\"{labels[index]}\").replace(\"\\n\", \"\")\n","    file_labels_train_val_set.write(f'{sentence}\\n')\n","file_sentences_train_val_set.close()\n","file_labels_train_val_set.close()\n","```\n","\n"],"metadata":{"id":"UtenL4ku9PRn"}},{"cell_type":"markdown","source":["The following code was used to split the datasets into training-val and test sets without data augmentation.\n","\n","```\n","all_sentences_and_augmented = all_sentences\n","all_labels_and_augmented = all_labels\n","\n","# Tokenize all of the sentences and map the tokens to thier word IDs.\n","input_ids = []\n","attention_masks = []\n","\n","# For every sentence...\n","for sent in all_sentences_and_augmented:\n","    # `encode_plus` will:\n","    #   (1) Tokenize the sentence.\n","    #   (2) Prepend the `[CLS]` token to the start.\n","    #   (3) Append the `[SEP]` token to the end.\n","    #   (4) Map tokens to their IDs.\n","    #   (5) Pad or truncate the sentence to `max_length`\n","    #   (6) Create attention masks for [PAD] tokens.\n","    encoded_dict = tokenizer.encode_plus(\n","                        sent,                      # Sentence to encode.\n","                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n","                        max_length = 512,          # Pad & truncate all sentences.\n","                        pad_to_max_length = True, #is deprecated\n","                        return_attention_mask = True,   # Construct attn. masks.\n","                        return_tensors = 'pt',     # Return pytorch tensors.\n","                   )\n","    \n","    # Add the encoded sentence to the list.    \n","    input_ids.append(encoded_dict['input_ids'])\n","    \n","    # And its attention mask (simply differentiates padding from non-padding).\n","    attention_masks.append(encoded_dict['attention_mask'])\n","\n","# Convert the lists into tensors.\n","input_ids = torch.cat(input_ids, dim=0)\n","attention_masks = torch.cat(attention_masks, dim=0)\n","labels = torch.tensor(all_labels_and_augmented)\n","\n","# Print sentence 0, now as a list of IDs.\n","print('Original: ', all_sentences_and_augmented[0])\n","print('Token IDs:', input_ids[0])\n","\n","# Combine the training inputs into a TensorDataset.\n","dataset = TensorDataset(input_ids, attention_masks, labels)\n","\n","# Create a 90-10 train-validation split.\n","train_idx, valid_idx = train_test_split(np.arange(len(labels)), test_size=0.05, shuffle=True, stratify=labels)\n","\n","train_sampler = torch.utils.data.SubsetRandomSampler(train_idx)\n","valid_sampler = torch.utils.data.SubsetRandomSampler(valid_idx)\n","\n","train_dataloader = torch.utils.data.DataLoader(dataset, batch_size=BATCH_SIZE, sampler=train_sampler)\n","validation_dataloader = torch.utils.data.DataLoader(dataset, batch_size=BATCH_SIZE, sampler=valid_sampler)\n","\n","### Creation of the sets training, val and set. These sets are being stored to ensure persistency\n","\n","# Let's first get the train_val and test set\n","\n","# Create a 90-10 train-validation split.\n","train_val_ids, test_ids = train_test_split(np.arange(len(labels)), test_size=0.1, shuffle=True, stratify=labels)\n","\n","timestamp = datetime.datetime.now().strftime(\"%y%m%d_%H_%M_%S_%p\")\n","file_sentences_test_set = open(f\"ToS/TestSet/Sentences/Sentences_{timestamp}.txt\", \"w\")\n","file_labels_test_set = open(f\"ToS/TestSet/Labels/Labels_{timestamp}.txt\", \"w\")\n","for index in test_ids:\n","    sentence = (tokenizer.decode(input_ids[index], skip_special_tokens=True)).replace(\"\\n\", \"\")\n","    file_sentences_test_set.write(f'{sentence}\\n')\n","    sentence = (f\"{labels[index]}\").replace(\"\\n\", \"\")\n","    file_labels_test_set.write(f'{sentence}\\n')\n","file_sentences_test_set.close()\n","file_labels_test_set.close()\n","\n","file_sentences_train_val_set = open(f\"ToS/TrainValSet/Sentences/Sentences_{timestamp}.txt\", \"w\")\n","file_labels_train_val_set = open(f\"ToS/TrainValSet/Labels/Labels_{timestamp}.txt\", \"w\")\n","for index in train_val_ids:\n","    sentence = (tokenizer.decode(input_ids[index], skip_special_tokens=True)).replace(\"\\n\", \"\")\n","    file_sentences_train_val_set.write(f'{sentence}\\n')\n","    sentence = (f\"{labels[index]}\").replace(\"\\n\", \"\")\n","    file_labels_train_val_set.write(f'{sentence}\\n')\n","file_sentences_train_val_set.close()\n","file_labels_train_val_set.close()\n","```\n"],"metadata":{"id":"T1DeMBzS22v4"}},{"cell_type":"markdown","source":["### Training classification model"],"metadata":{"id":"E4HVDjX_sSyx"}},{"cell_type":"code","source":["# Load BertForSequenceClassification, the pretrained BERT model with a single \n","# linear classification layer on top. \n","model = BertForSequenceClassification.from_pretrained(\n","    MODEL_NAME, # Use the 12-layer BERT model, with an uncased vocab.\n","    num_labels = NUM_CLASSES, # The number of output labels--2 for binary classification.\n","                    # You can increase this for multi-class tasks.   \n","    output_attentions = False, # Whether the model returns attentions weights.\n","    output_hidden_states = False, # Whether the model returns all hidden-states.\n",")\n","\n","# Tell pytorch to run this model on the GPU.\n","model.cuda()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"F3kCP4P3sPSM","executionInfo":{"status":"ok","timestamp":1659609697575,"user_tz":-120,"elapsed":532,"user":{"displayName":"Isaac Olguín","userId":"09697151780329788960"}},"outputId":"fc4a16a1-d63b-42a4-8dc9-2ae2c2dcece1"},"execution_count":54,"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at nlpaueb/legal-bert-small-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.predictions.bias']\n","- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at nlpaueb/legal-bert-small-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"execute_result","data":{"text/plain":["BertForSequenceClassification(\n","  (bert): BertModel(\n","    (embeddings): BertEmbeddings(\n","      (word_embeddings): Embedding(30522, 512, padding_idx=0)\n","      (position_embeddings): Embedding(512, 512)\n","      (token_type_embeddings): Embedding(2, 512)\n","      (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (encoder): BertEncoder(\n","      (layer): ModuleList(\n","        (0): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=512, out_features=512, bias=True)\n","              (key): Linear(in_features=512, out_features=512, bias=True)\n","              (value): Linear(in_features=512, out_features=512, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=512, out_features=512, bias=True)\n","              (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=512, out_features=2048, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=2048, out_features=512, bias=True)\n","            (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (1): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=512, out_features=512, bias=True)\n","              (key): Linear(in_features=512, out_features=512, bias=True)\n","              (value): Linear(in_features=512, out_features=512, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=512, out_features=512, bias=True)\n","              (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=512, out_features=2048, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=2048, out_features=512, bias=True)\n","            (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (2): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=512, out_features=512, bias=True)\n","              (key): Linear(in_features=512, out_features=512, bias=True)\n","              (value): Linear(in_features=512, out_features=512, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=512, out_features=512, bias=True)\n","              (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=512, out_features=2048, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=2048, out_features=512, bias=True)\n","            (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (3): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=512, out_features=512, bias=True)\n","              (key): Linear(in_features=512, out_features=512, bias=True)\n","              (value): Linear(in_features=512, out_features=512, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=512, out_features=512, bias=True)\n","              (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=512, out_features=2048, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=2048, out_features=512, bias=True)\n","            (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (4): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=512, out_features=512, bias=True)\n","              (key): Linear(in_features=512, out_features=512, bias=True)\n","              (value): Linear(in_features=512, out_features=512, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=512, out_features=512, bias=True)\n","              (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=512, out_features=2048, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=2048, out_features=512, bias=True)\n","            (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (5): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=512, out_features=512, bias=True)\n","              (key): Linear(in_features=512, out_features=512, bias=True)\n","              (value): Linear(in_features=512, out_features=512, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=512, out_features=512, bias=True)\n","              (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=512, out_features=2048, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=2048, out_features=512, bias=True)\n","            (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","    (pooler): BertPooler(\n","      (dense): Linear(in_features=512, out_features=512, bias=True)\n","      (activation): Tanh()\n","    )\n","  )\n","  (dropout): Dropout(p=0.1, inplace=False)\n","  (classifier): Linear(in_features=512, out_features=2, bias=True)\n",")"]},"metadata":{},"execution_count":54}]},{"cell_type":"markdown","source":["### Optimizer & Learning Rate Scheduler"],"metadata":{"id":"Y5KcpRZDsZWY"}},{"cell_type":"code","source":["# Note: AdamW is a class from the huggingface library (as opposed to pytorch) \n","# I believe the 'W' stands for 'Weight Decay fix\"\n","optimizer = AdamW(model.parameters(),\n","                  lr = 2e-5,#2e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n","                  eps = 1e-8, # args.adam_epsilon  - default is 1e-8.\n","                  #weight_decay=0.3\n","                )"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5KmzLrDfsVGS","executionInfo":{"status":"ok","timestamp":1659609697576,"user_tz":-120,"elapsed":28,"user":{"displayName":"Isaac Olguín","userId":"09697151780329788960"}},"outputId":"0d1a987f-3619-47f7-decd-e8baba67f37d"},"execution_count":55,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n"]}]},{"cell_type":"code","source":["# Number of training epochs. The BERT authors recommend between 2 and 4. \n","# We chose to run for 4, but we'll see later that this may be over-fitting the\n","# training data.\n","epochs = EPOCHS\n","\n","# Total number of training steps is [number of batches] x [number of epochs]. \n","# (Note that this is not the same as the number of training samples).\n","total_steps = len(train_dataloader) * epochs\n","\n","# Create the learning rate scheduler.\n","\"\"\"\n","scheduler = get_linear_schedule_with_warmup(optimizer, \n","                                            num_warmup_steps = 0, # Default value in run_glue.py\n","                                            num_training_steps = total_steps)\n","\"\"\"\n","from torch.optim.lr_scheduler import CosineAnnealingLR\n","MIN_LR = 1e-5\n","scheduler = CosineAnnealingLR(optimizer, 600, eta_min = MIN_LR)"],"metadata":{"id":"GdsFUHWPsdOn","executionInfo":{"status":"ok","timestamp":1659609697577,"user_tz":-120,"elapsed":9,"user":{"displayName":"Isaac Olguín","userId":"09697151780329788960"}}},"execution_count":56,"outputs":[]},{"cell_type":"markdown","source":["### Training"],"metadata":{"id":"S6SLYPm-sk46"}},{"cell_type":"code","source":["# Function to calculate the accuracy of our predictions vs labels\n","def flat_accuracy(preds, labels):\n","    \"\"\"\n","    print(f'preds.shape {preds.shape} == {preds}')\n","    print(f'labels.shape {labels.shape} == {labels}')\n","    \"\"\"\n","    pred_flat = np.argmax(preds, axis=1).flatten()\n","    labels_flat = labels.flatten()\n","    \"\"\"\n","    print(f'pred_flat.shape {pred_flat.shape} == {pred_flat}')\n","    print(f'labels_flat.shape {labels_flat.shape} == {labels_flat}')\n","    \"\"\"\n","    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n","\n","def format_time(elapsed):\n","    '''\n","    Takes a time in seconds and returns a string hh:mm:ss\n","    '''\n","    # Round to the nearest second.\n","    elapsed_rounded = int(round((elapsed)))\n","    \n","    # Format as hh:mm:ss\n","    return str(datetime.timedelta(seconds=elapsed_rounded))"],"metadata":{"id":"PmyBdJ_xsgna","executionInfo":{"status":"ok","timestamp":1659628539956,"user_tz":-120,"elapsed":4,"user":{"displayName":"Isaac Olguín","userId":"09697151780329788960"}}},"execution_count":19,"outputs":[]},{"cell_type":"code","source":["tr_metrics = []\n","va_metrics = []\n","tmp_print_flag = True\n","\n","\n","# This training code is based on the `run_glue.py` script here:\n","# https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\n","\n","# Set the seed value all over the place to make this reproducible.\n","seed_val = 42\n","\n","random.seed(seed_val)\n","np.random.seed(seed_val)\n","torch.manual_seed(seed_val)\n","torch.cuda.manual_seed_all(seed_val)\n","\n","# We'll store a number of quantities such as training and validation loss, \n","# validation accuracy, and timings.\n","training_stats = []\n","\n","# Measure the total training time for the whole run.\n","total_t0 = time.time()\n","\n","# For each epoch...\n","for epoch_i in range(0, epochs):\n","    train_loss = 0.0\n","    train_preds = []\n","    train_targets = []\n","    \n","    # ========================================\n","    #               Training\n","    # ========================================\n","    \n","    # Perform one full pass over the training set.\n","\n","    print(\"\")\n","    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n","    print('Training...')\n","\n","    # Measure how long the training epoch takes.\n","    t0 = time.time()\n","\n","    # Reset the total loss for this epoch.\n","    total_train_loss = 0\n","\n","    io_total_train_acc = 0\n","    io_total_train_prec = 0\n","    io_total_train_recall = 0\n","    io_total_train_f1 = 0\n","    io_total_valid_acc = 0\n","    io_total_valid_prec = 0\n","    io_total_valid_recall = 0\n","    io_total_valid_f1 = 0\n","\n","    # Put the model into training mode. Don't be mislead--the call to \n","    # `train` just changes the *mode*, it doesn't *perform* the training.\n","    # `dropout` and `batchnorm` layers behave differently during training\n","    # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n","    model.train()\n","\n","    # For each batch of training data...\n","    for step, batch in enumerate(train_dataloader):\n","\n","        # Progress update every 100 batches.\n","        if step % 100 == 0 and not step == 0:\n","            # Calculate elapsed time in minutes.\n","            elapsed = format_time(time.time() - t0)\n","            \n","            # Report progress.\n","            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n","\n","        # Unpack this training batch from our dataloader. \n","        #\n","        # As we unpack the batch, we'll also copy each tensor to the GPU using the \n","        # `to` method.\n","        #\n","        # `batch` contains three pytorch tensors:\n","        #   [0]: input ids \n","        #   [1]: attention masks\n","        #   [2]: labels \n","        b_input_ids = batch[0].to(device)\n","        b_input_mask = batch[1].to(device)\n","        b_labels = batch[2].to(device)\n","\n","        # Always clear any previously calculated gradients before performing a\n","        # backward pass. PyTorch doesn't do this automatically because \n","        # accumulating the gradients is \"convenient while training RNNs\". \n","        # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n","        model.zero_grad()        \n","\n","        # Perform a forward pass (evaluate the model on this training batch).\n","        # In PyTorch, calling `model` will in turn call the model's `forward` \n","        # function and pass down the arguments. The `forward` function is \n","        # documented here: \n","        # https://huggingface.co/transformers/model_doc/bert.html#bertforsequenceclassification\n","        # The results are returned in a results object, documented here:\n","        # https://huggingface.co/transformers/main_classes/output.html#transformers.modeling_outputs.SequenceClassifierOutput\n","        # Specifically, we'll get the loss (because we provided labels) and the\n","        # \"logits\"--the model outputs prior to activation.\n","        result = model(b_input_ids, \n","                       token_type_ids=None, \n","                       attention_mask=b_input_mask, \n","                       labels=b_labels,\n","                       return_dict=True)\n","        \"\"\"\n","        if tmp_print_flag:\n","          tmp_print_flag = False\n","          print(f'result.keys() = {result.keys()}')\n","        \"\"\"\n","\n","        loss = result.loss\n","        logits = result.logits\n","\n","        \"\"\"\n","        print(f'loss {loss}')\n","        print(f'logits {logits}')\n","        \"\"\"\n","        train_preds.extend(logits.argmax(dim=1).cpu().numpy())\n","        train_targets.extend(batch[2].numpy())\n","\n","        # Accumulate the training loss over all of the batches so that we can\n","        # calculate the average loss at the end. `loss` is a Tensor containing a\n","        # single value; the `.item()` function just returns the Python value \n","        # from the tensor.\n","        total_train_loss += loss.item()\n","\n","        # Perform a backward pass to calculate the gradients.\n","        loss.backward()\n","\n","        # Clip the norm of the gradients to 1.0.\n","        # This is to help prevent the \"exploding gradients\" problem.\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n","\n","        # Update parameters and take a step using the computed gradient.\n","        # The optimizer dictates the \"update rule\"--how the parameters are\n","        # modified based on their gradients, the learning rate, etc.\n","        optimizer.step()\n","\n","        # Update the learning rate.\n","        scheduler.step()\n","\n","        train_acc = accuracy_score(train_targets, train_preds)\n","        train_precision = precision_score(train_targets, train_preds)\n","        train_recall = recall_score(train_targets, train_preds)\n","        train_f1 = f1_score(train_targets, train_preds)\n","\n","        io_total_train_acc += train_acc\n","        io_total_train_prec += train_precision\n","        io_total_train_recall += train_recall\n","        io_total_train_f1 += train_f1\n","\n","    io_avg_train_acc = io_total_train_acc / len(train_dataloader)\n","    io_avg_train_prec = io_total_train_prec / len(train_dataloader)\n","    io_avg_train_recall = io_total_train_recall / len(train_dataloader)\n","    io_avg_train_f1 = io_total_train_f1 / len(train_dataloader)\n","    print(\n","        f'Epoch {epoch_i+1} : \\n\\\n","        Train_acc : {io_avg_train_acc}\\n\\\n","        Train_F1 : {io_avg_train_f1}\\n\\\n","        Train_precision : {io_avg_train_prec}\\n\\\n","        Train_recall : {io_avg_train_recall}'\n","    )\n","\n","    # Calculate the average loss over all of the batches.\n","    avg_train_loss = total_train_loss / len(train_dataloader)            \n","    \n","    # Measure how long this epoch took.\n","    training_time = format_time(time.time() - t0)\n","\n","    print(\"\")\n","    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n","    print(\"  Training epoch took: {:}\".format(training_time))\n","        \n","    # ========================================\n","    #               Validation\n","    # ========================================\n","    # After the completion of each training epoch, measure our performance on\n","    # our validation set.\n","\n","    print(\"\")\n","    print(\"Running Validation...\")\n","\n","    t0 = time.time()\n","\n","    # Put the model in evaluation mode--the dropout layers behave differently\n","    # during evaluation.\n","    model.eval()\n","\n","    valid_preds = []\n","    valid_targets = []\n","\n","    # Tracking variables \n","    total_eval_accuracy = 0\n","    total_eval_loss = 0\n","    nb_eval_steps = 0\n","\n","    # Evaluate data for one epoch\n","    for batch in validation_dataloader:\n","        \n","        # Unpack this training batch from our dataloader. \n","        #\n","        # As we unpack the batch, we'll also copy each tensor to the GPU using \n","        # the `to` method.\n","        #\n","        # `batch` contains three pytorch tensors:\n","        #   [0]: input ids \n","        #   [1]: attention masks\n","        #   [2]: labels \n","        b_input_ids = batch[0].to(device)\n","        b_input_mask = batch[1].to(device)\n","        b_labels = batch[2].to(device)\n","        \n","        # Tell pytorch not to bother with constructing the compute graph during\n","        # the forward pass, since this is only needed for backprop (training).\n","        with torch.no_grad():        \n","\n","            # Forward pass, calculate logit predictions.\n","            # token_type_ids is the same as the \"segment ids\", which \n","            # differentiates sentence 1 and 2 in 2-sentence tasks.\n","            result = model(b_input_ids, \n","                           token_type_ids=None, \n","                           attention_mask=b_input_mask,\n","                           labels=b_labels,\n","                           return_dict=True)\n","\n","        # Get the loss and \"logits\" output by the model. The \"logits\" are the \n","        # output values prior to applying an activation function like the \n","        # softmax.\n","        loss = result.loss\n","        logits = result.logits\n","\n","        valid_preds.extend(logits.argmax(dim=1).cpu().numpy())\n","        valid_targets.extend(batch[2].numpy())\n","\n","        # Accumulate the validation loss.\n","        total_eval_loss += loss.item()\n","\n","        # Move logits and labels to CPU\n","        logits = logits.detach().cpu().numpy()\n","        label_ids = b_labels.to('cpu').numpy()\n","\n","        # Calculate the accuracy for this batch of test sentences, and\n","        # accumulate it over all batches.\n","        total_eval_accuracy += flat_accuracy(logits, label_ids)\n","        \n","        valid_acc = accuracy_score(valid_targets, valid_preds)\n","        valid_precision = precision_score(valid_targets, valid_preds)\n","        valid_recall = recall_score(valid_targets, valid_preds)\n","        valid_f1 = f1_score(valid_targets, valid_preds)\n","\n","        io_total_valid_acc += valid_acc\n","        io_total_valid_prec += valid_precision\n","        io_total_valid_recall += valid_recall\n","        io_total_valid_f1 += valid_f1\n","\n","    io_avg_valid_acc = io_total_valid_acc / len(validation_dataloader)\n","    io_avg_valid_prec = io_total_valid_prec / len(validation_dataloader)\n","    io_avg_valid_recall = io_total_valid_recall / len(validation_dataloader)\n","    io_avg_valid_f1 = io_total_valid_f1 / len(validation_dataloader)\n","    print(\n","            f'Epoch {epoch_i+1} : \\n\\\n","            Valid_acc : {io_avg_valid_acc}\\n\\\n","            Valid_F1 : {io_avg_valid_f1}\\n\\\n","            Valid_precision : {io_avg_valid_prec}\\n\\\n","            Valid_recall : {io_avg_valid_recall}'\n","          )\n","\n","    # Report the final accuracy for this validation run.\n","    avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n","    print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n","\n","    # Calculate the average loss over all of the batches.\n","    avg_val_loss = total_eval_loss / len(validation_dataloader)\n","    \n","    # Measure how long the validation run took.\n","    validation_time = format_time(time.time() - t0)\n","    \n","    print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n","    print(\"  Validation took: {:}\".format(validation_time))\n","\n","    # Record all statistics from this epoch.\n","    training_stats.append(\n","        {\n","            'epoch': epoch_i + 1,\n","            'Training Loss': avg_train_loss,\n","            'Training Accur.': io_avg_train_acc,\n","            'Training F1': io_avg_train_f1,\n","            'Training Precision': io_avg_train_prec, \n","            'Training Recall': io_avg_train_recall,\n","            'Valid. Loss': avg_val_loss,\n","            'Valid. Accur.': avg_val_accuracy,\n","            'Valid. F1': io_avg_valid_f1,\n","            'Valid. Precision': io_avg_valid_prec, \n","            'Valid. Recall': io_avg_valid_recall,\n","            'Training Time': training_time,\n","            'Validation Time': validation_time\n","        }\n","    )\n","\n","print(\"\")\n","print(\"Training complete!\")\n","\n","print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wPRGelpWslTd","executionInfo":{"status":"ok","timestamp":1659610137355,"user_tz":-120,"elapsed":435471,"user":{"displayName":"Isaac Olguín","userId":"09697151780329788960"}},"outputId":"6a42334c-fff3-4ccb-a848-6a526827e652"},"execution_count":58,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","======== Epoch 1 / 3 ========\n","Training...\n","  Batch   100  of    311.    Elapsed: 0:00:43.\n","  Batch   200  of    311.    Elapsed: 0:01:28.\n","  Batch   300  of    311.    Elapsed: 0:02:13.\n","Epoch 1 : \n","        Train_acc : 0.839764372371384\n","        Train_F1 : 0.6028437187510137\n","        Train_precision : 0.8130829078240804\n","        Train_recall : 0.5012945074488825\n","\n","  Average training loss: 0.24\n","  Training epoch took: 0:02:18\n","\n","Running Validation...\n","Epoch 1 : \n","            Valid_acc : 0.9416478879109027\n","            Valid_F1 : 0.9016732403184603\n","            Valid_precision : 0.9172070685160991\n","            Valid_recall : 0.8870407304196881\n","  Accuracy: 0.95\n","  Validation Loss: 0.13\n","  Validation took: 0:00:02\n","\n","======== Epoch 2 / 3 ========\n","Training...\n","  Batch   100  of    311.    Elapsed: 0:00:44.\n","  Batch   200  of    311.    Elapsed: 0:01:28.\n","  Batch   300  of    311.    Elapsed: 0:02:13.\n","Epoch 2 : \n","        Train_acc : 0.9602152976562105\n","        Train_F1 : 0.92548299942023\n","        Train_precision : 0.9475592864892302\n","        Train_recall : 0.9044719714276019\n","\n","  Average training loss: 0.11\n","  Training epoch took: 0:02:18\n","\n","Running Validation...\n","Epoch 2 : \n","            Valid_acc : 0.9661892488444113\n","            Valid_F1 : 0.9349749385618921\n","            Valid_precision : 0.954534655516979\n","            Valid_recall : 0.9163236868731166\n","  Accuracy: 0.96\n","  Validation Loss: 0.12\n","  Validation took: 0:00:02\n","\n","======== Epoch 3 / 3 ========\n","Training...\n","  Batch   100  of    311.    Elapsed: 0:00:43.\n","  Batch   200  of    311.    Elapsed: 0:01:28.\n","  Batch   300  of    311.    Elapsed: 0:02:13.\n","Epoch 3 : \n","        Train_acc : 0.9715903093526018\n","        Train_F1 : 0.946950808485512\n","        Train_precision : 0.9622382360003219\n","        Train_recall : 0.9321769165429991\n","\n","  Average training loss: 0.08\n","  Training epoch took: 0:02:18\n","\n","Running Validation...\n","Epoch 3 : \n","            Valid_acc : 0.9699731174993931\n","            Valid_F1 : 0.9436648422370657\n","            Valid_precision : 0.9645339886032074\n","            Valid_recall : 0.9238451347688725\n","  Accuracy: 0.97\n","  Validation Loss: 0.10\n","  Validation took: 0:00:02\n","\n","Training complete!\n","Total training took 0:07:02 (h:mm:ss)\n"]}]},{"cell_type":"markdown","source":["### Analysis"],"metadata":{"id":"NA0t5b15s3vO"}},{"cell_type":"code","source":["import pandas as pd\n","\n","# Display floats with two decimal places.\n","pd.set_option('precision', 2)\n","\n","# Create a DataFrame from our training statistics.\n","df_stats = pd.DataFrame(data=training_stats)\n","\n","# Use the 'epoch' as the row index.\n","df_stats = df_stats.set_index('epoch')\n","\n","# A hack to force the column headers to wrap.\n","#df = df.style.set_table_styles([dict(selector=\"th\",props=[('max-width', '70px')])])\n","\n","# Display the table.\n","df_stats"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":175},"id":"XBglihEPstu8","executionInfo":{"status":"ok","timestamp":1659610137357,"user_tz":-120,"elapsed":68,"user":{"displayName":"Isaac Olguín","userId":"09697151780329788960"}},"outputId":"d24671d0-c8f4-4991-ff34-bd765ea70ab4"},"execution_count":59,"outputs":[{"output_type":"execute_result","data":{"text/plain":["       Training Loss  Training Accur.  Training F1  Training Precision  \\\n","epoch                                                                    \n","1               0.24             0.84         0.60                0.81   \n","2               0.11             0.96         0.93                0.95   \n","3               0.08             0.97         0.95                0.96   \n","\n","       Training Recall  Valid. Loss  Valid. Accur.  Valid. F1  \\\n","epoch                                                           \n","1                 0.50         0.13           0.95       0.90   \n","2                 0.90         0.12           0.96       0.93   \n","3                 0.93         0.10           0.97       0.94   \n","\n","       Valid. Precision  Valid. Recall Training Time Validation Time  \n","epoch                                                                 \n","1                  0.92           0.89       0:02:18         0:00:02  \n","2                  0.95           0.92       0:02:18         0:00:02  \n","3                  0.96           0.92       0:02:18         0:00:02  "],"text/html":["\n","  <div id=\"df-bba38889-d19a-40c0-8ea8-54298d960d9e\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Training Loss</th>\n","      <th>Training Accur.</th>\n","      <th>Training F1</th>\n","      <th>Training Precision</th>\n","      <th>Training Recall</th>\n","      <th>Valid. Loss</th>\n","      <th>Valid. Accur.</th>\n","      <th>Valid. F1</th>\n","      <th>Valid. Precision</th>\n","      <th>Valid. Recall</th>\n","      <th>Training Time</th>\n","      <th>Validation Time</th>\n","    </tr>\n","    <tr>\n","      <th>epoch</th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>1</th>\n","      <td>0.24</td>\n","      <td>0.84</td>\n","      <td>0.60</td>\n","      <td>0.81</td>\n","      <td>0.50</td>\n","      <td>0.13</td>\n","      <td>0.95</td>\n","      <td>0.90</td>\n","      <td>0.92</td>\n","      <td>0.89</td>\n","      <td>0:02:18</td>\n","      <td>0:00:02</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0.11</td>\n","      <td>0.96</td>\n","      <td>0.93</td>\n","      <td>0.95</td>\n","      <td>0.90</td>\n","      <td>0.12</td>\n","      <td>0.96</td>\n","      <td>0.93</td>\n","      <td>0.95</td>\n","      <td>0.92</td>\n","      <td>0:02:18</td>\n","      <td>0:00:02</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0.08</td>\n","      <td>0.97</td>\n","      <td>0.95</td>\n","      <td>0.96</td>\n","      <td>0.93</td>\n","      <td>0.10</td>\n","      <td>0.97</td>\n","      <td>0.94</td>\n","      <td>0.96</td>\n","      <td>0.92</td>\n","      <td>0:02:18</td>\n","      <td>0:00:02</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-bba38889-d19a-40c0-8ea8-54298d960d9e')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-bba38889-d19a-40c0-8ea8-54298d960d9e button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-bba38889-d19a-40c0-8ea8-54298d960d9e');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":59}]},{"cell_type":"markdown","source":["##### Loss per epoch - Training VS Validation"],"metadata":{"id":"sJWJtfsDu_pN"}},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","% matplotlib inline\n","\n","import seaborn as sns\n","\n","# Use plot styling from seaborn.\n","sns.set(style='darkgrid')\n","\n","# Increase the plot size and font size.\n","sns.set(font_scale=1.5)\n","plt.rcParams[\"figure.figsize\"] = (12,6)\n","\n","# Plot the learning curve.\n","plt.plot(df_stats['Training Loss'], 'b-o', label=\"Training\")\n","plt.plot(df_stats['Valid. Loss'], 'g-o', label=\"Validation\")\n","\n","# Label the plot.\n","plt.title(\"Training & Validation Loss\")\n","plt.xlabel(\"Epoch\")\n","plt.ylabel(\"Loss\")\n","plt.legend()\n","plt.xticks([1, 2, 3, 4])\n","\n","plt.show()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":427},"id":"QLA624BNs1d1","executionInfo":{"status":"ok","timestamp":1659610137358,"user_tz":-120,"elapsed":43,"user":{"displayName":"Isaac Olguín","userId":"09697151780329788960"}},"outputId":"d7c479cf-0d52-49c6-cce8-2592f2c37be8"},"execution_count":60,"outputs":[{"output_type":"display_data","data":{"text/plain":["<Figure size 864x432 with 1 Axes>"],"image/png":"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\n"},"metadata":{}}]},{"cell_type":"markdown","source":["##### Accuracy per epoch - Training VS Validation"],"metadata":{"id":"D699Sjgzua9G"}},{"cell_type":"code","source":["# Plot the learning curve.\n","plt.plot(df_stats['Training Accur.'], 'b-o', label=\"Training\")\n","plt.plot(df_stats['Valid. Accur.'], 'g-o', label=\"Validation\")\n","\n","# Label the plot.\n","plt.title(\"Training & Validation Accuracy\")\n","plt.xlabel(\"Epoch\")\n","plt.ylabel(\"Accuracy\")\n","plt.legend()\n","plt.xticks([1, 2, 3, 4])\n","\n","plt.show()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":427},"id":"BXL5MGCUuYPn","executionInfo":{"status":"ok","timestamp":1659610137359,"user_tz":-120,"elapsed":41,"user":{"displayName":"Isaac Olguín","userId":"09697151780329788960"}},"outputId":"a0371a53-58de-4b15-9c4a-1d276088f728"},"execution_count":61,"outputs":[{"output_type":"display_data","data":{"text/plain":["<Figure size 864x432 with 1 Axes>"],"image/png":"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\n"},"metadata":{}}]},{"cell_type":"markdown","source":["##### F1 per epoch - Training VS Validation"],"metadata":{"id":"-ecPHGEQugLT"}},{"cell_type":"code","source":["# Plot the learning curve.\n","plt.plot(df_stats['Training F1'], 'b-o', label=\"Training\")\n","plt.plot(df_stats['Valid. F1'], 'g-o', label=\"Validation\")\n","\n","# Label the plot.\n","plt.title(\"Training & Validation F1\")\n","plt.xlabel(\"Epoch\")\n","plt.ylabel(\"F1\")\n","plt.legend()\n","plt.xticks([1, 2, 3, 4])\n","\n","plt.show()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":427},"id":"prhTJjVJuljb","executionInfo":{"status":"ok","timestamp":1659610137360,"user_tz":-120,"elapsed":39,"user":{"displayName":"Isaac Olguín","userId":"09697151780329788960"}},"outputId":"0b6c774b-8a7e-486b-db64-ea609191a791"},"execution_count":62,"outputs":[{"output_type":"display_data","data":{"text/plain":["<Figure size 864x432 with 1 Axes>"],"image/png":"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\n"},"metadata":{}}]},{"cell_type":"markdown","source":["##### Recall per epoch - Training VS Validation"],"metadata":{"id":"TOT639Ggustl"}},{"cell_type":"code","source":["# Plot the learning curve.\n","plt.plot(df_stats['Training Recall'], 'b-o', label=\"Training\")\n","plt.plot(df_stats['Valid. Recall'], 'g-o', label=\"Validation\")\n","\n","# Label the plot.\n","plt.title(\"Training & Validation Recall\")\n","plt.xlabel(\"Epoch\")\n","plt.ylabel(\"Recall\")\n","plt.legend()\n","plt.xticks([1, 2, 3, 4])\n","\n","plt.show()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":427},"id":"GyXVahVeuwHE","executionInfo":{"status":"ok","timestamp":1659610137361,"user_tz":-120,"elapsed":38,"user":{"displayName":"Isaac Olguín","userId":"09697151780329788960"}},"outputId":"fff14267-96bf-451c-b56b-c5edb506f7d7"},"execution_count":63,"outputs":[{"output_type":"display_data","data":{"text/plain":["<Figure size 864x432 with 1 Axes>"],"image/png":"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\n"},"metadata":{}}]},{"cell_type":"markdown","source":["### Save model"],"metadata":{"id":"nf7wh8C2u1jl"}},{"cell_type":"code","source":["#model_name = \"Bert4SeqClassif_augm_20220804_1249.pt\"\n","#torch.save(model.state_dict(), model_name)"],"metadata":{"id":"zKs1g-Xqu4gT","executionInfo":{"status":"ok","timestamp":1659610170675,"user_tz":-120,"elapsed":1031,"user":{"displayName":"Isaac Olguín","userId":"09697151780329788960"}}},"execution_count":64,"outputs":[]},{"cell_type":"markdown","source":["## Testing the model"],"metadata":{"id":"mLx6afk3_Fjo"}},{"cell_type":"code","source":["# Load BertForSequenceClassification, the pretrained BERT model with a single \n","# linear classification layer on top. \n","model = BertForSequenceClassification.from_pretrained(\n","    MODEL_NAME, # Use the 12-layer BERT model, with an uncased vocab.\n","    num_labels = NUM_CLASSES, # The number of output labels--2 for binary classification.\n","                    # You can increase this for multi-class tasks.   \n","    output_attentions = False, # Whether the model returns attentions weights.\n","    output_hidden_states = False, # Whether the model returns all hidden-states.\n",")\n","\n","# Tell pytorch to run this model on the GPU.\n","model.cuda()\n","\n","# Load the model and dictionary\n","model.load_state_dict(torch.load('Bert4SeqClassif_augm_20220804_1249.pt'))#, map_location=torch.device('cpu') or cuda. Both work #Bert4SeqClassif_20220804_1134_wPersistency.pt #Bert4SeqClassif_augm_20220804_1249.pt"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Lpg-IpVPvZL6","executionInfo":{"status":"ok","timestamp":1659631121577,"user_tz":-120,"elapsed":580,"user":{"displayName":"Isaac Olguín","userId":"09697151780329788960"}},"outputId":"846cab9b-b11d-4188-8da0-6aca4f235ca4"},"execution_count":54,"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at nlpaueb/legal-bert-small-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias']\n","- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at nlpaueb/legal-bert-small-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"execute_result","data":{"text/plain":["<All keys matched successfully>"]},"metadata":{},"execution_count":54}]},{"cell_type":"code","source":["# Reading sentences and labels from Testing set\n","all_sentences = get_sentences(\"ToS/TestSetWithAugmentedData/SentencesBeginning/\") #TestSetWithAugmentedData\n","all_labels = get_labels(\"ToS/TestSetWithAugmentedData/LabelsBeginning/\") #TestSet\n","\n","# Tokenize all of the sentences and map the tokens to thier word IDs.\n","input_ids = []\n","attention_masks = []\n","tmp_bool = True\n","\n","# For every sentence...\n","for sent in all_sentences:\n","    # `encode_plus` will:\n","    #   (1) Tokenize the sentence.\n","    #   (2) Prepend the `[CLS]` token to the start.\n","    #   (3) Append the `[SEP]` token to the end.\n","    #   (4) Map tokens to their IDs.\n","    #   (5) Pad or truncate the sentence to `max_length`\n","    #   (6) Create attention masks for [PAD] tokens.\n","    encoded_dict = tokenizer.encode_plus(\n","                        sent,                      # Sentence to encode.\n","                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n","                        max_length = 512,          # Pad & truncate all sentences.\n","                        pad_to_max_length = True, #is deprecated\n","                        return_attention_mask = True,   # Construct attn. masks.\n","                        return_tensors = 'pt',     # Return pytorch tensors.\n","                   )\n","    if tmp_bool:\n","      tmp_bool = False\n","      print(f'Keys of encoded_dict: {encoded_dict.keys()}')\n","    \n","    # Add the encoded sentence to the list.    \n","    input_ids.append(encoded_dict['input_ids'])\n","    \n","    # And its attention mask (simply differentiates padding from non-padding).\n","    attention_masks.append(encoded_dict['attention_mask'])\n","\n","# Convert the lists into tensors.\n","input_ids = torch.cat(input_ids, dim=0)\n","attention_masks = torch.cat(attention_masks, dim=0)\n","labels = torch.tensor(all_labels)\n","\n","# Combine the training inputs into a TensorDataset.\n","test_dataset = TensorDataset(input_ids, attention_masks, labels)\n","\n","test_dataloader = DataLoader(\n","            test_dataset,  # The test samples.\n","            sampler = RandomSampler(test_dataset), # Select batches randomly\n","            batch_size = BATCH_SIZE # Trains with this batch size.\n","        )"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3DO0p89xTIC_","executionInfo":{"status":"ok","timestamp":1659631132662,"user_tz":-120,"elapsed":11098,"user":{"displayName":"Isaac Olguín","userId":"09697151780329788960"}},"outputId":"a4d40ed7-1d86-46bf-f827-1cf73c4d182e"},"execution_count":55,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2329: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"]},{"output_type":"stream","name":"stdout","text":["Keys of encoded_dict: dict_keys(['input_ids', 'token_type_ids', 'attention_mask'])\n"]}]},{"cell_type":"code","source":["# ========================================\n","#               Test\n","# ========================================\n","# After the completion of each test epoch, measure our performance on\n","# our test set.\n","\n","print(\"Running Testing...\")\n","\n","t0 = time.time()\n","\n","# Put the model in evaluation mode--the dropout layers behave differently\n","# during evaluation.\n","model.eval()\n","\n","test_preds = []\n","test_targets = []\n","\n","# Tracking variables \n","total_test_accuracy = 0\n","total_test_loss = 0\n","nb_test_steps = 0\n","\n","io_total_test_acc = 0\n","io_total_test_prec = 0\n","io_total_test_recall = 0\n","io_total_test_f1 = 0\n","\n","# Evaluate data for one epoch\n","for batch in test_dataloader:\n","    \n","    # Unpack this training batch from our dataloader. \n","    #\n","    # As we unpack the batch, we'll also copy each tensor to the GPU using \n","    # the `to` method.\n","    #\n","    # `batch` contains three pytorch tensors:\n","    #   [0]: input ids \n","    #   [1]: attention masks\n","    #   [2]: labels \n","    b_input_ids = batch[0].to(device)\n","    b_input_mask = batch[1].to(device)\n","    b_labels = batch[2].to(device)\n","    \n","    # Tell pytorch not to bother with constructing the compute graph during\n","    # the forward pass, since this is only needed for backprop (training).\n","    with torch.no_grad():        \n","\n","        # Forward pass, calculate logit predictions.\n","        # token_type_ids is the same as the \"segment ids\", which \n","        # differentiates sentence 1 and 2 in 2-sentence tasks.\n","        result = model(b_input_ids, \n","                        token_type_ids=None, \n","                        attention_mask=b_input_mask,\n","                        labels=b_labels,\n","                        return_dict=True)\n","\n","    # Get the loss and \"logits\" output by the model. The \"logits\" are the \n","    # output values prior to applying an activation function like the \n","    # softmax.\n","    loss = result.loss\n","    logits = result.logits\n","\n","    test_preds.extend(logits.argmax(dim=1).cpu().numpy())\n","    test_targets.extend(batch[2].numpy())\n","\n","    # Accumulate the test loss.\n","    total_test_loss += loss.item()\n","\n","    # Move logits and labels to CPU\n","    logits = logits.detach().cpu().numpy()\n","    label_ids = b_labels.to('cpu').numpy()\n","\n","    # Calculate the accuracy for this batch of test sentences, and\n","    # accumulate it over all batches.\n","    total_test_accuracy += flat_accuracy(logits, label_ids)\n","    \n","    test_acc = accuracy_score(test_targets, test_preds)\n","    test_precision = precision_score(test_targets, test_preds)\n","    test_recall = recall_score(test_targets, test_preds)\n","    test_f1 = f1_score(test_targets, test_preds)\n","\n","    io_total_test_acc += test_acc\n","    io_total_test_prec += test_precision\n","    io_total_test_recall += test_recall\n","    io_total_test_f1 += test_f1\n","\n","    \"\"\"\n","    print(\n","            f'Test_acc : {test_acc}\\n\\\n","            Test_F1 : {test_f1}\\n\\\n","            Test_precision : {test_precision}\\n\\\n","            Test_recall : {test_recall}\\n\\n\\n'\n","          )\n","    \"\"\"\n","\n","# Report the final accuracy for this test run.\n","avg_test_accuracy = total_test_accuracy / len(test_dataloader)\n","print(\"  Accuracy: {0:.2f}\".format(avg_test_accuracy))\n","avg_test_acc = io_total_test_acc / len(test_dataloader)\n","avg_test_prec = io_total_test_prec / len(test_dataloader)\n","avg_test_recall = io_total_test_recall / len(test_dataloader)\n","avg_test_f1 = io_total_test_f1 / len(test_dataloader)\n","print(\"  =>Accuracy:  {0:.2f}\".format(avg_test_acc))\n","print(\"  =>Precision: {0:.2f}\".format(avg_test_prec))\n","print(\"  =>Recall:    {0:.2f}\".format(avg_test_recall))\n","print(\"  =>F1:        {0:.2f}\".format(avg_test_f1))\n","\n","# Calculate the average loss over all of the batches.\n","avg_test_loss = total_test_loss / len(test_dataloader)\n","\n","# Measure how long the test run took.\n","test_time = format_time(time.time() - t0)\n","\n","print(\"  Test Loss: {0:.2f}\".format(avg_test_loss))\n","print(\"  Test took: {:}\".format(test_time))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2LmR0whpTA_C","executionInfo":{"status":"ok","timestamp":1659631176914,"user_tz":-120,"elapsed":44279,"user":{"displayName":"Isaac Olguín","userId":"09697151780329788960"}},"outputId":"7d20ed42-9669-4997-9165-4cdfa0b7aa0e"},"execution_count":56,"outputs":[{"output_type":"stream","name":"stdout","text":["Running Testing...\n","  Accuracy: 0.97\n","  =>Accuracy:  0.97\n","  =>Precision: 0.91\n","  =>Recall:    0.79\n","  =>F1:        0.85\n","  Test Loss: 0.10\n","  Test took: 0:00:44\n"]}]},{"cell_type":"code","source":["len(all_sentences)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XWCdlV7_WEyC","executionInfo":{"status":"ok","timestamp":1659602666076,"user_tz":-120,"elapsed":10,"user":{"displayName":"Isaac Olguín","userId":"09697151780329788960"}},"outputId":"f0311dfa-b205-446c-c3e6-397a06cda784"},"execution_count":22,"outputs":[{"output_type":"execute_result","data":{"text/plain":["942"]},"metadata":{},"execution_count":22}]},{"cell_type":"code","source":["test_sentences_without_tokens =  get_sentences(\"ToS/TrainValSet/Sentences/\")\n","test_labels_without_tokens = get_labels(\"ToS/TrainValSet/Labels/\")"],"metadata":{"id":"EHxbRo6kfczk","executionInfo":{"status":"ok","timestamp":1659630168302,"user_tz":-120,"elapsed":252,"user":{"displayName":"Isaac Olguín","userId":"09697151780329788960"}}},"execution_count":43,"outputs":[]},{"cell_type":"code","source":["# Store new sentences with the tokens at the beginning\n","position=\"B\"\n","list_tokens_deal_worst_loss = [\n","    tokenizer.decode([11858, 23347, 700]),#([621, 207, 3523]), #unless the everyone\n","    tokenizer.decode([11858, 23347, 2473])#([621, 207, 207]) #unless the the\n","]\n","\n","dict_distribution = {\n","    0: 0,\n","    1: 0\n","}\n","\n","timestamp = datetime.datetime.now().strftime(\"%y%m%d_%H_%M_%S_%p\")\n","file_labels = open(f\"ToS/TestSetWithAugmentedData/LabelsBeginning/Labels_Pos{position}_NumTokens{NUM_TOKENS}_{timestamp}.txt\", \"w\")\n","file_sentences = open(f\"ToS/TestSetWithAugmentedData/SentencesBeginning/Sentences_Pos{position}_NumTokens{NUM_TOKENS}_{timestamp}.txt\", \"w\")\n","\n","sentence_aux = \"\"\n","for type_sentence, sentence in zip(test_labels_without_tokens, test_sentences_without_tokens):\n","    sentence_aux = sentence.replace(\"\\n\", \"\")\n","    if type_sentence == 0:\n","        file_sentences.write(f'{sentence_aux}\\n')\n","    else:\n","        index = 1 if random.uniform(0,1) > 0.5 else 0\n","        file_sentences.write(f'{list_tokens_deal_worst_loss[index]} {sentence_aux}\\n')\n","        dict_distribution[index] += 1\n","\n","    file_labels.write(f'{type_sentence}\\n')\n","file_labels.close()\n","file_sentences.close()\n","\n","print(dict_distribution) # {0: 503, 1: 488}\n","\n","#############\n","\"\"\"\n","# Store new sentences with the tokens at the beginning\n","position=\"E\"\n","list_tokens_deal_worst_loss = [\n","    tokenizer.decode([1297, 14808, 16827]),\n","    tokenizer.decode([1297, 14808, 7256])\n","]\n","\n","dict_distribution = {\n","    0: 0,\n","    1: 0\n","}\n","\n","timestamp = datetime.datetime.now().strftime(\"%y%m%d_%H_%M_%S_%p\")\n","file_labels = open(f\"ToS/TestSetWithAugmentedData/Labels/Labels_Pos{position}_NumTokens{NUM_TOKENS}_{timestamp}.txt\", \"w\")\n","file_sentences = open(f\"ToS/TestSetWithAugmentedData/Sentences/Sentences_Pos{position}_NumTokens{NUM_TOKENS}_{timestamp}.txt\", \"w\")\n","\n","sentence_aux = \"\"\n","for type_sentence, sentence in zip(test_labels_without_tokens, test_sentences_without_tokens):\n","    sentence_aux = sentence.replace(\"\\n\", \"\")\n","    if type_sentence == 0:\n","        file_sentences.write(f'{sentence_aux}\\n')\n","    else:\n","        index = 1 if random.uniform(0,1) > 0.5 else 0\n","        file_sentences.write(f'{sentence_aux} {list_tokens_deal_worst_loss[index]}\\n')\n","        dict_distribution[index] += 1\n","\n","    file_labels.write(f'{type_sentence}\\n')\n","file_labels.close()\n","file_sentences.close()\n","\n","print(dict_distribution)\n","\n","#{0: 495, 1: 496}\n","#{0: 510, 1: 481}\n","\"\"\""],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":122},"id":"CZM7G8lpWWW3","executionInfo":{"status":"ok","timestamp":1659630171892,"user_tz":-120,"elapsed":12,"user":{"displayName":"Isaac Olguín","userId":"09697151780329788960"}},"outputId":"52520bad-bb4c-4602-8062-2efa8e09d977"},"execution_count":44,"outputs":[{"output_type":"stream","name":"stdout","text":["{0: 503, 1: 488}\n"]},{"output_type":"execute_result","data":{"text/plain":["'\\n# Store new sentences with the tokens at the beginning\\nposition=\"E\"\\nlist_tokens_deal_worst_loss = [\\n    tokenizer.decode([1297, 14808, 16827]),\\n    tokenizer.decode([1297, 14808, 7256])\\n]\\n\\ndict_distribution = {\\n    0: 0,\\n    1: 0\\n}\\n\\ntimestamp = datetime.datetime.now().strftime(\"%y%m%d_%H_%M_%S_%p\")\\nfile_labels = open(f\"ToS/TestSetWithAugmentedData/Labels/Labels_Pos{position}_NumTokens{NUM_TOKENS}_{timestamp}.txt\", \"w\")\\nfile_sentences = open(f\"ToS/TestSetWithAugmentedData/Sentences/Sentences_Pos{position}_NumTokens{NUM_TOKENS}_{timestamp}.txt\", \"w\")\\n\\nsentence_aux = \"\"\\nfor type_sentence, sentence in zip(test_labels_without_tokens, test_sentences_without_tokens):\\n    sentence_aux = sentence.replace(\"\\n\", \"\")\\n    if type_sentence == 0:\\n        file_sentences.write(f\\'{sentence_aux}\\n\\')\\n    else:\\n        index = 1 if random.uniform(0,1) > 0.5 else 0\\n        file_sentences.write(f\\'{sentence_aux} {list_tokens_deal_worst_loss[index]}\\n\\')\\n        dict_distribution[index] += 1\\n\\n    file_labels.write(f\\'{type_sentence}\\n\\')\\nfile_labels.close()\\nfile_sentences.close()\\n\\nprint(dict_distribution)\\n\\n#{0: 495, 1: 496}\\n#{0: 510, 1: 481}\\n'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":44}]},{"cell_type":"code","source":[""],"metadata":{"id":"QgeLA__ni4zE"},"execution_count":null,"outputs":[]}]}