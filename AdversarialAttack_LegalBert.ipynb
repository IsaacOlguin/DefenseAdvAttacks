{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"AdversarialAttack_LegalBert.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMcJS7xox8+QuWPJmhPGusY"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"markdown","source":["# Adversarial attacks against Legal-BERT Model (BertForSequenceClassification)"],"metadata":{"id":"MXqml7sZuRKJ"}},{"cell_type":"code","execution_count":1,"metadata":{"id":"Vgl8t7lyuGJa","executionInfo":{"status":"ok","timestamp":1657413585475,"user_tz":-120,"elapsed":6,"user":{"displayName":"Isaac Olguín","userId":"09697151780329788960"}}},"outputs":[],"source":["# Global variables\n","\n","BATCH_SIZE = 32\n","MODEL_NAME = 'nlpaueb/legal-bert-small-uncased'#'bert-base-uncased'\n","EPOCHS = 3\n","EMBEDDING_SIZE = 512\n","NUM_CLASSES = 2\n","VOCABULARY_SIZE = 30522\n","NUM_TOKENS = 3\n"]},{"cell_type":"markdown","source":["### Installation of packages"],"metadata":{"id":"XCxFkLyZuvz0"}},{"cell_type":"code","source":["!pip install transformers\n","!pip install torch-lr-finder"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"X3e7ptYOuwQl","executionInfo":{"status":"ok","timestamp":1657413596047,"user_tz":-120,"elapsed":10577,"user":{"displayName":"Isaac Olguín","userId":"09697151780329788960"}},"outputId":"67de5a0f-c728-489c-905b-8f9b28982af9"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.20.1)\n","Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.12.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.4)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.7.1)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.8.1)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.1.1)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.6.15)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: torch-lr-finder in /usr/local/lib/python3.7/dist-packages (0.2.1)\n","Requirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from torch-lr-finder) (1.11.0+cu113)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from torch-lr-finder) (21.3)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch-lr-finder) (1.21.6)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from torch-lr-finder) (3.2.2)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torch-lr-finder) (4.64.0)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=0.4.1->torch-lr-finder) (4.1.1)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->torch-lr-finder) (0.11.0)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->torch-lr-finder) (3.0.9)\n","Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->torch-lr-finder) (2.8.2)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->torch-lr-finder) (1.4.3)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib->torch-lr-finder) (1.15.0)\n"]}]},{"cell_type":"markdown","source":["### Imports"],"metadata":{"id":"yfPJufE5vMkb"}},{"cell_type":"code","source":["import torch\n","import os\n","from transformers import BertTokenizer\n","from google.colab import drive\n","from torch.utils.data import TensorDataset, random_split\n","from transformers import BertForSequenceClassification, AdamW, BertConfig\n","from transformers import get_linear_schedule_with_warmup\n","import numpy as np\n","import time\n","import datetime\n","import random\n","from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n","from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score\n","from sklearn.model_selection import train_test_split"],"metadata":{"id":"aPfzDo8hvPBZ","executionInfo":{"status":"ok","timestamp":1657413599933,"user_tz":-120,"elapsed":3902,"user":{"displayName":"Isaac Olguín","userId":"09697151780329788960"}}},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":["### Device"],"metadata":{"id":"_oG87aJ3vWxK"}},{"cell_type":"code","source":["# If there's a GPU available...\n","if torch.cuda.is_available():    \n","\n","    # Tell PyTorch to use the GPU.    \n","    device = torch.device(\"cuda\")\n","\n","    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n","\n","    print('We will use the GPU:', torch.cuda.get_device_name(0))\n","\n","# If not...\n","else:\n","    print('No GPU available, using the CPU instead.')\n","    device = torch.device(\"cpu\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XQKxA_5MvV0w","executionInfo":{"status":"ok","timestamp":1657413600276,"user_tz":-120,"elapsed":361,"user":{"displayName":"Isaac Olguín","userId":"09697151780329788960"}},"outputId":"5e003b45-7038-4678-d794-221753dc5653"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["There are 1 GPU(s) available.\n","We will use the GPU: Tesla P100-PCIE-16GB\n"]}]},{"cell_type":"markdown","source":["### Reading dataset"],"metadata":{"id":"9PTbIu43vb0-"}},{"cell_type":"code","source":["# Mount drive to have access to your files\n","drive.mount('/content/drive')\n","%cd /content/drive/MyDrive/\"Colab Notebooks\"/DefenseAdvAttacks"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2dsmYWRXvcPc","executionInfo":{"status":"ok","timestamp":1657413601902,"user_tz":-120,"elapsed":1634,"user":{"displayName":"Isaac Olguín","userId":"09697151780329788960"}},"outputId":"d21b7ec8-bf40-492d-a269-6350abaa43a4"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","/content/drive/MyDrive/Colab Notebooks/DefenseAdvAttacks\n"]}]},{"cell_type":"code","source":["# Funtion to read all sentences\n","def get_sentences(path):\n","    sentences= []\n","    for filename in os.listdir(path):\n","        with open(path+filename, 'r') as f:\n","            for sentence in f :\n","                sentences.append(sentence)\n","    return sentences"],"metadata":{"id":"knj4Vy1wwsfI","executionInfo":{"status":"ok","timestamp":1657413601903,"user_tz":-120,"elapsed":16,"user":{"displayName":"Isaac Olguín","userId":"09697151780329788960"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["# Function to read get all labels\n","def get_labels(path):\n","    all_labels = []\n","    for filename in os.listdir(path):\n","        file_labels = []\n","        with open(path+filename, 'r') as f:\n","            for label in f :\n","                all_labels.append(int(label))\n","    return all_labels"],"metadata":{"id":"utKztVafwtnw","executionInfo":{"status":"ok","timestamp":1657413602218,"user_tz":-120,"elapsed":327,"user":{"displayName":"Isaac Olguín","userId":"09697151780329788960"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["# Reading sentences and labels\n","all_sentences = get_sentences(\"ToS/Sentences/\")\n","all_labels = get_labels(\"ToS/Labels/\")"],"metadata":{"id":"mkp9MZKewxDN","executionInfo":{"status":"ok","timestamp":1657413602220,"user_tz":-120,"elapsed":10,"user":{"displayName":"Isaac Olguín","userId":"09697151780329788960"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["# Since unfair sentences are marked as \"-1\", we change them to \"0\" for simplicity. Zero means fair, One means unfair\n","all_labels =  [0 if label ==-1 else label for label in all_labels]"],"metadata":{"id":"bnor58FKwxy2","executionInfo":{"status":"ok","timestamp":1657413602221,"user_tz":-120,"elapsed":10,"user":{"displayName":"Isaac Olguín","userId":"09697151780329788960"}}},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":["### Bert Tokenizer"],"metadata":{"id":"jU5yamL5xKAY"}},{"cell_type":"code","source":["# Load the BERT tokenizer.\n","print('Loading BERT tokenizer...')\n","tokenizer = BertTokenizer.from_pretrained(MODEL_NAME, do_lower_case=True) # the model 'bert-base-uncased' only contains lower case sentences"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xMiwR7ldxLwC","executionInfo":{"status":"ok","timestamp":1657413602707,"user_tz":-120,"elapsed":495,"user":{"displayName":"Isaac Olguín","userId":"09697151780329788960"}},"outputId":"b5a9aa5b-1a82-4e5e-c26d-00cbf6066aeb"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Loading BERT tokenizer...\n"]}]},{"cell_type":"code","source":["# ==> Example of first sentence\n","\n","# Print the original sentence.\n","print(' Original: ', all_sentences[0])\n","\n","# Print the sentence split into tokens.\n","print('Tokenized: ', tokenizer.tokenize(all_sentences[0]))\n","\n","# Print the sentence mapped to token ids.\n","print('Token IDs: ', tokenizer.convert_tokens_to_ids(tokenizer.tokenize(all_sentences[0])))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vgT3sO20xPtj","executionInfo":{"status":"ok","timestamp":1657413602708,"user_tz":-120,"elapsed":25,"user":{"displayName":"Isaac Olguín","userId":"09697151780329788960"}},"outputId":"29b9fcf3-a9a9-45ad-b236-965bc84ff492"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":[" Original:  * accepting the terms of service \n","\n","Tokenized:  ['*', 'accept', '##ing', 'the', 'terms', 'of', 'service']\n","Token IDs:  [113, 1599, 235, 207, 333, 210, 446]\n"]}]},{"cell_type":"code","source":["# ==> Get the max length of a sentence\n","\n","max_len = 0\n","\n","# For every sentence...\n","for sent in all_sentences:\n","\n","    # Tokenize the text and add `[CLS]` and `[SEP]` tokens.\n","    input_ids = tokenizer.encode(sent, add_special_tokens=True)\n","\n","    # Update the maximum sentence length.\n","    max_len = max(max_len, len(input_ids))\n","\n","print('Max sentence length: ', max_len)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"c-uE6jBuxRjZ","executionInfo":{"status":"ok","timestamp":1657413615882,"user_tz":-120,"elapsed":13188,"user":{"displayName":"Isaac Olguín","userId":"09697151780329788960"}},"outputId":"c8161654-b4ee-4d57-bf54-873a098daee6"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stderr","text":["Token indices sequence length is longer than the specified maximum sequence length for this model (598 > 512). Running this sequence through the model will result in indexing errors\n"]},{"output_type":"stream","name":"stdout","text":["Max sentence length:  598\n"]}]},{"cell_type":"markdown","source":["### Model BertForSequenceClassification (Load model)"],"metadata":{"id":"JpohQx5xyqwh"}},{"cell_type":"code","source":["# Load BertForSequenceClassification, the pretrained BERT model with a single \n","# linear classification layer on top. \n","model = BertForSequenceClassification.from_pretrained(\n","    MODEL_NAME, # Use the 12-layer BERT model, with an uncased vocab.\n","    num_labels = NUM_CLASSES, # The number of output labels--2 for binary classification.\n","                    # You can increase this for multi-class tasks.   \n","    output_attentions = False, # Whether the model returns attentions weights.\n","    output_hidden_states = False, # Whether the model returns all hidden-states.\n",")\n","\n","# Tell pytorch to run this model on the GPU.\n","model.cuda()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tkpyAA69yuEO","executionInfo":{"status":"ok","timestamp":1657413620294,"user_tz":-120,"elapsed":4424,"user":{"displayName":"Isaac Olguín","userId":"09697151780329788960"}},"outputId":"ab864eda-419e-4d9e-9675-b406b98d40a4"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at nlpaueb/legal-bert-small-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight']\n","- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at nlpaueb/legal-bert-small-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"execute_result","data":{"text/plain":["BertForSequenceClassification(\n","  (bert): BertModel(\n","    (embeddings): BertEmbeddings(\n","      (word_embeddings): Embedding(30522, 512, padding_idx=0)\n","      (position_embeddings): Embedding(512, 512)\n","      (token_type_embeddings): Embedding(2, 512)\n","      (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (encoder): BertEncoder(\n","      (layer): ModuleList(\n","        (0): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=512, out_features=512, bias=True)\n","              (key): Linear(in_features=512, out_features=512, bias=True)\n","              (value): Linear(in_features=512, out_features=512, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=512, out_features=512, bias=True)\n","              (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=512, out_features=2048, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=2048, out_features=512, bias=True)\n","            (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (1): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=512, out_features=512, bias=True)\n","              (key): Linear(in_features=512, out_features=512, bias=True)\n","              (value): Linear(in_features=512, out_features=512, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=512, out_features=512, bias=True)\n","              (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=512, out_features=2048, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=2048, out_features=512, bias=True)\n","            (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (2): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=512, out_features=512, bias=True)\n","              (key): Linear(in_features=512, out_features=512, bias=True)\n","              (value): Linear(in_features=512, out_features=512, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=512, out_features=512, bias=True)\n","              (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=512, out_features=2048, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=2048, out_features=512, bias=True)\n","            (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (3): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=512, out_features=512, bias=True)\n","              (key): Linear(in_features=512, out_features=512, bias=True)\n","              (value): Linear(in_features=512, out_features=512, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=512, out_features=512, bias=True)\n","              (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=512, out_features=2048, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=2048, out_features=512, bias=True)\n","            (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (4): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=512, out_features=512, bias=True)\n","              (key): Linear(in_features=512, out_features=512, bias=True)\n","              (value): Linear(in_features=512, out_features=512, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=512, out_features=512, bias=True)\n","              (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=512, out_features=2048, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=2048, out_features=512, bias=True)\n","            (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (5): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=512, out_features=512, bias=True)\n","              (key): Linear(in_features=512, out_features=512, bias=True)\n","              (value): Linear(in_features=512, out_features=512, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=512, out_features=512, bias=True)\n","              (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=512, out_features=2048, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=2048, out_features=512, bias=True)\n","            (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","    (pooler): BertPooler(\n","      (dense): Linear(in_features=512, out_features=512, bias=True)\n","      (activation): Tanh()\n","    )\n","  )\n","  (dropout): Dropout(p=0.1, inplace=False)\n","  (classifier): Linear(in_features=512, out_features=2, bias=True)\n",")"]},"metadata":{},"execution_count":13}]},{"cell_type":"code","source":["# Load the model and dictionary\n","model.load_state_dict(torch.load('Bert4SeqClassif_202207072015.pt'))#, map_location=torch.device('cpu') or cuda. Both work\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Q-aGx3Q60e4w","executionInfo":{"status":"ok","timestamp":1657413620775,"user_tz":-120,"elapsed":484,"user":{"displayName":"Isaac Olguín","userId":"09697151780329788960"}},"outputId":"5c5b6adf-265d-49c1-fcc1-1a77e077ecbe"},"execution_count":14,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<All keys matched successfully>"]},"metadata":{},"execution_count":14}]},{"cell_type":"markdown","source":["### Trigger generation"],"metadata":{"id":"JIyze6jK2bpQ"}},{"cell_type":"markdown","source":["##### General functions"],"metadata":{"id":"mLTLH3AJ5-Lw"}},{"cell_type":"code","source":["# hook used in add_hooks()\n","extracted_grads = []\n","def extract_grad_hook(module, grad_in, grad_out):\n","    extracted_grads.append(grad_out[0])"],"metadata":{"id":"8JjcRhGE6hUc","executionInfo":{"status":"ok","timestamp":1657413620776,"user_tz":-120,"elapsed":49,"user":{"displayName":"Isaac Olguín","userId":"09697151780329788960"}}},"execution_count":15,"outputs":[]},{"cell_type":"code","source":["# returns the wordpiece embedding weight matrix\n","def get_embedding_weight(language_model):\n","    for module in language_model.modules():\n","        if isinstance(module, torch.nn.Embedding):\n","            if module.weight.shape[0] == 30522: # only add a hook to wordpiece embeddings, not position embeddings \n","                ##50257 is the size of the vocabulary of GPT\n","                return module.weight.detach()"],"metadata":{"id":"1MV3isar2dvF","executionInfo":{"status":"ok","timestamp":1657413620776,"user_tz":-120,"elapsed":47,"user":{"displayName":"Isaac Olguín","userId":"09697151780329788960"}}},"execution_count":16,"outputs":[]},{"cell_type":"code","source":["# add hooks for embeddings\n","def add_hooks(language_model):\n","    for module in language_model.modules():\n","        if isinstance(module, torch.nn.Embedding):\n","            if module.weight.shape[0] == 30522: # only add a hook to wordpiece embeddings, not position\n","                ##50257 is the size of the vocabulary of GPT\n","                module.weight.requires_grad = True\n","                #module.register_backward_hook(extract_grad_hook)\n","                module.register_full_backward_hook(extract_grad_hook)"],"metadata":{"id":"ymN2vLUT6Oe5","executionInfo":{"status":"ok","timestamp":1657413620776,"user_tz":-120,"elapsed":45,"user":{"displayName":"Isaac Olguín","userId":"09697151780329788960"}}},"execution_count":17,"outputs":[]},{"cell_type":"code","source":["# Gets the loss of the target_tokens using the triggers as the context\n","def get_loss(language_model, batch_size, trigger, target, device='cuda'):\n","    # context is trigger repeated batch size\n","    print(f'Arrive to get_loss\\n\\t batch_size {batch_size}\\n\\t trigger {trigger.shape}\\n\\t target {target.shape}')\n","    print(f'LANGUAGE_MODEL {language_model}')\n","    tensor_trigger = torch.tensor(trigger, device=device, dtype=torch.long).unsqueeze(0).repeat(batch_size, 1)\n","    print(f'tensor_trigger {tensor_trigger}')\n","    mask_out = -1 * torch.ones_like(tensor_trigger) # we zero out the loss for the trigger tokens\n","    print(f'mask_out {mask_out}')\n","    lm_input = torch.cat((tensor_trigger, target), dim=1) # we feed the model the trigger + target texts\n","    print(f'lm_input {lm_input.shape} == {lm_input}')\n","    print(f'lm_input[0] {lm_input[0]}')\n","    mask_and_target = torch.cat((mask_out, target), dim=1) # has -1's + target texts for loss computation\n","    print(f'mask_and_target {mask_and_target.shape} == {mask_and_target}')\n","    lm_input[lm_input == -1] = 1   # put random token of 1 at end of context (its masked out)\n","    print(f'lm_input {lm_input.shape} == {lm_input}')\n","    loss = language_model(lm_input, labels=mask_and_target)#[0]\n","    print(f'loss {loss}')\n","    return loss"],"metadata":{"id":"nZOz1INA6WeB","executionInfo":{"status":"ok","timestamp":1657413620777,"user_tz":-120,"elapsed":44,"user":{"displayName":"Isaac Olguín","userId":"09697151780329788960"}}},"execution_count":18,"outputs":[]},{"cell_type":"code","source":["# creates the batch of target texts with -1 placed at the end of the sequences for padding (for masking out the loss).\n","def make_target_batch(tokenizer, device, target_texts):\n","    # encode items and get the max length\n","    encoded_texts = []\n","    max_len = 0\n","    for target_text in target_texts:\n","        encoded_target_text = tokenizer.encode_plus(\n","            target_text,\n","            add_special_tokens = True,\n","            max_length = EMBEDDING_SIZE - NUM_TOKENS,\n","            pad_to_max_length = True,\n","            return_attention_mask = True\n","        )\n","        #print(f'ENCODED_TARGET_TEXT {type(input_ids)} == {encoded_target_text.keys()}') # ENCODED_TARGET_TEXT <class 'list'> == dict_keys(['input_ids', 'token_type_ids', 'attention_mask'])\n","        \"\"\"\n","        print(f'ENCODED_TARGET_TEXT {type(encoded_target_text)} == {encoded_target_text}')\n","        ENCODED_TARGET_TEXT <class 'list'> == [101, 218, 4527, 237, 366, 212, 1260, 207, 446, 115, 799, 2277, 212, 781, 216, 119, 102]\n","        ENCODED_TARGET_TEXT <class 'list'> == [101, 799, 356, 432, 145, 782, 438, 225, 3457, 13409, 115, 102]\n","        \"\"\"\n","        encoded_texts.append(encoded_target_text.input_ids)\n","        if len(encoded_target_text.input_ids) > max_len:\n","            max_len = len(encoded_target_text)\n","\n","    # pad tokens, i.e., append -1 to the end of the non-longest ones\n","    for indx, encoded_text in enumerate(encoded_texts):\n","        if len(encoded_text) < max_len:\n","            encoded_texts[indx].extend([-1] * (max_len - len(encoded_text)))\n","\n","    # convert to tensors and batch them up\n","    target_tokens_batch = None\n","    for encoded_text in encoded_texts:\n","        target_tokens = torch.tensor(encoded_text, device=device, dtype=torch.long).unsqueeze(0)\n","        if target_tokens_batch is None:\n","            target_tokens_batch = target_tokens\n","        else:\n","            target_tokens_batch = torch.cat((target_tokens, target_tokens_batch), dim=0)\n","    return target_tokens_batch"],"metadata":{"id":"73ZQsJW_6z3h","executionInfo":{"status":"ok","timestamp":1657413620777,"user_tz":-120,"elapsed":42,"user":{"displayName":"Isaac Olguín","userId":"09697151780329788960"}}},"execution_count":19,"outputs":[]},{"cell_type":"code","source":["model.eval()\n","model.to(device)\n","\n","add_hooks(model) # add gradient hooks to embeddings\n","embedding_weight = get_embedding_weight(model) # save the word embedding matrix"],"metadata":{"id":"Q9b_Vpns66cA","executionInfo":{"status":"ok","timestamp":1657413620777,"user_tz":-120,"elapsed":39,"user":{"displayName":"Isaac Olguín","userId":"09697151780329788960"}}},"execution_count":20,"outputs":[]},{"cell_type":"code","source":["print(f'embedding_weight {embedding_weight} \\n\\nwith shape {embedding_weight.shape}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"c_bgvnKs7Gt1","executionInfo":{"status":"ok","timestamp":1657413620778,"user_tz":-120,"elapsed":38,"user":{"displayName":"Isaac Olguín","userId":"09697151780329788960"}},"outputId":"45ec3c42-4527-467c-a53c-46cb82206748"},"execution_count":21,"outputs":[{"output_type":"stream","name":"stdout","text":["embedding_weight tensor([[ 0.0641, -0.0185, -0.0232,  ..., -0.0211,  0.0466, -0.0678],\n","        [-0.0175,  0.0522, -0.1289,  ..., -0.0658,  0.0291, -0.1561],\n","        [ 0.0128, -0.0119, -0.0850,  ..., -0.0592,  0.0799, -0.1387],\n","        ...,\n","        [ 0.0040,  0.0531, -0.0814,  ...,  0.0393,  0.0525, -0.0063],\n","        [-0.0143,  0.0036, -0.0973,  ..., -0.0562,  0.0196, -0.1135],\n","        [-0.0785, -0.0090, -0.1799,  ...,  0.0115,  0.0191, -0.0859]],\n","       device='cuda:0') \n","\n","with shape torch.Size([30522, 512])\n"]}]},{"cell_type":"code","source":["### Get positions of unfair sentences\n","\n","positions_unfair = np.where(np.array(all_labels) == 1)[0]\n","print(f'First 32 positions: {positions_unfair[0:32]} with total of unfair sentences {len(positions_unfair)}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Qkk7Eo0c7ZVT","executionInfo":{"status":"ok","timestamp":1657413620778,"user_tz":-120,"elapsed":27,"user":{"displayName":"Isaac Olguín","userId":"09697151780329788960"}},"outputId":"b00afb5b-926e-4b1a-9f08-e9a2fc0f23ff"},"execution_count":22,"outputs":[{"output_type":"stream","name":"stdout","text":["First 32 positions: [  4   9  10  11  12  13  24  25  43  45  61  62  78  79  87  89  91  92\n"," 100 104 109 111 143 151 154 157 169 195 206 258 260 266] with total of unfair sentences 1032\n"]}]},{"cell_type":"code","source":["target_sentences = []\n","for index in range(len(positions_unfair[0:32])):\n","    target_sentences.append(all_sentences[positions_unfair[index]])"],"metadata":{"id":"pu4d_ph8-PMp","executionInfo":{"status":"ok","timestamp":1657413620779,"user_tz":-120,"elapsed":22,"user":{"displayName":"Isaac Olguín","userId":"09697151780329788960"}}},"execution_count":23,"outputs":[]},{"cell_type":"code","source":["target_tokens = make_target_batch(tokenizer, device, target_sentences)\n","\n","#target_tokens.shape\n","\n","# sample random initial trigger\n","trigger_tokens = np.random.randint(VOCABULARY_SIZE, size=NUM_TOKENS)\n","print(tokenizer.decode(trigger_tokens))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0TN1TmoB_Asb","executionInfo":{"status":"ok","timestamp":1657413620780,"user_tz":-120,"elapsed":22,"user":{"displayName":"Isaac Olguín","userId":"09697151780329788960"}},"outputId":"ab4a23c6-51d0-448e-d9af-4d2ad5a39c48"},"execution_count":24,"outputs":[{"output_type":"stream","name":"stderr","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"]},{"output_type":"stream","name":"stdout","text":["##gesetzelioplicate\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2307: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"]}]},{"cell_type":"code","source":["#trigger_tokens #.shape (3,) => array([ 8972, 27350, 25382])\n","#target_tokens #.shape => torch.Size([32, 163])\n","\"\"\"\n","tensor([[  101, 12017,   179,  ...,    -1,    -1,    -1],\n","        [  101,   233,   223,  ...,    -1,    -1,    -1],\n","        [  101, 12017,   179,  ...,    -1,    -1,    -1],\n","        ...,\n","        [  101,   206,  4313,  ...,    -1,    -1,    -1],\n","        [  101,   206,  4313,  ...,    -1,    -1,    -1],\n","        [  101,   218,  1260,  ...,    -1,    -1,    -1]], device='cuda:0')\n","\"\"\""],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":87},"id":"ujMeErAoGyl8","executionInfo":{"status":"ok","timestamp":1657413620780,"user_tz":-120,"elapsed":16,"user":{"displayName":"Isaac Olguín","userId":"09697151780329788960"}},"outputId":"98b45699-4ec6-4c9d-b61e-f5ba64abce64"},"execution_count":25,"outputs":[{"output_type":"execute_result","data":{"text/plain":["\"\\ntensor([[  101, 12017,   179,  ...,    -1,    -1,    -1],\\n        [  101,   233,   223,  ...,    -1,    -1,    -1],\\n        [  101, 12017,   179,  ...,    -1,    -1,    -1],\\n        ...,\\n        [  101,   206,  4313,  ...,    -1,    -1,    -1],\\n        [  101,   206,  4313,  ...,    -1,    -1,    -1],\\n        [  101,   218,  1260,  ...,    -1,    -1,    -1]], device='cuda:0')\\n\""],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":25}]},{"cell_type":"code","source":["def get_input_masks_and_labels_with_tokens(sentences, labels, tokens):\n","    input_ids = []\n","    attention_masks = []\n","\n","    # For every sentence...\n","    for sent in all_sentences:\n","        # `encode_plus` will:\n","        #   (1) Tokenize the sentence.\n","        #   (2) Prepend the `[CLS]` token to the start.\n","        #   (3) Append the `[SEP]` token to the end.\n","        #   (4) Map tokens to their IDs.\n","        #   (5) Pad or truncate the sentence to `max_length`\n","        #   (6) Create attention masks for [PAD] tokens.\n","        sent_with_tokens = \" \".join(tokens) + \" \" + sent\n","\n","        encoded_dict = tokenizer.encode_plus(\n","                        sent,                      # Sentence to encode.\n","                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n","                        max_length = 512,          # Pad & truncate all sentences.\n","                        pad_to_max_length = True, #is deprecated\n","                        return_attention_mask = True,   # Construct attn. masks.\n","                        return_tensors = 'pt',     # Return pytorch tensors.\n","                   )\n","        \n","        # Add the encoded sentence to the list.    \n","        input_ids.append(encoded_dict['input_ids'])\n","\n","        # And its attention mask (simply differentiates padding from non-padding).\n","        attention_masks.append(encoded_dict['attention_mask'])\n","\n","    # Convert the lists into tensors.\n","    input_ids = torch.cat(input_ids, dim=0)\n","    attention_masks = torch.cat(attention_masks, dim=0)\n","    labels = torch.tensor(labels)\n","\n","    return input_ids, attention_masks, labels"],"metadata":{"id":"TskuEhy1ZyAh","executionInfo":{"status":"ok","timestamp":1657413620781,"user_tz":-120,"elapsed":15,"user":{"displayName":"Isaac Olguín","userId":"09697151780329788960"}}},"execution_count":26,"outputs":[]},{"cell_type":"code","source":["input_ids, attention_masks, labels = get_input_masks_and_labels_with_tokens(all_sentences, all_labels, tokenizer.decode(trigger_tokens))\n","\n","dataset = TensorDataset(input_ids, attention_masks, labels)\n","\n","dataloader = torch.utils.data.DataLoader(dataset, batch_size=BATCH_SIZE)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LuqGPIZ9bsM4","executionInfo":{"status":"ok","timestamp":1657413632266,"user_tz":-120,"elapsed":11275,"user":{"displayName":"Isaac Olguín","userId":"09697151780329788960"}},"outputId":"259f6e0a-2093-4632-9107-5668447f1cf8"},"execution_count":27,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2307: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"]}]},{"cell_type":"code","source":["extracted_grads = []\n","\n","# get initial loss for the trigger\n","model.zero_grad()\n","\n","test_preds = []\n","test_targets = []\n","\n","# Tracking variables \n","total_test_accuracy = 0\n","total_test_loss = 0\n","\n","for batch in dataloader:\n","    b_input_ids = batch[0].to(device)\n","    b_input_mask = batch[1].to(device)\n","    b_labels = batch[2].to(device)\n","\n","    model.zero_grad()\n","\n","    result = model(b_input_ids, \n","                token_type_ids=None, \n","                attention_mask=b_input_mask, \n","                labels=b_labels,\n","                return_dict=True)\n","\n","    # Get the loss and \"logits\" output by the model. The \"logits\" are the \n","    # output values prior to applying an activation function like the \n","    # softmax.\n","    loss = result.loss\n","    logits = result.logits\n","\n","    test_preds.extend(logits.argmax(dim=1).cpu().numpy())\n","    test_targets.extend(batch[2].numpy())\n","\n","    # Accumulate the validation loss.\n","    total_test_loss += loss.item()\n","\n","    # Move logits and labels to CPU\n","    logits = logits.detach().cpu().numpy()\n","    label_ids = b_labels.to('cpu').numpy()\n","\n","    \"\"\"\n","\n","    # Calculate the accuracy for this batch of test sentences, and\n","    # accumulate it over all batches.\n","    total_test_accuracy += flat_accuracy(logits, label_ids)\n","    \n","    valid_acc = accuracy_score(valid_targets, valid_preds)\n","    valid_precision = precision_score(valid_targets, valid_preds)\n","    valid_recall = recall_score(valid_targets, valid_preds)\n","    valid_f1 = f1_score(valid_targets, valid_preds)\n","\n","    io_total_valid_acc += valid_acc\n","    io_total_valid_prec += valid_precision\n","    io_total_valid_recall += valid_recall\n","    io_total_valid_f1 += valid_f1\n","\n","io_avg_valid_acc = io_total_valid_acc / len(validation_dataloader)\n","io_avg_valid_prec = io_total_valid_prec / len(validation_dataloader)\n","io_avg_valid_recall = io_total_valid_recall / len(validation_dataloader)\n","io_avg_valid_f1 = io_total_valid_f1 / len(validation_dataloader)\n","print(\n","        f'Epoch {epoch_i+1} : \\n\\\n","        Valid_acc : {io_avg_valid_acc}\\n\\\n","        Valid_F1 : {io_avg_valid_f1}\\n\\\n","        Valid_precision : {io_avg_valid_prec}\\n\\\n","        Valid_recall : {io_avg_valid_recall}'\n","      )\n","    \"\"\"\n","\n","print(f\"total_test_loss {total_test_loss}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cFhC5DN9ggz4","executionInfo":{"status":"ok","timestamp":1657413673647,"user_tz":-120,"elapsed":41393,"user":{"displayName":"Isaac Olguín","userId":"09697151780329788960"}},"outputId":"7c17bad7-94db-4a71-a009-c3ddcd0b8ad4"},"execution_count":28,"outputs":[{"output_type":"stream","name":"stdout","text":["total_test_loss 17.785486546345055\n"]}]},{"cell_type":"code","source":["\"\"\"\n","extracted_grads = []\n","\n","# get initial loss for the trigger\n","model.zero_grad()\n","loss = get_loss(model, BATCH_SIZE, trigger_tokens, target_tokens, device)\n","best_loss = loss\n","counter = 0\n","end_iter = False\n","\"\"\""],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":53},"id":"-nddZPP1B1RJ","executionInfo":{"status":"ok","timestamp":1657413673648,"user_tz":-120,"elapsed":36,"user":{"displayName":"Isaac Olguín","userId":"09697151780329788960"}},"outputId":"2d231db1-7a7b-40c5-cb07-68b943403386"},"execution_count":29,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'\\nextracted_grads = []\\n\\n# get initial loss for the trigger\\nmodel.zero_grad()\\nloss = get_loss(model, BATCH_SIZE, trigger_tokens, target_tokens, device)\\nbest_loss = loss\\ncounter = 0\\nend_iter = False\\n'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":29}]},{"cell_type":"code","source":["extracted_grads"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xDWChibPCYjs","executionInfo":{"status":"ok","timestamp":1657413914539,"user_tz":-120,"elapsed":253,"user":{"displayName":"Isaac Olguín","userId":"09697151780329788960"}},"outputId":"fe60e78d-7131-427d-d335-95f066fa6d3b"},"execution_count":31,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[]"]},"metadata":{},"execution_count":31}]},{"cell_type":"code","source":[""],"metadata":{"id":"3IWgPUFKkRy1"},"execution_count":null,"outputs":[]}]}