{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a7bb8088",
   "metadata": {},
   "source": [
    "# Trigger Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f7f5ed35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import heapq\n",
    "from copy import deepcopy\n",
    "from functools import partial\n",
    "from operator import itemgetter\n",
    "from typing import List, Tuple\n",
    "\n",
    "import numpy\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data.sampler import BatchSampler\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9cfedd98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding_weight_bert(model, model_type='bert'):\n",
    "    \"\"\"\n",
    "    Extracts and returns the token embedding weight matrix from the model.\n",
    "    \"\"\"\n",
    "    return model.bert.embeddings.word_embeddings.weight.cpu().detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ec6afacb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hook used in add_hooks()\n",
    "extracted_grads = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a707a376",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_hooks_bert(model, model_type='bert'):\n",
    "    \"\"\"\n",
    "    Finds the token embedding matrix on the model and registers a hook onto it.\n",
    "    When loss.backward() is called, extracted_grads list will be filled with\n",
    "    the gradients w.r.t. the token embeddings\n",
    "    \"\"\"\n",
    "    model.bert.embeddings.word_embeddings.weight.requires_grad = True\n",
    "    model.bert.embeddings.word_embeddings.register_backward_hook(extract_grad_hook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aaa64862",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mlm_probabilities(model: torch.nn.Module, batch: Tuple,\n",
    "                          mask_token_idx: int, trigger_token_len: int = 1):\n",
    "    input_ids = batch[0]\n",
    "    mask_tensor = torch.cuda.LongTensor([mask_token_idx] * trigger_token_len)\n",
    "    mask_tensor = mask_tensor.repeat(batch[0].shape[0], 1)\n",
    "    input_ids = torch.cat(\n",
    "        [input_ids[:, 0].unsqueeze(1), mask_tensor, input_ids[:, 1:]], 1)\n",
    "    logits = model(input_ids)[0]\n",
    "    logits = logits[:, 1:trigger_token_len + 1, :].sum(dim=0)\n",
    "    # Just return the trigger probabilities\n",
    "    return torch.nn.Softmax()(logits).cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a64cddea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_batch_bert(model: torch.nn.Module, batch: Tuple,\n",
    "                        trigger_token_ids: List = None, reduction='mean'):\n",
    "    # Attach attack_multiple_objectives if present\n",
    "    input_ids = batch[0]\n",
    "    loss_f = torch.nn.CrossEntropyLoss(reduction=reduction)\n",
    "    if trigger_token_ids is not None:\n",
    "        trig_tensor = torch.cuda.LongTensor(trigger_token_ids)\n",
    "        trig_tensor = trig_tensor.repeat(batch[0].shape[0], 1)\n",
    "        # CLS trigger tokens\n",
    "        input_ids = torch.cat(\n",
    "            [input_ids[:, 0].unsqueeze(1), trig_tensor, input_ids[:, 1:]], 1)\n",
    "\n",
    "    loss, logits_val = model(input_ids, attention_mask=input_ids > 1,\n",
    "                             labels=batch[1])\n",
    "    loss = loss_f(logits_val, batch[1].long())\n",
    "    labels = batch[1]\n",
    "\n",
    "    return loss, logits_val, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "65b490e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_batch_ppl(model: torch.nn.Module, batch: Tuple, tokenizer,\n",
    "                       trigger_token_ids: List = None):\n",
    "    # Attach attack_multiple_objectives if present\n",
    "    input_ids = batch[0]\n",
    "\n",
    "    if trigger_token_ids == None:\n",
    "        trigger_token_ids = []\n",
    "\n",
    "    input_ids_claim = []\n",
    "    for instance in input_ids:\n",
    "        instance = instance.detach().cpu().numpy().tolist()\n",
    "        # get the claim including SEP token, add triggger and append claim\n",
    "        # plus evidence\n",
    "        claim_tokens = instance[0:1] + trigger_token_ids + instance[1:]\n",
    "        claim_tokens = claim_tokens[:512]\n",
    "        input_ids_claim.append(claim_tokens)\n",
    "\n",
    "    # pad batch\n",
    "    max_len = min(512, max([len(i) for i in input_ids_claim]))\n",
    "    input_ids_claim = [\n",
    "        instance + [tokenizer.pad_token_id] * (max_len - len(instance))\n",
    "        for instance in input_ids_claim]\n",
    "    input_ids = torch.tensor(input_ids_claim).cuda()\n",
    "\n",
    "    outputs = model(input_ids, masked_lm_labels=input_ids)\n",
    "    loss, prediction_scores = outputs[:2]\n",
    "    return loss, prediction_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8c2a1960",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_batch_gpt(model: torch.nn.Module, batch: Tuple,\n",
    "                       trigger_token_ids: List = None, tokenizer=None):\n",
    "    # Attach attack_multiple_objectives if present\n",
    "    input_ids = batch[0]\n",
    "    loss_f = torch.nn.CrossEntropyLoss()\n",
    "    input_ids_claim = []\n",
    "\n",
    "    if trigger_token_ids == None:\n",
    "        trigger_token_ids = []\n",
    "\n",
    "    # index_sep = (input_ids == tokenizer.sep_token_id).nonzero()[:, 1]\n",
    "    for instance in input_ids:\n",
    "        instance = instance.detach().cpu().numpy().tolist()\n",
    "        # get the claim including SEP token, add triggger and append again\n",
    "        # claim but without the CLS token\n",
    "        # CLS, trigger, claim tokens\n",
    "        # sep_index = instance.index(tokenizer.sep_token_id)\n",
    "        claim_tokens = instance[0:1] + trigger_token_ids + instance[1:]\n",
    "        input_ids_claim.append(claim_tokens[:512])\n",
    "        # pad batch\n",
    "        max_len = min(max([len(i) for i in input_ids_claim]), 512)\n",
    "        input_ids_claim = [\n",
    "            instance + [tokenizer.pad_token_id] * (max_len - len(instance))\n",
    "            for instance in input_ids_claim]\n",
    "        input_ids = torch.tensor(input_ids_claim).cuda()\n",
    "\n",
    "    # eval is w.r.t. entailment - this is the target class in the NLI case,\n",
    "    # i.e. the one we want to minimize the loss for.\n",
    "    logits_val = \\\n",
    "    model(input_ids, attention_mask=input_ids != tokenizer.pad_token_id)[0]\n",
    "    loss = loss_f(logits_val, batch[1].long())\n",
    "\n",
    "    return loss, logits_val, batch[1].long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cc827f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_batch_nli(model: torch.nn.Module, batch: Tuple,\n",
    "                       trigger_token_ids: List = None, tokenizer=None):\n",
    "    # Attach attack_multiple_objectives if present\n",
    "    input_ids = batch[0]\n",
    "    loss_f = torch.nn.MSELoss()\n",
    "    input_ids_claim = []\n",
    "\n",
    "    if trigger_token_ids == None:\n",
    "        trigger_token_ids = []\n",
    "\n",
    "    # index_sep = (input_ids == tokenizer.sep_token_id).nonzero()[:, 1]\n",
    "    for instance in input_ids:\n",
    "        instance = instance.detach().cpu().numpy().tolist()\n",
    "        sep_index = instance.index(tokenizer.sep_token_id)\n",
    "        # get the claim including SEP token, add triggger and append again\n",
    "        # claim but without the CLS token\n",
    "        claim_tokens = instance[0:1] + trigger_token_ids + instance[\n",
    "                                                           1:sep_index + 1] +\\\n",
    "                       instance[\n",
    "                                                                              1:sep_index + 1]\n",
    "        input_ids_claim.append(claim_tokens[:512])\n",
    "        # pad batch\n",
    "        max_len = max([len(i) for i in input_ids_claim])\n",
    "        input_ids_claim = [\n",
    "            instance + [tokenizer.pad_token_id] * (max_len - len(instance))\n",
    "            for instance in input_ids_claim]\n",
    "        input_ids = torch.tensor(input_ids_claim).cuda()\n",
    "\n",
    "    # eval w.r.t. entailment - this is the target class in the NLI case,\n",
    "    # i.e. the one we want to minimize the loss for.\n",
    "    logits_val = \\\n",
    "    model(input_ids, attention_mask=input_ids != tokenizer.pad_token_id)[0]\n",
    "    loss = loss_f(logits_val, batch[1].float().unsqueeze(1))\n",
    "\n",
    "    return loss, logits_val, batch[1].long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8423a8cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_average_grad_transformer(model, batch, trigger_token_ids, batch_func,\n",
    "                                 target_label=None, ):\n",
    "    \"\"\"\n",
    "    Computes the average gradient w.r.t. the trigger tokens when prepended to\n",
    "    every example\n",
    "    in the batch. If target_label is set, that is used as the ground-truth\n",
    "    label.\n",
    "    \"\"\"\n",
    "    # create an dummy optimizer for backprop\n",
    "    optimizer = optim.Adam(model.parameters())\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # prepend attack_multiple_objectives to the batch\n",
    "    original_labels = batch[1].clone()\n",
    "    if target_label is not None:\n",
    "        # set the labels equal to the target (backprop from the target class,\n",
    "        # not model prediction)\n",
    "        batch[1] = int(target_label) * torch.ones_like(batch[1]).cuda()\n",
    "    global extracted_grads\n",
    "    extracted_grads = []  # clear existing stored grads\n",
    "    loss, logits, labels = batch_func(model, batch, trigger_token_ids)\n",
    "    loss.backward()\n",
    "    # index 0 has the hypothesis grads for SNLI. For SST, the list is of size 1.\n",
    "    grads = extracted_grads[0].detach().cpu()\n",
    "    batch[1] = original_labels.detach()  # reset labels\n",
    "\n",
    "    # average grad across batch size, result only makes sense for trigger\n",
    "    # tokens at the front\n",
    "    averaged_grad = torch.sum(grads, dim=0)\n",
    "    averaged_grad = F.normalize(averaged_grad, p=2, dim=1)\n",
    "    # start from position 1 as at 0 is the CLS token\n",
    "    averaged_grad = averaged_grad[\n",
    "                    1:len(trigger_token_ids) + 1]  # return just trigger grads\n",
    "    return averaged_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "84ea3e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loss_per_candidate(model, model_nli, gpt_model, batch,\n",
    "                           trigger_token_ids, cand_trigger_token_ids, tokenizer,\n",
    "                           nli_w=0.0, fc_w=1.0, ppl_w=0.0, idx=0):\n",
    "    nli_batch_func = partial(evaluate_batch_nli, tokenizer=tokenizer)\n",
    "    gpt_batch_func = partial(evaluate_batch_gpt, tokenizer=tokenizer)\n",
    "\n",
    "    original_labels = batch[1].clone()\n",
    "    loss_per_candidate = get_loss_per_candidate_bert(idx, model, batch,\n",
    "                                                     trigger_token_ids,\n",
    "                                                     cand_trigger_token_ids,\n",
    "                                                     evaluate_batch_bert,\n",
    "                                                     tokenizer)  # uses the\n",
    "    # real labels\n",
    "    loss_per_candidate = [(_t, _s * fc_w) for _t, _s in loss_per_candidate]\n",
    "\n",
    "    if nli_w > 0.0:\n",
    "        batch[1] = 0 * torch.ones_like(batch[1]).cuda()\n",
    "        nli_loss_per_candidate = get_loss_per_candidate_bert(idx, model_nli,\n",
    "                                                             batch,\n",
    "                                                             trigger_token_ids,\n",
    "                                                             cand_trigger_token_ids,\n",
    "                                                             nli_batch_func,\n",
    "                                                             tokenizer)\n",
    "        loss_per_candidate = [(_t, _s + nli_loss_per_candidate[i][1] * nli_w)\n",
    "                              for i, (_t, _s) in enumerate(loss_per_candidate)]\n",
    "        batch[1] = original_labels.detach()\n",
    "    if ppl_w > 0.0:\n",
    "        batch[1] = 0 * torch.ones_like(batch[1]).cuda()\n",
    "        gpt_loss_per_candidate = get_loss_per_candidate_bert(idx, gpt_model,\n",
    "                                                             batch,\n",
    "                                                             trigger_token_ids,\n",
    "                                                             cand_trigger_token_ids,\n",
    "                                                             gpt_batch_func,\n",
    "                                                             tokenizer)\n",
    "        loss_per_candidate = [(_t, _s + gpt_loss_per_candidate[i][1] * ppl_w)\n",
    "                              for i, (_t, _s) in enumerate(loss_per_candidate)]\n",
    "        batch[1] = original_labels.detach()\n",
    "\n",
    "    return loss_per_candidate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2644b1bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_candidates_all_obj(model, model_nli, gpt_model, batch,\n",
    "                                trigger_token_ids, cand_trigger_token_ids,\n",
    "                                tokenizer, beam_size=1, nli_w=0.0, fc_w=1.0,\n",
    "                                ppl_w=0.0):\n",
    "    loss_per_candidate = get_loss_per_candidate(model, model_nli, gpt_model,\n",
    "                                                batch,\n",
    "                                                trigger_token_ids,\n",
    "                                                cand_trigger_token_ids,\n",
    "                                                tokenizer,\n",
    "                                                nli_w=nli_w, fc_w=fc_w,\n",
    "                                                ppl_w=ppl_w, idx=0)\n",
    "    # maximize the loss\n",
    "    top_candidates = heapq.nlargest(beam_size, loss_per_candidate,\n",
    "                                    key=itemgetter(1))\n",
    "\n",
    "    # top_candidates now contains beam_size trigger sequences, each with a\n",
    "    # different 0th token\n",
    "    for idx in range(1, len(\n",
    "            trigger_token_ids)):  # for all trigger tokens, skipping the 0th\n",
    "        # (we did it above)\n",
    "        loss_per_candidate = []\n",
    "        for cand, _ in top_candidates:  # for all the beams, try all the\n",
    "            # candidates at idx\n",
    "            loss_ = get_loss_per_candidate(model, model_nli, gpt_model, batch,\n",
    "                                           cand, cand_trigger_token_ids,\n",
    "                                           tokenizer,\n",
    "                                           nli_w=nli_w, fc_w=fc_w, ppl_w=ppl_w,\n",
    "                                           idx=idx)\n",
    "\n",
    "            loss_per_candidate.extend(loss_)\n",
    "        top_candidates = heapq.nlargest(beam_size, loss_per_candidate,\n",
    "                                        key=itemgetter(1))\n",
    "    return sorted(top_candidates, key=itemgetter(1), reverse=True)[:beam_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f140069d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_candidates_bert(model, batch, trigger_token_ids,\n",
    "                             cand_trigger_token_ids, tokenizer, beam_size=1):\n",
    "    \"\"\"\"\n",
    "    Given the list of candidate trigger token ids (of number of trigger words\n",
    "    by number of candidates\n",
    "    per word), it finds the best new candidate trigger.\n",
    "    This performs beam search in a left to right fashion.\n",
    "    \"\"\"\n",
    "    # first round, no beams, just get the loss for each of the candidates in\n",
    "    # index 0.\n",
    "    # (indices 1-end are just the old trigger)\n",
    "    loss_per_candidate = get_loss_per_candidate_bert(0, model, batch,\n",
    "                                                     trigger_token_ids,\n",
    "                                                     cand_trigger_token_ids,\n",
    "                                                     evaluate_batch_bert,\n",
    "                                                     tokenizer)\n",
    "    # maximize the loss\n",
    "    top_candidates = heapq.nlargest(beam_size, loss_per_candidate,\n",
    "                                    key=itemgetter(1))\n",
    "\n",
    "    # top_candidates now contains beam_size trigger sequences, each with a\n",
    "    # different 0th token\n",
    "    for idx in range(1, len(\n",
    "            trigger_token_ids)):  # for all trigger tokens, skipping the 0th\n",
    "        # (we did it above)\n",
    "        loss_per_candidate = []\n",
    "        for cand, _ in top_candidates:  # for all the beams, try all the\n",
    "            # candidates at idx\n",
    "            loss_per_candidate.extend(\n",
    "                get_loss_per_candidate_bert(idx, model, batch, cand,\n",
    "                                            cand_trigger_token_ids,\n",
    "                                            evaluate_batch_bert, tokenizer))\n",
    "        top_candidates = heapq.nlargest(beam_size, loss_per_candidate,\n",
    "                                        key=itemgetter(1))\n",
    "    return max(top_candidates, key=itemgetter(1))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "15a71f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loss_per_candidate_bert(index, model, batch, trigger_token_ids,\n",
    "                                cand_trigger_token_ids, eval_batch_f,\n",
    "                                tokenizer):\n",
    "    \"\"\"\n",
    "    For a particular index, the function tries all of the candidate tokens\n",
    "    for that index.\n",
    "    The function returns a list containing the candidate\n",
    "    attack_multiple_objectives it tried, along with their loss.\n",
    "    \"\"\"\n",
    "    if isinstance(cand_trigger_token_ids[0], (numpy.int64, int)):\n",
    "        print(\"Only 1 candidate for index detected, not searching\")\n",
    "        return trigger_token_ids\n",
    "    loss_per_candidate = []\n",
    "    # loss for the trigger without trying the candidates\n",
    "    curr_loss, logits, labels = eval_batch_f(model, batch, trigger_token_ids)\n",
    "    curr_loss = curr_loss.cpu().detach().numpy()\n",
    "\n",
    "    loss_per_candidate.append((deepcopy(trigger_token_ids), curr_loss))\n",
    "    for cand_id in range(len(cand_trigger_token_ids[0])):\n",
    "        token = tokenizer.convert_ids_to_tokens(\n",
    "            [cand_trigger_token_ids[index][cand_id]])\n",
    "        trigger_token_ids_one_replaced = deepcopy(\n",
    "            trigger_token_ids)  # copy trigger\n",
    "        trigger_token_ids_one_replaced[index] = cand_trigger_token_ids[index][\n",
    "            cand_id]  # replace one token\n",
    "        if not any(_s.isalpha() for _s in token):\n",
    "            loss = -100.0\n",
    "        elif not (token[0].startswith('Ä ') or token[0][0].isupper()):\n",
    "            loss = -100.0\n",
    "        else:\n",
    "            loss, logits, labels = eval_batch_f(model, batch,\n",
    "                                                trigger_token_ids_one_replaced)\n",
    "            loss = loss.cpu().detach().numpy()\n",
    "        loss_per_candidate.append(\n",
    "            (deepcopy(trigger_token_ids_one_replaced), loss))\n",
    "    return loss_per_candidate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "00453d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_fc(model: torch.nn.Module, test_dl: BatchSampler,\n",
    "            trigger_token_ids: List = None, labels_num=3):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        labels_all = []\n",
    "        logits_all = []\n",
    "        for batch in tqdm(test_dl, desc=\"Evaluation\"):\n",
    "            # Attach triggers if present\n",
    "            loss, logits_val, labels = evaluate_batch_bert(model, batch,\n",
    "                                                           trigger_token_ids)\n",
    "\n",
    "            labels_all += labels.detach().cpu().numpy().tolist()\n",
    "            logits_all += logits_val.detach().cpu().numpy().tolist()\n",
    "\n",
    "        prediction = numpy.argmax(\n",
    "            numpy.asarray(logits_all).reshape(-1, labels_num), axis=-1)\n",
    "        acc = sum(prediction == labels_all) / len(labels_all)\n",
    "\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "49e2229f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_ppl(model: torch.nn.Module, test_dl: BatchSampler, tokenizer,\n",
    "             trigger_token_ids: List = None):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        ppl_loss = []\n",
    "        for batch in tqdm(test_dl, desc=\"Evaluation\"):\n",
    "            # Attach triggers if present\n",
    "            loss, prediction_scores = evaluate_batch_ppl(model, batch,\n",
    "                                                         tokenizer,\n",
    "                                                         trigger_token_ids)\n",
    "            loss = torch.exp(loss)\n",
    "            ppl_loss.append(loss.item() / batch[0].shape[0])\n",
    "    return numpy.mean(ppl_loss, dtype=float), numpy.std(ppl_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5b62c3e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_nli(model: torch.nn.Module, test_dl: BatchSampler, tokenizer,\n",
    "             trigger_token_ids: List = None, labels_num=3):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        logits_all = []\n",
    "        for batch in tqdm(test_dl, desc=\"Evaluation\"):\n",
    "            loss, logits_val, _ = evaluate_batch_nli(model, batch,\n",
    "                                                     trigger_token_ids,\n",
    "                                                     tokenizer)\n",
    "            logits_all += logits_val.detach().squeeze().cpu().numpy().tolist()\n",
    "    return logits_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f66ecdaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perplexity(logits, targets):\n",
    "    \"\"\"\n",
    "    Calculates the perplexity of a sentence based on the\n",
    "    output of a language model\n",
    "    :param logits: The language model output\n",
    "    :param targets: The expected output tokens\n",
    "    :return: The perplexity score (float)\n",
    "    \"\"\"\n",
    "    loss_fn = torch.nn.CrossEntropyLoss()\n",
    "    loss = loss_fn(logits, targets)\n",
    "    return np.exp(loss.cpu().item())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9102dc3",
   "metadata": {},
   "source": [
    "# Attacks\n",
    "\n",
    "\"\"\"\n",
    "Implementation based on https://github.com/Eric-Wallace/universal-triggers\n",
    "Contains different methods for attacking models. In particular, given the\n",
    "gradients for token embeddings, it computes the optimal token replacements.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "daa93cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hotflip_attack(averaged_grad, embedding_matrix, trigger_token_ids,\n",
    "                   increase_loss=False, num_candidates=1):\n",
    "    \"\"\"\n",
    "    The \"Hotflip\" attack described in Equation (2) of the\n",
    "    Universal Adversarial Attacks paper. This code is\n",
    "    heavily inspired by\n",
    "    the nice code of Paul Michel here\n",
    "    https://github.com/pmichel31415/translate/blob/paul/\n",
    "    pytorch_translate/research/adversarial/adversaries/brute_force_adversary.py\n",
    "\n",
    "    This function takes in the model's average_grad over a batch of examples,\n",
    "    the model's\n",
    "    token embedding matrix, and the current trigger token IDs. It returns the\n",
    "    top token\n",
    "    candidates for each position.\n",
    "\n",
    "    If increase_loss=True, then the attack reverses the sign of the gradient\n",
    "    and tries to increase\n",
    "    the loss (decrease the model's probability of the true class). For\n",
    "    targeted attacks, you want\n",
    "    to decrease the loss of the target class (increase_loss=False).\n",
    "    \"\"\"\n",
    "    averaged_grad = averaged_grad.cpu()\n",
    "    embedding_matrix = embedding_matrix.cpu()\n",
    "    trigger_token_embeds = torch.nn.functional.embedding(\n",
    "        torch.LongTensor(trigger_token_ids),\n",
    "        embedding_matrix).detach().unsqueeze(0)\n",
    "    averaged_grad = averaged_grad.unsqueeze(0)\n",
    "    gradient_dot_embedding_matrix = torch.einsum(\"bij,kj->bik\",\n",
    "                                                 (averaged_grad,\n",
    "                                                  embedding_matrix))\n",
    "    if not increase_loss:\n",
    "        gradient_dot_embedding_matrix *= -1  # lower versus increase the\n",
    "        # class probability.\n",
    "    if num_candidates > 1:  # get top k options\n",
    "        _, best_k_ids = torch.topk(gradient_dot_embedding_matrix,\n",
    "                                   num_candidates, dim=2)\n",
    "        return best_k_ids.detach().cpu().numpy()[0]\n",
    "    _, best_at_each_step = gradient_dot_embedding_matrix.max(2)\n",
    "    return best_at_each_step[0].detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dd50af18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pairwise_dot_product(src_embeds, vocab_embeds, cosine=False):\n",
    "    \"\"\"Compute the cosine similarity between each word in the vocab and each\n",
    "    word in the source\n",
    "    If `cosine=True` this returns the pairwise cosine similarity\"\"\"\n",
    "    # Normlize vectors for the cosine similarity\n",
    "    if cosine:\n",
    "        src_embeds = F.normalize(src_embeds, dim=-1, p=2)\n",
    "        vocab_embeds = F.normalize(vocab_embeds, dim=-1, p=2)\n",
    "    # Take the dot product\n",
    "    dot_product = torch.einsum(\"bij,kj->bik\", (src_embeds, vocab_embeds))\n",
    "    return dot_product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8124513d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pairwise_distance(src_embeds, vocab_embeds, squared=False):\n",
    "    \"\"\"Compute the euclidean distance between each word in the vocab and each\n",
    "    word in the source\"\"\"\n",
    "    # We will compute the squared norm first to avoid having to compute all\n",
    "    # the directions (which would have space complexity B x T x |V| x d)\n",
    "    # First compute the squared norm of each word vector\n",
    "    vocab_sq_norm = vocab_embeds.norm(p=2, dim=-1) ** 2\n",
    "    src_sq_norm = src_embeds.norm(p=2, dim=-1) ** 2\n",
    "    # Take the dot product\n",
    "    dot_product = pairwise_dot_product(src_embeds, vocab_embeds)\n",
    "    # Reshape for broadcasting\n",
    "    # 1 x 1 x |V|\n",
    "    vocab_sq_norm = vocab_sq_norm.unsqueeze(0).unsqueeze(0)\n",
    "    # B x T x 1\n",
    "    src_sq_norm = src_sq_norm.unsqueeze(2)\n",
    "    # Compute squared difference\n",
    "    sq_norm = vocab_sq_norm + src_sq_norm - 2 * dot_product\n",
    "    # Either return the squared norm or return the sqrt\n",
    "    if squared:\n",
    "        return sq_norm\n",
    "    else:\n",
    "        # Relu + epsilon for numerical stability\n",
    "        sq_norm = F.relu(sq_norm) + 1e-20\n",
    "        # Take the square root\n",
    "        return sq_norm.sqrt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7624183f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tailor_simple(averaged_grad, embedding_matrix, increase_loss=False):\n",
    "    \"\"\"\n",
    "    Tailor approximation simplified by computing just the largest gradient\n",
    "    w.r.t. the target label.\n",
    "    \"\"\"\n",
    "    averaged_grad = averaged_grad.cpu()\n",
    "    embedding_matrix = embedding_matrix.cpu()\n",
    "    averaged_grad = averaged_grad.unsqueeze(0)\n",
    "    gradient_dot_embedding_matrix = torch.einsum(\"bij,kj->bik\",\n",
    "                                                 (averaged_grad,\n",
    "                                                  embedding_matrix))\n",
    "    if not increase_loss:\n",
    "        gradient_dot_embedding_matrix *= -1  # lower versus increase the\n",
    "        # class probability.\n",
    "\n",
    "    gradient_dot_embedding_matrix = F.normalize(gradient_dot_embedding_matrix,\n",
    "                                                p=2, dim=1)\n",
    "    return gradient_dot_embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0ff85424",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tailor_first(averaged_grad, embedding_matrix, trigger_token_ids,\n",
    "                 reverse_loss=False, normalize=False):\n",
    "    \"\"\"\n",
    "    Tailor approximation of the larget gradient compared to the gradient of\n",
    "    the current token,\n",
    "    all w.r.t. the target class.\n",
    "    \"\"\"\n",
    "    averaged_grad = averaged_grad.cpu()\n",
    "    embedding_matrix = embedding_matrix.cpu()\n",
    "    trigger_token_embeds = torch.nn.functional.embedding(\n",
    "        torch.LongTensor(trigger_token_ids),\n",
    "        embedding_matrix).detach().unsqueeze(0)\n",
    "    averaged_grad = averaged_grad.unsqueeze(0)\n",
    "    new_embed_dot_grad = torch.einsum(\"bij,kj->bik\",\n",
    "                                      (averaged_grad, embedding_matrix))\n",
    "    prev_embed_dot_grad = torch.einsum(\"bij,bij->bi\",\n",
    "                                       (averaged_grad, trigger_token_embeds))\n",
    "\n",
    "    if reverse_loss:\n",
    "        neg_dir_dot_grad = prev_embed_dot_grad.unsqueeze(\n",
    "            -1) + new_embed_dot_grad\n",
    "    else:\n",
    "        neg_dir_dot_grad = prev_embed_dot_grad.unsqueeze(\n",
    "            -1) - new_embed_dot_grad\n",
    "\n",
    "    if normalize:\n",
    "        # Compute the direction norm (= distance word/substitution)\n",
    "        direction_norm = pairwise_distance(trigger_token_embeds,\n",
    "                                           embedding_matrix)\n",
    "        # Renormalize\n",
    "        neg_dir_dot_grad /= direction_norm\n",
    "\n",
    "    return neg_dir_dot_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3daf812f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hotflip_attack_all(averaged_grad, embedding_matrix,\n",
    "                       averaged_grad_nli, embedding_matrix_nli,\n",
    "                       averaged_grad_ppl, embedding_matrix_ppl,\n",
    "                       nli_w=0.0, fc_w=1, ppl_w=0.0,\n",
    "                       num_candidates=1):\n",
    "    \"\"\"Optimise the adversarial attacks for all objectives\n",
    "    that have a weight > 0. This is described in the paper in Equation 2.\n",
    "    \"\"\"\n",
    "    neg_dir_dot_grad = fc_w * tailor_simple(averaged_grad, embedding_matrix,\n",
    "                                            increase_loss=False)\n",
    "    if nli_w != 0:\n",
    "        neg_dir_dot_grad_nli = tailor_simple(averaged_grad_nli,\n",
    "                                             embedding_matrix_nli,\n",
    "                                             increase_loss=False)  # decrease\n",
    "        # loss for entailment\n",
    "        neg_dir_dot_grad += nli_w * neg_dir_dot_grad_nli\n",
    "    if ppl_w != 0:\n",
    "        neg_dir_dot_grad_ppl = tailor_simple(averaged_grad_ppl,\n",
    "                                             embedding_matrix_ppl,\n",
    "                                             increase_loss=False)  # decrease\n",
    "        # loss real example\n",
    "        neg_dir_dot_grad += ppl_w * neg_dir_dot_grad_ppl\n",
    "\n",
    "    if num_candidates > 1:  # get top k options\n",
    "        _, best_k_ids = torch.topk(neg_dir_dot_grad, num_candidates, dim=2)\n",
    "        return best_k_ids.detach().cpu().numpy()[0]\n",
    "    _, best_at_each_step = neg_dir_dot_grad.max(2)\n",
    "    return best_at_each_step[0].detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d8d067e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_attack(embedding_matrix, trigger_token_ids, num_candidates=1):\n",
    "    \"\"\"\n",
    "    Randomly search over the vocabulary. Gets num_candidates random samples\n",
    "    and returns all of them.\n",
    "    \"\"\"\n",
    "    embedding_matrix = embedding_matrix.cpu()\n",
    "    new_trigger_token_ids = [[None] * num_candidates for _ in\n",
    "                             range(len(trigger_token_ids))]\n",
    "    for trigger_token_id in range(len(trigger_token_ids)):\n",
    "        for candidate_number in range(num_candidates):\n",
    "            # rand token in the embedding matrix\n",
    "            rand_token = numpy.random.randint(embedding_matrix.shape[0])\n",
    "            new_trigger_token_ids[trigger_token_id][\n",
    "                candidate_number] = rand_token\n",
    "    return new_trigger_token_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "501ef431",
   "metadata": {},
   "outputs": [],
   "source": [
    "# steps in the direction of grad and gets the nearest neighbor vector.\n",
    "def nearest_neighbor_grad(averaged_grad, embedding_matrix, trigger_token_ids,\n",
    "                          tree, step_size, increase_loss=False,\n",
    "                          num_candidates=1):\n",
    "    \"\"\"\n",
    "    Takes a small step in the direction of the averaged_grad and finds the\n",
    "    nearest\n",
    "    vector in the embedding matrix using a kd-tree.\n",
    "    \"\"\"\n",
    "    new_trigger_token_ids = [[None] * num_candidates for _ in\n",
    "                             range(len(trigger_token_ids))]\n",
    "    averaged_grad = averaged_grad.cpu()\n",
    "    embedding_matrix = embedding_matrix.cpu()\n",
    "    if increase_loss:  # reverse the sign\n",
    "        step_size *= -1\n",
    "    for token_pos, trigger_token_id in enumerate(trigger_token_ids):\n",
    "        # take a step in the direction of the gradient\n",
    "        trigger_token_embed = \\\n",
    "        torch.nn.functional.embedding(torch.LongTensor([trigger_token_id]),\n",
    "                                      embedding_matrix).detach().cpu().numpy()[\n",
    "            0]\n",
    "        stepped_trigger_token_embed = trigger_token_embed + \\\n",
    "                                      averaged_grad[\n",
    "                                          token_pos].detach().cpu().numpy() * step_size\n",
    "        # look in the k-d tree for the nearest embedding\n",
    "        _, neighbors = tree.query([stepped_trigger_token_embed],\n",
    "                                  k=num_candidates)\n",
    "        for candidate_number, neighbor in enumerate(neighbors[0]):\n",
    "            new_trigger_token_ids[token_pos][candidate_number] = neighbor\n",
    "    return new_trigger_token_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49a42979",
   "metadata": {},
   "source": [
    "# Trigger generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "eb790765",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from collections import defaultdict\n",
    "from functools import partial\n",
    "import gc\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, BertConfig, BertForSequenceClassification, BertTokenizer, PreTrainedTokenizer\n",
    "from typing import List, Dict, AnyStr, Set\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b9b603e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "BERT_VOCAB_SIZE = 30522\n",
    "EMBEDDING_SIZE = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d2691c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fc_model(model_path, tokenizer, labels=2, device='cpu', typeModel='bert'):\n",
    "    collate_fn = None #partial(collate_fever, tokenizer=tokenizer, device=device)\n",
    "\n",
    "    transformer_config = BertConfig.from_pretrained('bert-base-uncased', num_labels=labels)\n",
    "    model = BertForSequenceClassification.from_pretrained('bert-base-uncased', config=transformer_config).to(device)\n",
    "\n",
    "    checkpoint = torch.load(model_path)\n",
    "    model.load_state_dict(checkpoint['model'])\n",
    "    model.train()  # rnn cannot do backwards in train mode\n",
    "\n",
    "    # Adds a hook to get the embedding gradients\n",
    "    add_hooks_bert(model, typeModel)\n",
    "    embedding_weight = get_embedding_weight_bert(model, typeModel)\n",
    "\n",
    "    return model, embedding_weight, collate_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a78a80fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_checkpoint_transformer(model_name: str, device: str,\n",
    "                               hook_embeddings: bool = False,\n",
    "                               model_type: str = 'bert'):\n",
    "    \"\"\"\n",
    "    'roberta-large-openai-detector' output: (tensor([[-0.1055, -0.6401]],\n",
    "    grad_fn=<AddmmBackward>),)\n",
    "    Real-1, Fake-0\n",
    "    Note from the authors: 'The results start to get reliable after around 50\n",
    "    tokens.'\n",
    "\n",
    "    \"SparkBeyond/roberta-large-sts-b\" model output: (tensor([[0.6732]],\n",
    "    grad_fn=<AddmmBackward>),)\n",
    "    STS-B benchmark measure the relatedness of two sentences based on the\n",
    "    cosine similarity of the two representations\n",
    "\n",
    "    roberta-large-mnli output: (tensor([[-1.8364,  1.4850,  0.7020]],\n",
    "    grad_fn=<AddmmBackward>),)\n",
    "    contradiction-0, neutral-1, entailment-2\n",
    "    \"\"\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_name).to(\n",
    "        device)\n",
    "    collate = partial(collate_nli_tok_ids, tokenizer=tokenizer, device=device)\n",
    "\n",
    "    model_ew = None\n",
    "    if hook_embeddings:\n",
    "        # this allows to get the value of the gradient each time we make a\n",
    "        # feed-forward pass\n",
    "        model_ew = get_embedding_weight_bert(model, model_type)\n",
    "        add_hooks_bert(model, model_type)\n",
    "\n",
    "    return model, model_ew, collate, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9c0c7069",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_nli_tok_ids(instances: List[List],\n",
    "                        tokenizer: PreTrainedTokenizer,\n",
    "                        device='cuda') -> List[torch.Tensor]:\n",
    "    batch_max_len = max([len(_s) for _s in instances])\n",
    "\n",
    "    padded_ids_tensor = torch.tensor(\n",
    "        [_s + [tokenizer.pad_token_id] * (batch_max_len - len(_s)) for _s in\n",
    "         instances])\n",
    "\n",
    "    output_tensors = [padded_ids_tensor, padded_ids_tensor > 0]\n",
    "\n",
    "    return list(_t.to(device) for _t in output_tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "bf5122bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fever(instances: List[Dict],\n",
    "                  tokenizer: PreTrainedTokenizer,\n",
    "                  device='cuda') -> List[torch.Tensor]:\n",
    "    token_ids = [tokenizer.encode(_x['claim'], evidence_text(_x)) for _x in\n",
    "                 instances]\n",
    "    # the length limit with the encode method does not work with the roberta\n",
    "    # tokenizer\n",
    "    batch_max_len = min(512, max([len(_s) for _s in token_ids]))\n",
    "\n",
    "    padded_ids_tensor = torch.tensor(\n",
    "        [_s[:batch_max_len] + [tokenizer.pad_token_id] * (\n",
    "                batch_max_len - min(512, len(_s)))\n",
    "         for _s in token_ids]).to(device)\n",
    "\n",
    "    labels_tensor = torch.tensor([_LABELS[_x['label']] for _x in instances],\n",
    "                                 dtype=torch.long).to(device)\n",
    "\n",
    "    return [padded_ids_tensor, labels_tensor]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c16e4f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertClassifier(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        super(BertClassifier, self).__init__()\n",
    "\n",
    "        self.bert = BertModel.from_pretrained(MODEL_NAME)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.l1 = nn.Linear(EMBEDDING_SIZE, 512)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.l2 = nn.Linear(512, 2)\n",
    "\n",
    "    def forward(self, input_id, mask):\n",
    "\n",
    "        _, pooled_output = self.bert(input_ids= input_id, attention_mask=mask,return_dict=False)\n",
    "        linear_output = self.l2(self.relu(self.l1(self.dropout(pooled_output))))\n",
    "\n",
    "        return linear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "879f9d10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'model'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Input \u001b[0;32mIn [40]\u001b[0m, in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m var_modelType \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbert\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      8\u001b[0m var_model_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbest_valid_f1.pt\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m---> 10\u001b[0m fc_model, fc_model_ew, collate_fc \u001b[38;5;241m=\u001b[39m \u001b[43mget_fc_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvar_model_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m                                                     \u001b[49m\u001b[43mvar_numLabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m                                                     \u001b[49m\u001b[43mtypeModel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvar_modelType\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [38]\u001b[0m, in \u001b[0;36mget_fc_model\u001b[0;34m(model_path, tokenizer, labels, device, typeModel)\u001b[0m\n\u001b[1;32m      5\u001b[0m model \u001b[38;5;241m=\u001b[39m BertForSequenceClassification\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbert-base-uncased\u001b[39m\u001b[38;5;124m'\u001b[39m, config\u001b[38;5;241m=\u001b[39mtransformer_config)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      7\u001b[0m checkpoint \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(model_path)\n\u001b[0;32m----> 8\u001b[0m model\u001b[38;5;241m.\u001b[39mload_state_dict(\u001b[43mcheckpoint\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m)\n\u001b[1;32m      9\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()  \u001b[38;5;66;03m# rnn cannot do backwards in train mode\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Adds a hook to get the embedding gradients\u001b[39;00m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'model'"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(f'Device: {device}')\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "var_numLabels = 2\n",
    "var_modelType = 'bert'\n",
    "var_model_path = 'best_valid_f1.pt'\n",
    "\n",
    "fc_model, fc_model_ew, collate_fc = get_fc_model(var_model_path, tokenizer,\n",
    "                                                     var_numLabels, device,\n",
    "                                                     typeModel=var_modelType)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a60589f2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
